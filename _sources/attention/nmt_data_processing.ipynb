{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Neual Machine Translation\r\n",
    "==========================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the following two notebooks, we use a dataset of English–French sentence pairs from the Tatoeba Project. Such paired dataset is referred to as a **parallel corpus**. This dataset is composed of pairs of English sentences and their corresponding French translations. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from argparse import Namespace\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "args = Namespace(\r\n",
    "    source_data_path=\"../data/nmt/eng-fra.txt\",\r\n",
    "    output_data_path=\"../data/nmt/simplest_eng_fra.csv\",\r\n",
    "    perc_train=0.7,\r\n",
    "    perc_val=0.15,\r\n",
    "    perc_test=0.15,\r\n",
    "    seed=1337\r\n",
    ")\r\n",
    "\r\n",
    "assert args.perc_test > 0 and (args.perc_test + args.perc_val + args.perc_train == 1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\r\n",
    "\r\n",
    "The data preprocessing begins by reading the lines in and  making all sentences lowercase and applying NLTK’s English and French tokenizers to each of the sentence pairs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "with open(args.source_data_path, encoding=\"utf-8\") as fp:\r\n",
    "    lines = fp.readlines()\r\n",
    "    \r\n",
    "lines = [line.replace(\"\\n\", \"\").lower().split(\"\\t\") for line in lines]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we apply NLTK's language­-specific word tokenizer to create a list of tokens. Even though we do further computations, which we describe next, this list of tokens is a preprocessed dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "data = []\r\n",
    "for english_sentence, french_sentence in lines:\r\n",
    "    data.append({\"english_tokens\": word_tokenize(english_sentence, language=\"english\"),\r\n",
    "                 \"french_tokens\": word_tokenize(french_sentence, language=\"french\")})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selecting a subset of data\r\n",
    "\r\n",
    "Here we select a subset of the data we select consists of the English sentences that begin with \"i am\", \"he is\", \"she is\", \"they are\", \"you are\", or \"we are\". This reduces the dataset from 135,842 sentence pairs to just\r\n",
    "13,062 sentence pairs, a factor of 10. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "filter_phrases = (\r\n",
    "    (\"i\", \"am\"), (\"i\", \"'m\"), \r\n",
    "    (\"he\", \"is\"), (\"he\", \"'s\"),\r\n",
    "    (\"she\", \"is\"), (\"she\", \"'s\"),\r\n",
    "    (\"you\", \"are\"), (\"you\", \"'re\"),\r\n",
    "    (\"we\", \"are\"), (\"we\", \"'re\"),\r\n",
    "    (\"they\", \"are\"), (\"they\", \"'re\")\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating empty lists as place holders for each filter phrase. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "data_subset = {phrase: [] for phrase in filter_phrases}\r\n",
    "data_subset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{('i', 'am'): [],\n",
       " ('i', \"'m\"): [],\n",
       " ('he', 'is'): [],\n",
       " ('he', \"'s\"): [],\n",
       " ('she', 'is'): [],\n",
       " ('she', \"'s\"): [],\n",
       " ('you', 'are'): [],\n",
       " ('you', \"'re\"): [],\n",
       " ('we', 'are'): [],\n",
       " ('we', \"'re\"): [],\n",
       " ('they', 'are'): [],\n",
       " ('they', \"'re\"): []}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the first two tokens of the English sentence as a key, if it is in the prepared data_subset keys, append the datum into the list of that key. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for datum in data:\r\n",
    "    key = tuple(datum['english_tokens'][:2])\r\n",
    "    if key in data_subset:\r\n",
    "        data_subset[key].append(datum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "counts = {k: len(v) for k,v in data_subset.items()}\r\n",
    "counts, sum(counts.values())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({('i', 'am'): 805,\n",
       "  ('i', \"'m\"): 4760,\n",
       "  ('he', 'is'): 1069,\n",
       "  ('he', \"'s\"): 787,\n",
       "  ('she', 'is'): 504,\n",
       "  ('she', \"'s\"): 316,\n",
       "  ('you', 'are'): 449,\n",
       "  ('you', \"'re\"): 2474,\n",
       "  ('we', 'are'): 181,\n",
       "  ('we', \"'re\"): 1053,\n",
       "  ('they', 'are'): 194,\n",
       "  ('they', \"'re\"): 470},\n",
       " 13062)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training, Test, Validation Split\r\n",
    "\r\n",
    "To finalize the learning setup, we split the subset of 13,062 sentence pairs into 70% training, 15% validation, and 15% test sets. The proportion of each sentence beginning with the just listed syntax is held constant by first grouping by sentence beginning, creating the splits from those groups, and then merging the splits from each group."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "np.random.seed(args.seed)\r\n",
    "\r\n",
    "dataset_stage3 = []\r\n",
    "for phrase, datum_list in sorted(data_subset.items()):\r\n",
    "    np.random.shuffle(datum_list)\r\n",
    "    n_train = int(len(datum_list) * args.perc_train)\r\n",
    "    n_val = int(len(datum_list) * args.perc_val)\r\n",
    "\r\n",
    "    for datum in datum_list[:n_train]:\r\n",
    "        datum['split'] = 'train'\r\n",
    "        \r\n",
    "    for datum in datum_list[n_train:n_train+n_val]:\r\n",
    "        datum['split'] = 'val'\r\n",
    "        \r\n",
    "    for datum in datum_list[n_train+n_val:]:\r\n",
    "        datum['split'] = 'test'\r\n",
    "    \r\n",
    "    dataset_stage3.extend(datum_list)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# here we pop and assign into the dictionary, thus modifying in place\r\n",
    "for datum in dataset_stage3:\r\n",
    "    datum['source_language'] = \" \".join(datum.pop('english_tokens'))\r\n",
    "    datum['target_language'] = \" \".join(datum.pop('french_tokens'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A glimpse of the processed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "nmt_df = pd.DataFrame(dataset_stage3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "nmt_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's the cutest boy in town .</td>\n",
       "      <td>c'est le garçon le plus mignon en ville .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a nonsmoker .</td>\n",
       "      <td>il est non-fumeur .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's smarter than me .</td>\n",
       "      <td>il est plus intelligent que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a lovely young man .</td>\n",
       "      <td>c'est un adorable jeune homme .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's three years older than me .</td>\n",
       "      <td>il a trois ans de plus que moi .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                    source_language  \\\n",
       "0  train     he 's the cutest boy in town .   \n",
       "1  train                he 's a nonsmoker .   \n",
       "2  train            he 's smarter than me .   \n",
       "3  train         he 's a lovely young man .   \n",
       "4  train  he 's three years older than me .   \n",
       "\n",
       "                             target_language  \n",
       "0  c'est le garçon le plus mignon en ville .  \n",
       "1                        il est non-fumeur .  \n",
       "2          il est plus intelligent que moi .  \n",
       "3            c'est un adorable jeune homme .  \n",
       "4           il a trois ans de plus que moi .  "
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write the processed dataset to disk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "nmt_df.to_csv(args.output_data_path)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}