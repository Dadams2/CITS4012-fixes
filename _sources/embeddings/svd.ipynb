{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Word Vectors from Word-Word Coocurrence Matrix\r\n",
    "==============================================\r\n",
    "\r\n",
    ":::{note}\r\n",
    "To run the code in this notebook, you will need the cits4012_py37 environment.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# All Import Statements Defined Here\r\n",
    "# ----------------\r\n",
    "\r\n",
    "import sys\r\n",
    "assert sys.version_info[0]==3\r\n",
    "assert sys.version_info[1] >= 5\r\n",
    "\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from gensim.test.utils import datapath\r\n",
    "import pprint\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\r\n",
    "import nltk\r\n",
    "nltk.download('reuters')\r\n",
    "from nltk.corpus import reuters\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import scipy as sp\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "START_TOKEN = '<START>'\r\n",
    "END_TOKEN = '<END>'\r\n",
    "\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "# ----------------"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most word vector models start from the following idea:\r\n",
    "\r\n",
    "*You shall know a word by the company it keeps ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))*\r\n",
    "\r\n",
    "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e.,  contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, *co-occurrence matrices* (for more information, see [here](http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf) or [here](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Co-Occurrence Matrix - Revisited\r\n",
    "\r\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window among all documents.\r\n",
    "\r\n",
    "**Example: Co-Occurrence with Fixed Window of n=1**:\r\n",
    "\r\n",
    "Document 1: \"all that glitters is not gold\"\r\n",
    "\r\n",
    "Document 2: \"all is well that ends well\"\r\n",
    "\r\n",
    "\r\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\r\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\r\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\r\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\r\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\r\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\r\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\r\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\r\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\r\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\r\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\r\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\r\n",
    "\r\n",
    "**Note:** In NLP, we often add `<START>` and `<END>` tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine `<START>` and `<END>` tokens encapsulating each document, e.g., \"`<START>` All that glitters is not gold `<END>`\", and include these tokens in our co-occurrence counts.\r\n",
    "\r\n",
    "The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run *dimensionality reduction*. In particular, we will run *SVD (Singular Value Decomposition)*, which is a kind of generalized *PCA (Principal Components Analysis)* to select the top $k$ principal components. Here's a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.\r\n",
    "\r\n",
    "![Picture of an SVD](images/svd.png \"SVD\")\r\n",
    "\r\n",
    "This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. *doctor* and *hospital* will be closer than *doctor* and *dog*. \r\n",
    "\r\n",
    ":::{note} \r\n",
    "If you can barely remember what an eigenvalue is, here's [a slow, friendly introduction to SVD](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf). If you want to learn more thoroughly about PCA or SVD, feel free to check out lectures [7](https://web.stanford.edu/class/cs168/l/l7.pdf), [8](http://theory.stanford.edu/~tim/s15/l/l8.pdf), and [9](https://web.stanford.edu/class/cs168/l/l9.pdf) of CS168. These course notes provide a great high-level treatment of these general purpose algorithms. Though, for the purpose of this class, you only need to know how to extract the k-dimensional embeddings by utilizing pre-programmed implementations of these algorithms from the numpy, scipy, or sklearn python packages. In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform PCA or SVD. However, if you only want the top $k$ vector components for relatively small $k$ — known as [Truncated SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD) — then there are reasonably scalable techniques to compute those iteratively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting Co-Occurrence Word Embeddings\n",
    "\n",
    "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a `read_corpus` function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds `<START>` and `<END>` tokens to each of the documents, and lowercases words. You do **not** have to perform any other kind of pre-processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def read_corpus(category=\"crude\"):\r\n",
    "    \"\"\" Read files from the specified Reuter's category.\r\n",
    "        Params:\r\n",
    "            category (string): category name\r\n",
    "        Return:\r\n",
    "            list of lists, with words from each of the processed files\r\n",
    "    \"\"\"\r\n",
    "    files = reuters.fileids(category)\r\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look what these documents are like…."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "reuters_corpus = read_corpus()\r\n",
    "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
      "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
      "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
      "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
      "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
      "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
      "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
      "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
      "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
      "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
      "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
      "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
      "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
      "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
      "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
      "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
      "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
      "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
      "  'they', 'noted', '.', '<END>'],\n",
      " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
      "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
      "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
      "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
      "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
      "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
      "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
      "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
      "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
      "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
      "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
      "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
      "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
      "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
      "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
      "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
      "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
      "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
      "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
      "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
      "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
      "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
      "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
      "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
      "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
      "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
      "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
      "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
      "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
      "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
      "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
      "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
      "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
      "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
      "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
      "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
      "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
      "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
      "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
      "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
      "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
      "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
      "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
      "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
      "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
      "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
      "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
      "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
      "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
      "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
      "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
      "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
      "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
      "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
      "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
      "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
      "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
      "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
      "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
      "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
      "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
      "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
      "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
      "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
      "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
      "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
      "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
      "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
      "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
      "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
      "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
      "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
      "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
      "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
      "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
      "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
      "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
      "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
      "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
      "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
      "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
      "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
      "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
      "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
      "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
      "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
      "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
      "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
      "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
      "  '<END>'],\n",
      " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
      "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
      "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
      "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
      "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
      "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
      "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
      "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
      "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
      "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
      "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
      "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
      "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
      "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
      "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
      "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
      "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
      "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
      "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
      "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
      "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find `distinct_words`\r\n",
    "\r\n",
    "Let's write a method to work out the distinct words (word types) that occur in the corpus. You can do this with `for` loops, but it's more efficient to do it with **Python list comprehensions**. In particular, [this](https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python) may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's [more information](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html).\r\n",
    "\r\n",
    "You may find it useful to use [Python sets](https://www.w3schools.com/python/python_sets.asp) to remove duplicate words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def distinct_words(corpus):\r\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\r\n",
    "        Params:\r\n",
    "            corpus (list of list of strings): corpus of documents\r\n",
    "        Return:\r\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\r\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\r\n",
    "    \"\"\"\r\n",
    "    corpus_words = []\r\n",
    "    num_corpus_words = -1\r\n",
    "    \r\n",
    "    # ------------------\r\n",
    "    # Write your implementation here.\r\n",
    "    corpus_words = sorted(list(set([y for x in corpus for y in x])))\r\n",
    "#     corpus_words = [y for x in corpus for y in x] \r\n",
    "#     corpus_words = list(set(corpus_words)) # unique words \r\n",
    "#     corpus_words = sorted(corpus_words) # sorts\r\n",
    "    num_corpus_words = len(corpus_words)\r\n",
    "    # ------------------\r\n",
    "\r\n",
    "    return corpus_words, num_corpus_words"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{admonition}{Test-driven coding practice}\r\n",
    ":class: tip \r\n",
    "A good practice is to write test code to validate your program output against expected output.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# ---------------------\r\n",
    "# Run this sanity check\r\n",
    "# Note that this not an exhaustive check for correctness.\r\n",
    "# Very simple tokenization using the Python string split \r\n",
    "# with space - string.split(\" \"). \r\n",
    "# ---------------------\r\n",
    "\r\n",
    "# Define toy corpus\r\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\r\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\r\n",
    "\r\n",
    "# Correct answers\r\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\r\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\r\n",
    "\r\n",
    "# Test correct number of words\r\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\r\n",
    "\r\n",
    "# Test correct words\r\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\r\n",
    "\r\n",
    "# Print Success\r\n",
    "print (\"-\" * 80)\r\n",
    "print(\"Passed All Tests!\")\r\n",
    "print (\"-\" * 80)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `compute_co_occurrence_matrix`\r\n",
    "\r\n",
    "Write a method that constructs a co-occurrence matrix for a certain window-size $n$ (with a default of 4), considering words $n$ before and $n$ after the word in the center of the window. Here, we start to use `numpy (np)` to represent vectors, matrices, and tensors. If you're not familiar with NumPy, there's a NumPy tutorial [Python NumPy tutorial](http://cs231n.github.io/python-numpy-tutorial/).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\r\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\r\n",
    "    \r\n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\r\n",
    "              number of co-occurring words.\r\n",
    "              \r\n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\r\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\r\n",
    "    \r\n",
    "        Params:\r\n",
    "            corpus (list of list of strings): corpus of documents\r\n",
    "            window_size (int): size of context window\r\n",
    "        Return:\r\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \r\n",
    "                Co-occurence matrix of word counts. \r\n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\r\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\r\n",
    "    \"\"\"\r\n",
    "    words, num_words = distinct_words(corpus)\r\n",
    "    M = None\r\n",
    "    word2Ind = {}\r\n",
    "    \r\n",
    "    # ------------------\r\n",
    "    M = np.zeros((num_words, num_words))\r\n",
    "    word2Ind = {w : i for i, w in enumerate(words)}\r\n",
    "\r\n",
    "\r\n",
    "    for sentence in corpus:\r\n",
    "        for cen_w_i, cen_w in enumerate(sentence): # center word\r\n",
    "            for con_w_i in range(cen_w_i-window_size, cen_w_i+window_size+1): # context word\r\n",
    "                if(cen_w_i!=con_w_i and con_w_i>=0 and con_w_i<len(sentence)):\r\n",
    "                    \r\n",
    "                    ce = word2Ind[cen_w]\r\n",
    "                    co = word2Ind[sentence[con_w_i]]\r\n",
    "                    \r\n",
    "                    M[ce][co] = M[ce][co] + 1\r\n",
    "\r\n",
    "    # ------------------\r\n",
    "\r\n",
    "    return M, word2Ind"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# ---------------------\r\n",
    "# Run this sanity check\r\n",
    "# Note that this is not an exhaustive check for correctness.\r\n",
    "# ---------------------\r\n",
    "\r\n",
    "# Define toy corpus and get student's co-occurrence matrix\r\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\r\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\r\n",
    "\r\n",
    "# Correct M and word2Ind\r\n",
    "M_test_ans = np.array( \r\n",
    "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\r\n",
    "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\r\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\r\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\r\n",
    "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\r\n",
    "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\r\n",
    "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\r\n",
    "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\r\n",
    "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\r\n",
    "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\r\n",
    ")\r\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\r\n",
    "word2Ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\r\n",
    "\r\n",
    "# Test correct word2Ind\r\n",
    "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\r\n",
    "\r\n",
    "# Test correct M shape\r\n",
    "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\r\n",
    "\r\n",
    "# Test correct M values\r\n",
    "for w1 in word2Ind_ans.keys():\r\n",
    "    idx1 = word2Ind_ans[w1]\r\n",
    "    for w2 in word2Ind_ans.keys():\r\n",
    "        idx2 = word2Ind_ans[w2]\r\n",
    "        student = M_test[idx1, idx2]\r\n",
    "        correct = M_test_ans[idx1, idx2]\r\n",
    "        if student != correct:\r\n",
    "            print(\"Correct M:\")\r\n",
    "            print(M_test_ans)\r\n",
    "            print(\"Your M: \")\r\n",
    "            print(M_test)\r\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\r\n",
    "\r\n",
    "# Print Success\r\n",
    "print (\"-\" * 80)\r\n",
    "print(\"Passed All Tests!\")\r\n",
    "print (\"-\" * 80)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using SVD to `reduce_to_k_dim`\r\n",
    "\r\n",
    "Now let's construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings. \r\n",
    "\r\n",
    ":::{note}\r\n",
    "All of numpy, scipy, and scikit-learn (`sklearn`) provide *some* implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import sklearn\r\n",
    "def reduce_to_k_dim(M, k=2):\r\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\r\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\r\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\r\n",
    "    \r\n",
    "        Params:\r\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\r\n",
    "            k (int): embedding size of each word after dimension reduction\r\n",
    "        Return:\r\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\r\n",
    "                    In terms of the SVD from math class, this actually returns U * S\r\n",
    "    \"\"\"    \r\n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\r\n",
    "    M_reduced = None\r\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\r\n",
    "    \r\n",
    "        # ------------------\r\n",
    "        # Write your implementation here.\r\n",
    "    M_r = sklearn.decomposition.TruncatedSVD(n_components=k,\r\n",
    "                                           algorithm='randomized', \r\n",
    "                                           n_iter=n_iters, \r\n",
    "                                           random_state=None, \r\n",
    "                                           tol=0.0)\r\n",
    "    M_reduced = M_r.fit_transform(M)\r\n",
    "        # ------------------\r\n",
    "\r\n",
    "    print(\"Done.\")\r\n",
    "    return M_reduced"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# ---------------------\r\n",
    "# Run this sanity check\r\n",
    "# Note that this is not an exhaustive check for correctness \r\n",
    "# In fact we only check that your M_reduced has the right dimensions.\r\n",
    "# ---------------------\r\n",
    "\r\n",
    "# Define toy corpus and run student code\r\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\r\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\r\n",
    "M_test_reduced = reduce_to_k_dim(M_test, k=2)\r\n",
    "\r\n",
    "# Test proper dimensions\r\n",
    "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\r\n",
    "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\r\n",
    "\r\n",
    "# Print Success\r\n",
    "print (\"-\" * 80)\r\n",
    "print(\"Passed All Tests!\")\r\n",
    "print (\"-\" * 80)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Truncated SVD over 10 words...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise the embeddings using `plot_embeddings`\r\n",
    "\r\n",
    "We can now write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (`plt`).\r\n",
    "\r\n",
    "For this example, you may find it useful to adapt [this code](https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/). In the future, a good way to make a plot is to look at [the Matplotlib gallery](https://matplotlib.org/gallery/index.html), find a plot that looks somewhat like what you want, and adapt the code they give."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\r\n",
    "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\r\n",
    "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\r\n",
    "        Include a label next to each point.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\r\n",
    "            word2Ind (dict): dictionary that maps word to indices for matrix M\r\n",
    "            words (list of strings): words whose embeddings we want to visualize\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # ------------------\r\n",
    "    import matplotlib.pyplot as plt \r\n",
    "    x_coords = M_reduced[:,0]\r\n",
    "    y_coords = M_reduced[:,1]\r\n",
    "    \r\n",
    "    for word in words:\r\n",
    "        i = word2Ind[word]\r\n",
    "        x = x_coords[i]\r\n",
    "        y = y_coords[i]\r\n",
    "        plt.scatter(x, y, color='r', marker='x')\r\n",
    "        plt.annotate(word, (x, y))\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    # ------------------"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# ---------------------\r\n",
    "# Run this sanity check\r\n",
    "# Note that this is not an exhaustive check for correctness.\r\n",
    "# The plot produced should look like the \"test solution plot\" depicted below. \r\n",
    "# ---------------------\r\n",
    "\r\n",
    "print (\"-\" * 80)\r\n",
    "print (\"Outputted Plot:\")\r\n",
    "\r\n",
    "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\r\n",
    "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\r\n",
    "words = ['test1', 'test2', 'test3', 'test4', 'test5']\r\n",
    "plot_embeddings(M_reduced_plot_test, word2Ind_plot_test, words)\r\n",
    "\r\n",
    "print (\"-\" * 80)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Outputted Plot:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"302.878125pt\" version=\"1.1\" viewBox=\"0 0 610.247301 302.878125\" width=\"610.247301pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-09-01T14:23:16.719246</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 302.878125 \r\nL 610.247301 302.878125 \r\nL 610.247301 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 44.845313 279 \r\nL 602.845312 279 \r\nL 602.845312 7.2 \r\nL 44.845313 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M -3 3 \r\nL 3 -3 \r\nM -3 -3 \r\nL 3 3 \r\n\" id=\"m85c40b9f09\" style=\"stroke:#ff0000;stroke-width:1.5;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p24ad6abec7)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"577.481676\" xlink:href=\"#m85c40b9f09\" y=\"19.554545\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PathCollection_2\">\r\n    <g clip-path=\"url(#p24ad6abec7)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"70.208949\" xlink:href=\"#m85c40b9f09\" y=\"266.645455\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PathCollection_3\">\r\n    <g clip-path=\"url(#p24ad6abec7)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"577.481676\" xlink:href=\"#m85c40b9f09\" y=\"266.645455\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PathCollection_4\">\r\n    <g clip-path=\"url(#p24ad6abec7)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"70.208949\" xlink:href=\"#m85c40b9f09\" y=\"19.554545\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PathCollection_5\">\r\n    <g clip-path=\"url(#p24ad6abec7)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"323.845312\" xlink:href=\"#m85c40b9f09\" y=\"143.1\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m09143f317b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.208949\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −1.00 -->\r\n      <g transform=\"translate(54.886293 293.598437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 678 2272 \r\nL 4684 2272 \r\nL 4684 1741 \r\nL 678 1741 \r\nL 678 2272 \r\nz\r\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"133.61804\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- −0.75 -->\r\n      <g transform=\"translate(118.295384 293.598437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.027131\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- −0.50 -->\r\n      <g transform=\"translate(181.704474 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.436222\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- −0.25 -->\r\n      <g transform=\"translate(245.113565 293.598437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"323.845312\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(312.7125 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"387.254403\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(376.121591 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"450.663494\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.50 -->\r\n      <g transform=\"translate(439.530682 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"514.072585\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.75 -->\r\n      <g transform=\"translate(502.939773 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"577.481676\" xlink:href=\"#m09143f317b\" y=\"279\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(566.348864 293.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m8a01126cad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"266.645455\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- −1.00 -->\r\n      <g transform=\"translate(7.2 270.444673)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"235.759091\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- −0.75 -->\r\n      <g transform=\"translate(7.2 239.55831)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"204.872727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- −0.50 -->\r\n      <g transform=\"translate(7.2 208.671946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"173.986364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- −0.25 -->\r\n      <g transform=\"translate(7.2 177.785582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-2212\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"143.1\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(15.579688 146.899219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"112.213636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(15.579688 116.012855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"81.327273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.50 -->\r\n      <g transform=\"translate(15.579688 85.126491)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"50.440909\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 0.75 -->\r\n      <g transform=\"translate(15.579688 54.240128)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#m8a01126cad\" y=\"19.554545\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(15.579688 23.353764)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 44.845313 279 \r\nL 44.845313 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 602.845312 279 \r\nL 602.845312 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 44.845312 279 \r\nL 602.845312 279 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 44.845312 7.2 \r\nL 602.845312 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_19\">\r\n    <!-- test1 -->\r\n    <g transform=\"translate(577.481676 19.554545)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-31\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_20\">\r\n    <!-- test2 -->\r\n    <g transform=\"translate(70.208949 266.645455)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_21\">\r\n    <!-- test3 -->\r\n    <g transform=\"translate(577.481676 266.645455)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-33\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_22\">\r\n    <!-- test4 -->\r\n    <g transform=\"translate(70.208949 19.554545)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-34\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_23\">\r\n    <!-- test5 -->\r\n    <g transform=\"translate(323.845312 143.1)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-35\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p24ad6abec7\">\r\n   <rect height=\"271.8\" width=\"558\" x=\"44.845313\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('lda': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e5f76115fc97e95e5e5f274d9a04d3733842aec59c7431144116cf5affd2efc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}