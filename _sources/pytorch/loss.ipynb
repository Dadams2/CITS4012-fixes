{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Loss Functions\r\n",
    "====================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "seed = 42\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed_all(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perceptron"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class Perceptron(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    A perceptron is one linear layer \r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, input_dim):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            input_dim (int): size of the input features\r\n",
    "        \"\"\"\r\n",
    "        super(Perceptron, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\r\n",
    "\r\n",
    "    def forward(self, x_in):\r\n",
    "        \"\"\"The forward pass of the perceptron\r\n",
    "\r\n",
    "        Args:\r\n",
    "            x_in (torch.Tensor): an input data tensor\r\n",
    "                x_in.shape should be (batch, num_features)\r\n",
    "        Returns:\r\n",
    "            the resulting tensor. tensor.shape should be (batch,).\r\n",
    "        \"\"\"\r\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does .squeeze() do in Python?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Python program explaining\r\n",
    "# numpy.squeeze function\r\n",
    "  \r\n",
    "import numpy as np\r\n",
    "  \r\n",
    "in_arr = np.array([[[2, 2, 2], [2, 2, 2]]])\r\n",
    "   \r\n",
    "print (\"Input array : \", in_arr) \r\n",
    "print(\"Shape of input array : \", in_arr.shape)  \r\n",
    "  \r\n",
    "out_arr = np.squeeze(in_arr) \r\n",
    "  \r\n",
    "print (\"output squeezed array : \", out_arr)\r\n",
    "print(\"Shape of output array : \", out_arr.shape) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input array :  [[[2 2 2]\n",
      "  [2 2 2]]]\n",
      "Shape of input array :  (1, 2, 3)\n",
      "output squeezed array :  [[2 2 2]\n",
      " [2 2 2]]\n",
      "Shape of output array :  (2, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "softmax = nn.Softmax(dim=1)\r\n",
    "x_input = torch.randn(1, 3)\r\n",
    "y_output = softmax(x_input)\r\n",
    "print(x_input)\r\n",
    "print(y_output)\r\n",
    "print(torch.sum(y_output, dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.3367, 0.1288, 0.2345]])\n",
      "tensor([[0.3683, 0.2992, 0.3325]])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MSELoss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "mse_loss = nn.MSELoss()\r\n",
    "torch.manual_seed(42)\r\n",
    "yhat = torch.randn(3, 5, requires_grad=True)\r\n",
    "y = torch.randn(3, 5)\r\n",
    "loss = mse_loss(yhat, y)\r\n",
    "loss.backward()\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0192, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MAE Loss - L1Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "mse_loss = nn.L1Loss()\r\n",
    "torch.manual_seed(42)\r\n",
    "yhat = torch.randn(3, 5, requires_grad=True)\r\n",
    "y = torch.randn(3, 5)\r\n",
    "loss = mse_loss(yhat, y)\r\n",
    "print(yhat)\r\n",
    "print(y)\r\n",
    "loss.backward()\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
      "        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094,  1.1103, -1.6898, -0.9890]], requires_grad=True)\n",
      "tensor([[ 0.9580,  1.3221,  0.8172, -0.7658, -0.7506],\n",
      "        [ 1.3525,  0.6863, -0.3278,  0.7950,  0.2815],\n",
      "        [ 0.0562,  0.5227, -0.2384, -0.0499,  0.5263]])\n",
      "tensor(0.8502, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RMSE Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "class RMSELoss(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.mse = nn.MSELoss()\r\n",
    "        \r\n",
    "    def forward(self,yhat,y):\r\n",
    "        eps = 1e-7\r\n",
    "        return torch.sqrt(self.mse(yhat,y) + eps)\r\n",
    "\r\n",
    "criterion = RMSELoss()\r\n",
    "\r\n",
    "torch.manual_seed(42)\r\n",
    "yhat = torch.randn(3, 5, requires_grad=True)\r\n",
    "y = torch.randn(3, 5)\r\n",
    "loss = criterion(yhat,y)\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0096, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CrossEntropyLoss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "ce_loss = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "torch.manual_seed(42)\r\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\r\n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64)\r\n",
    "loss = ce_loss(outputs, targets)\r\n",
    "loss.backward()\r\n",
    "print (loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.6812, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Entropy Loss - Manual Calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "labels = torch.tensor([1.0, 0.0])\r\n",
    "predictions = torch.tensor([.9, .2])\r\n",
    "# Positive class (labels == 1)\r\n",
    "positive_pred = predictions[labels == 1]\r\n",
    "first_summation = torch.log(positive_pred).sum()\r\n",
    "# Negative class (labels == 0)\r\n",
    "negative_pred = predictions[labels == 0]\r\n",
    "second_summation = torch.log(1 - negative_pred).sum()\r\n",
    "# n_total = n_pos + n_neg\r\n",
    "n_total = labels.size(0)\r\n",
    "loss = -(first_summation + second_summation) / n_total\r\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.1643)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "positive_pred = predictions[labels == 1]\r\n",
    "print(labels == 1)\r\n",
    "print(positive_pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ True, False])\n",
      "tensor([0.9000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "summation = torch.sum(labels * torch.log(predictions) + (1 - labels) * torch.log(1 - predictions))\r\n",
    "loss = -summation / n_total\r\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.1643)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary Cross Entropy Loss - PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "bce_loss = nn.BCELoss()\r\n",
    "sigmoid = nn.Sigmoid()\r\n",
    "\r\n",
    "torch.manual_seed(42)\r\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\r\n",
    "print(probabilities)\r\n",
    "\r\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\r\n",
    "loss = bce_loss(probabilities, targets)\r\n",
    "loss.backward()\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5834],\n",
      "        [0.5322],\n",
      "        [0.5583],\n",
      "        [0.5573]], grad_fn=<SigmoidBackward>)\n",
      "tensor(0.6741, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{alert}\r\n",
    "ORDER matters in calling the BCE Loss function. Remember, prediction first, truth second. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "labels = torch.tensor([1.0, 0.0])\r\n",
    "predictions = torch.tensor([.9, .2])\r\n",
    "# RIGHT\r\n",
    "right_loss = bce_loss(predictions, labels)\r\n",
    "# WRONG\r\n",
    "wrong_loss = bce_loss(labels, predictions)\r\n",
    "print(right_loss, wrong_loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1643) tensor(15.0000)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BCE with Logits Loss "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import numpy as np\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "def log_odds_ratio(prob):\r\n",
    "    return np.log(prob/(1-prob))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "logit1 = log_odds_ratio(.9)\r\n",
    "logit2 = log_odds_ratio(.2)\r\n",
    "labels = torch.tensor([1.0, 0.0])\r\n",
    "logits = torch.tensor([logit1, logit2])\r\n",
    "print(logits)\r\n",
    "loss_fn_logits = nn.BCEWithLogitsLoss()\r\n",
    "loss = loss_fn_logits(logits, labels)\r\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 2.1972, -1.3863], dtype=torch.float64)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.1643)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Negative Log-Likelihood Loss\r\n",
    "\r\n",
    "### Log Softmax\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "logits = torch.tensor([ 1.3863, 0.0000, -0.6931])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Softmax Manually"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# e^z is odds ratios\r\n",
    "odds_ratios = torch.exp(logits) \r\n",
    "odds_ratios"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([4.0000, 1.0000, 0.5000])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "softmaxed = odds_ratios/odds_ratios.sum()\r\n",
    "softmaxed"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Softmax in PyTorch\r\n",
    "\r\n",
    "PyTorch provide the typical implementations both as a function (`F.softmax`) and as a module (`nn.Softmax`), which both take in logits (the log odds) as input. We need to tell the softmax function which dimension should be applied to. \r\n",
    "\r\n",
    "In general, our models will produce logits with the shape `(number of\r\n",
    "data points, number of classes)`, so the right dimension to apply softmax to is the\r\n",
    "last one (dim=-1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "## Using module\r\n",
    "softmaxed_torch = nn.Softmax(dim=-1)(logits)\r\n",
    "softmaxed_torch"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Using functional\r\n",
    "softmaxed_torch = F.softmax(logits, dim=-1)\r\n",
    "softmaxed_torch"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LogSoftmax"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "torch.log(softmaxed)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.3185, -1.7048, -2.3979])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Use functional\r\n",
    "log_probs = F.log_softmax(logits, dim=-1)\r\n",
    "log_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.3185, -1.7048, -2.3979])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Use module\r\n",
    "log_probs = nn.LogSoftmax(dim=-1)(logits)\r\n",
    "log_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.3185, -1.7048, -2.3979])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Negative Loglikelihood Loss\r\n",
    "\r\n",
    "NLL loss is a summation of the negative log probilities (log softmax) for all classes over the data points. \r\n",
    "\r\n",
    "Let's take a look at the following 3 classes, 5 data points' NLL loss calculation.\r\n",
    "\r\n",
    "logits $\\rightarrow$ log softmax (log probability) $\\rightarrow$ NLL "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# manual implementation\r\n",
    "\r\n",
    "torch.manual_seed(11)\r\n",
    "logits = torch.randn((5, 3))\r\n",
    "labels = torch.tensor([0, 0, 1, 2, 1])\r\n",
    "log_probs = F.log_softmax(logits, dim=-1)\r\n",
    "log_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.5229, -0.3146, -2.9600],\n",
       "        [-1.7934, -1.0044, -0.7607],\n",
       "        [-1.2513, -1.0136, -1.0471],\n",
       "        [-2.6799, -0.2219, -2.0367],\n",
       "        [-1.0728, -1.9098, -0.6737]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLL is a negated sum of log probabilities. \r\n",
    "\r\n",
    "### Manual Calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "#indices = torch.tensor([[0], [0], [1], [2], [1]])\r\n",
    "indices = torch.tensor([[idx] for idx in labels])\r\n",
    "log_probs_cls = log_probs.gather(1, indices)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# for each data point, we select the log_probs of corresponding class\r\n",
    "-log_probs_cls.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "F.nll_loss(log_probs, labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "loss_fn = nn.NLLLoss()\r\n",
    "loss_fn(log_probs, labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The preferred module implementation `nn.NLLLoss` loss function is a higher-order function, and this one takes three optional arguments (the others are deprecated and you can safely ignore them).\r\n",
    "\r\n",
    "**reduction**: it takes either `mean`, `sum`, or `none`. The default, `mean`, corresponds to\r\n",
    "our equation above. As expected, `sum` will return the sum of the errors,\r\n",
    "instead of the average. The last option, `none`, corresponds to the unreduced\r\n",
    "form, that is, it returns the full array of errors.\r\n",
    "\r\n",
    "**weight**: it takes a tensor of length C, that is, containing as many weights as\r\n",
    "there are classes.\r\n",
    "\r\n",
    ":::{important} \r\n",
    "this weight argument can be used to handle imbalanced datasets, unlike the weight argument in the binary cross-entropy losses we've seen before. Also, unlike the pos_weight argument of BCEWithLogitsLoss, the NLLLoss computes a true weighted average when this argument\r\n",
    "is used.\r\n",
    ":::\r\n",
    "\r\n",
    "**ignore_index**: it takes one integer, corresponding to the one (and only one)\r\n",
    "class index that should be ignored when computing the loss. It can be used to\r\n",
    "mask a particular label that is not relevant to the classification task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of Using Weight \r\n",
    "\r\n",
    "What if we want to balance our dataset, giving data points with label (y=2) double the weight of the other classes?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\r\n",
    "loss_fn(log_probs, labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.7188)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of Using ignore_index\r\n",
    "\r\n",
    "What if we want to simply ignore data points with label (y=2)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "loss_fn = nn.NLLLoss(ignore_index=2)\r\n",
    "loss_fn(log_probs, labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.5599)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}