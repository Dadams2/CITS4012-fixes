{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Neural Networks in PyTorch\r\n",
    "======================================\r\n",
    "## A neural network with one hidden layer \r\n",
    "\r\n",
    "Extending the simple perceptron in the openning page of this lab, let's build a simple neural network that takes two binary inputs, and simulate the logical operation of XOR. The network has two sets of weights and biases, \r\n",
    "one set between the input and the hidden layer with two nodes, another set between the hidden layer and the single output, as shown in the figure below.\r\n",
    "\r\n",
    "```{figure} ../images/neural-network-xor.png\r\n",
    ":alt: The XOR Neural Network\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 400px\r\n",
    "```\r\n",
    "\r\n",
    "```{note}\r\n",
    "Compare the code below with the `Perceptron` code at the front page of this lab to have a better understanding of the building blocks. \r\n",
    "``` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the model class\r\n",
    "\r\n",
    "Let's define the XOR neural network model inherited from the `torch.nn.Module` class using an Object Oriented approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class XOR(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    An XOR is similuated using neural network with \r\n",
    "    two fully connected linear layers \r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, input_dim, output_dim):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            input_dim (int): size of the input features\r\n",
    "            output_dim (int): size of the output\r\n",
    "        \"\"\"\r\n",
    "        super(XOR, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(input_dim, 2)\r\n",
    "        self.fc2 = nn.Linear(2, output_dim)\r\n",
    "\r\n",
    "    def forward(self, x_in):\r\n",
    "        \"\"\"The forward pass of the perceptron\r\n",
    "\r\n",
    "        Args:\r\n",
    "            x_in (torch.Tensor): an input data tensor\r\n",
    "                x_in.shape should be (batch, num_features)\r\n",
    "        Returns:\r\n",
    "            the resulting tensor. tensor.shape should be (batch,).\r\n",
    "        \"\"\"\r\n",
    "        hidden = torch.sigmoid(self.fc1(x_in))\r\n",
    "        yhat = torch.sigmoid(self.fc2(hidden))\r\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Object Orientation in PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In PyTorch, a model (e.g. the `XOR` model) is represented by a regular Python class that inherits from the Module class.\r\n",
    "\r\n",
    ":::{important}\r\n",
    "IMPORTANT: If you are uncomfortable with object-oriented\r\n",
    "programming (OOP) concepts like *classes*, *constructors*, *methods/class methods*, *instances*, and *attributes*, it is strongly recommended to follow tutorials such as [Real Python's Objected-Oriented Programming (OOP) in Python 3](https://realpython.com/python3-object-oriented-programming/)\r\n",
    ":::\r\n",
    "\r\n",
    "The most fundamental methods a model class needs to implement are:\r\n",
    "- `__init__(self)`: it defines the parts that make up the model — in our case,\r\n",
    "two parameters, b and w.\r\n",
    "- `forward(self, x)`: it performs the actual computation, that is, it outputs a\r\n",
    "prediction, given the input x.\r\n",
    "\r\n",
    ":::{note}\r\n",
    "Do not forget to include `super().__init__()` to execute\r\n",
    "the `__init__()` method of the parent class `(nn.Module)` before\r\n",
    "your own.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XOR Model\r\n",
    " \r\n",
    "Now let's create a Linear Model with two inputs (features) and one output. \r\n",
    "\r\n",
    "Calling the XOR constructor with an `input_dim=2` and `output_dim=1`, namely `XOR(2,1)` results in two fully connected linear models, one `nn.Linear(2,2)` and ``nn.Linear(2,1)`, which will create a model with two input features, one output feature with biases at the input layer, hidden layer and output layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = XOR(2,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtain all model parameters using `state_dict()`\r\n",
    "\r\n",
    "We can get the current values of all parameters using our model’s\r\n",
    "state_dict() method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model.state_dict()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.5827,  0.0717],\n",
       "                      [-0.2969, -0.1214]])),\n",
       "             ('fc1.bias', tensor([-0.5186, -0.6406])),\n",
       "             ('fc2.weight', tensor([[0.3564, 0.5904]])),\n",
       "             ('fc2.bias', tensor([0.3660]))])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We used to manually assign random values to these weights and biases. Now PyTorch does it for us automatically. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `state_dict()` of a given model is simply a Python dictionary that maps each attribute/parameter to its corresponding tensor. But only learnable parameters are included, as its purpose is to keep track of parameters that are going to be updated by the optimizer.\r\n",
    "\r\n",
    "The optimizer itself has a `state_dict()` too, which contains its internal\r\n",
    "state, as well as other hyper-parameters. Let's take a quick look at it:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "lr = 0.01 \r\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "optimizer.state_dict()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.01,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model and Data need to be on the same device\r\n",
    "\r\n",
    ":::{important} \r\n",
    "We need to send our model to the same device\r\n",
    "where the data is. If our data is made of GPU tensors, our model\r\n",
    "must \"live\" inside the GPU as well.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training XOR \r\n",
    "\r\n",
    "Now let's put it all together to train an neural XOR model.\r\n",
    "\r\n",
    "### Training Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Training data preparation\r\n",
    "\r\n",
    "x_train_tensor = torch.tensor([[0,0],[0,1],[1,1],[1,0]], device=device).float()\r\n",
    "y_train_tensor = torch.tensor([0,1,1,0], device=device).view(4,1).float()\r\n",
    "\r\n",
    "x_val_tensor = torch.clone(x_train_tensor)\r\n",
    "y_val_tensor = torch.clone(y_train_tensor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Verify the shape of the output tensor\r\n",
    "y_train_tensor.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter setup\r\n",
    "\r\n",
    "We need to set up the learning rate and the number of epochs, and then select the three key compoenents of a neural model: model, optimiser and loss function before training. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like\r\n",
    "# Greek letter\r\n",
    "lr = 0.01\r\n",
    "\r\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\r\n",
    "torch.manual_seed(42)\r\n",
    "# Now we can create a model and send it at once to the device\r\n",
    "model = XOR(2,1)\r\n",
    "model = model.to(device)\r\n",
    "\r\n",
    "# Defines a SGD optimizer to update the parameters\r\n",
    "# (now retrieved directly from the model)\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "\r\n",
    "# Defines a MSE loss function\r\n",
    "loss_fn = nn.MSELoss(reduction='mean')\r\n",
    "\r\n",
    "# Defines number of epochs\r\n",
    "n_epochs = 100000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\r\n",
    "\r\n",
    "Now we are ready to train. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for epoch in range(n_epochs):\r\n",
    "    #for j in range(steps):\r\n",
    "    model.train() # What is this?!?\r\n",
    "\r\n",
    "    # Step 1 - Computes model's predicted output - forward pass\r\n",
    "    # No more manual prediction!\r\n",
    "    yhat = model(x_train_tensor)\r\n",
    "\r\n",
    "    # Step 2 - Computes the loss\r\n",
    "    loss = loss_fn(yhat, y_train_tensor)\r\n",
    "    \r\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\r\n",
    "    loss.backward()\r\n",
    "\r\n",
    "    # Step 4 - Updates parameters using gradients and\r\n",
    "    # the learning rate\r\n",
    "    optimizer.step()\r\n",
    "    optimizer.zero_grad()\r\n",
    "    if (epoch % 500 == 0):\r\n",
    "        print(\"Epoch: {0}, Loss: {1}, \".format(epoch, loss.to(\"cpu\").detach().numpy()))\r\n",
    "\r\n",
    "# We can also inspect its parameters using its state_dict\r\n",
    "print(model.state_dict())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.27375736832618713, \n",
      "Epoch: 500, Loss: 0.2555079460144043, \n",
      "Epoch: 1000, Loss: 0.25011080503463745, \n",
      "Epoch: 1500, Loss: 0.24655020236968994, \n",
      "Epoch: 2000, Loss: 0.24295492470264435, \n",
      "Epoch: 2500, Loss: 0.238966703414917, \n",
      "Epoch: 3000, Loss: 0.23448437452316284, \n",
      "Epoch: 3500, Loss: 0.22930260002613068, \n",
      "Epoch: 4000, Loss: 0.2232476770877838, \n",
      "Epoch: 4500, Loss: 0.21623794734477997, \n",
      "Epoch: 5000, Loss: 0.20812170207500458, \n",
      "Epoch: 5500, Loss: 0.19883160293102264, \n",
      "Epoch: 6000, Loss: 0.18850544095039368, \n",
      "Epoch: 6500, Loss: 0.1771014928817749, \n",
      "Epoch: 7000, Loss: 0.16488364338874817, \n",
      "Epoch: 7500, Loss: 0.15211743116378784, \n",
      "Epoch: 8000, Loss: 0.139143705368042, \n",
      "Epoch: 8500, Loss: 0.12626223266124725, \n",
      "Epoch: 9000, Loss: 0.11392521113157272, \n",
      "Epoch: 9500, Loss: 0.10234947502613068, \n",
      "Epoch: 10000, Loss: 0.09172774851322174, \n",
      "Epoch: 10500, Loss: 0.08199518918991089, \n",
      "Epoch: 11000, Loss: 0.0733182281255722, \n",
      "Epoch: 11500, Loss: 0.06560301780700684, \n",
      "Epoch: 12000, Loss: 0.05884382501244545, \n",
      "Epoch: 12500, Loss: 0.05294763296842575, \n",
      "Epoch: 13000, Loss: 0.04782135412096977, \n",
      "Epoch: 13500, Loss: 0.04328809678554535, \n",
      "Epoch: 14000, Loss: 0.039366669952869415, \n",
      "Epoch: 14500, Loss: 0.03590844199061394, \n",
      "Epoch: 15000, Loss: 0.03286368399858475, \n",
      "Epoch: 15500, Loss: 0.030230185016989708, \n",
      "Epoch: 16000, Loss: 0.02787426859140396, \n",
      "Epoch: 16500, Loss: 0.025831308215856552, \n",
      "Epoch: 17000, Loss: 0.02391224540770054, \n",
      "Epoch: 17500, Loss: 0.02228483371436596, \n",
      "Epoch: 18000, Loss: 0.02080184407532215, \n",
      "Epoch: 18500, Loss: 0.019479243084788322, \n",
      "Epoch: 19000, Loss: 0.018279727548360825, \n",
      "Epoch: 19500, Loss: 0.017226679250597954, \n",
      "Epoch: 20000, Loss: 0.01625426858663559, \n",
      "Epoch: 20500, Loss: 0.01537842396646738, \n",
      "Epoch: 21000, Loss: 0.014549647457897663, \n",
      "Epoch: 21500, Loss: 0.01378196757286787, \n",
      "Epoch: 22000, Loss: 0.01309845969080925, \n",
      "Epoch: 22500, Loss: 0.01248301099985838, \n",
      "Epoch: 23000, Loss: 0.011914841830730438, \n",
      "Epoch: 23500, Loss: 0.011366013437509537, \n",
      "Epoch: 24000, Loss: 0.01088915579020977, \n",
      "Epoch: 24500, Loss: 0.010406192392110825, \n",
      "Epoch: 25000, Loss: 0.010014466941356659, \n",
      "Epoch: 25500, Loss: 0.009598497301340103, \n",
      "Epoch: 26000, Loss: 0.009224366396665573, \n",
      "Epoch: 26500, Loss: 0.008882050402462482, \n",
      "Epoch: 27000, Loss: 0.008561154827475548, \n",
      "Epoch: 27500, Loss: 0.008238466456532478, \n",
      "Epoch: 28000, Loss: 0.007974416017532349, \n",
      "Epoch: 28500, Loss: 0.007680158130824566, \n",
      "Epoch: 29000, Loss: 0.0074332160875201225, \n",
      "Epoch: 29500, Loss: 0.007197014521807432, \n",
      "Epoch: 30000, Loss: 0.006972037721425295, \n",
      "Epoch: 30500, Loss: 0.0067596533335745335, \n",
      "Epoch: 31000, Loss: 0.006562951020896435, \n",
      "Epoch: 31500, Loss: 0.006372722331434488, \n",
      "Epoch: 32000, Loss: 0.006173261906951666, \n",
      "Epoch: 32500, Loss: 0.005995258688926697, \n",
      "Epoch: 33000, Loss: 0.005834578536450863, \n",
      "Epoch: 33500, Loss: 0.005697771906852722, \n",
      "Epoch: 34000, Loss: 0.005533096380531788, \n",
      "Epoch: 34500, Loss: 0.005399664863944054, \n",
      "Epoch: 35000, Loss: 0.005252258852124214, \n",
      "Epoch: 35500, Loss: 0.0051279375329613686, \n",
      "Epoch: 36000, Loss: 0.005004711449146271, \n",
      "Epoch: 36500, Loss: 0.00487515376880765, \n",
      "Epoch: 37000, Loss: 0.004761365242302418, \n",
      "Epoch: 37500, Loss: 0.004653751850128174, \n",
      "Epoch: 38000, Loss: 0.004538621753454208, \n",
      "Epoch: 38500, Loss: 0.004451357759535313, \n",
      "Epoch: 39000, Loss: 0.004348565824329853, \n",
      "Epoch: 39500, Loss: 0.004250797443091869, \n",
      "Epoch: 40000, Loss: 0.0041722762398421764, \n",
      "Epoch: 40500, Loss: 0.00408073328435421, \n",
      "Epoch: 41000, Loss: 0.004001058172434568, \n",
      "Epoch: 41500, Loss: 0.003916418645530939, \n",
      "Epoch: 42000, Loss: 0.0038418788462877274, \n",
      "Epoch: 42500, Loss: 0.0037747276946902275, \n",
      "Epoch: 43000, Loss: 0.003688385244458914, \n",
      "Epoch: 43500, Loss: 0.003622728865593672, \n",
      "Epoch: 44000, Loss: 0.0035602175630629063, \n",
      "Epoch: 44500, Loss: 0.003489775350317359, \n",
      "Epoch: 45000, Loss: 0.003426765091717243, \n",
      "Epoch: 45500, Loss: 0.003366705495864153, \n",
      "Epoch: 46000, Loss: 0.0033056712709367275, \n",
      "Epoch: 46500, Loss: 0.0032519223168492317, \n",
      "Epoch: 47000, Loss: 0.0031912513077259064, \n",
      "Epoch: 47500, Loss: 0.0031480954494327307, \n",
      "Epoch: 48000, Loss: 0.003087468910962343, \n",
      "Epoch: 48500, Loss: 0.0030377130024135113, \n",
      "Epoch: 49000, Loss: 0.0029869868885725737, \n",
      "Epoch: 49500, Loss: 0.0029457740020006895, \n",
      "Epoch: 50000, Loss: 0.0028982609510421753, \n",
      "Epoch: 50500, Loss: 0.0028544270899146795, \n",
      "Epoch: 51000, Loss: 0.0028089506085962057, \n",
      "Epoch: 51500, Loss: 0.0027682341169565916, \n",
      "Epoch: 52000, Loss: 0.0027215657755732536, \n",
      "Epoch: 52500, Loss: 0.0026850481517612934, \n",
      "Epoch: 53000, Loss: 0.002641090890392661, \n",
      "Epoch: 53500, Loss: 0.0026040119118988514, \n",
      "Epoch: 54000, Loss: 0.0025728303007781506, \n",
      "Epoch: 54500, Loss: 0.0025344102177768946, \n",
      "Epoch: 55000, Loss: 0.0024971726816147566, \n",
      "Epoch: 55500, Loss: 0.002462733769789338, \n",
      "Epoch: 56000, Loss: 0.002433920744806528, \n",
      "Epoch: 56500, Loss: 0.002397480420768261, \n",
      "Epoch: 57000, Loss: 0.0023658026475459337, \n",
      "Epoch: 57500, Loss: 0.002332453615963459, \n",
      "Epoch: 58000, Loss: 0.002302789594978094, \n",
      "Epoch: 58500, Loss: 0.002273410093039274, \n",
      "Epoch: 59000, Loss: 0.002243262715637684, \n",
      "Epoch: 59500, Loss: 0.0022170250304043293, \n",
      "Epoch: 60000, Loss: 0.002190019004046917, \n",
      "Epoch: 60500, Loss: 0.002158280462026596, \n",
      "Epoch: 61000, Loss: 0.0021350218448787928, \n",
      "Epoch: 61500, Loss: 0.0021104959305375814, \n",
      "Epoch: 62000, Loss: 0.0020827956032007933, \n",
      "Epoch: 62500, Loss: 0.002065179403871298, \n",
      "Epoch: 63000, Loss: 0.002039001788944006, \n",
      "Epoch: 63500, Loss: 0.0020126295275986195, \n",
      "Epoch: 64000, Loss: 0.0019906111992895603, \n",
      "Epoch: 64500, Loss: 0.0019670012407004833, \n",
      "Epoch: 65000, Loss: 0.0019415951101109385, \n",
      "Epoch: 65500, Loss: 0.0019239818211644888, \n",
      "Epoch: 66000, Loss: 0.0019025187939405441, \n",
      "Epoch: 66500, Loss: 0.001879572868347168, \n",
      "Epoch: 67000, Loss: 0.0018573695560917258, \n",
      "Epoch: 67500, Loss: 0.0018442103173583746, \n",
      "Epoch: 68000, Loss: 0.0018211827846243978, \n",
      "Epoch: 68500, Loss: 0.0017985039157792926, \n",
      "Epoch: 69000, Loss: 0.0017842620145529509, \n",
      "Epoch: 69500, Loss: 0.001763233682140708, \n",
      "Epoch: 70000, Loss: 0.0017411105800420046, \n",
      "Epoch: 70500, Loss: 0.0017304448410868645, \n",
      "Epoch: 71000, Loss: 0.0017097863601520658, \n",
      "Epoch: 71500, Loss: 0.001695991144515574, \n",
      "Epoch: 72000, Loss: 0.0016756795812398195, \n",
      "Epoch: 72500, Loss: 0.0016603447729721665, \n",
      "Epoch: 73000, Loss: 0.0016442921478301287, \n",
      "Epoch: 73500, Loss: 0.0016290463972836733, \n",
      "Epoch: 74000, Loss: 0.0016133070457726717, \n",
      "Epoch: 74500, Loss: 0.0015992799308151007, \n",
      "Epoch: 75000, Loss: 0.001579604228027165, \n",
      "Epoch: 75500, Loss: 0.0015679539646953344, \n",
      "Epoch: 76000, Loss: 0.0015530944801867008, \n",
      "Epoch: 76500, Loss: 0.0015387125313282013, \n",
      "Epoch: 77000, Loss: 0.0015230522258207202, \n",
      "Epoch: 77500, Loss: 0.0015115310670807958, \n",
      "Epoch: 78000, Loss: 0.0014988419134169817, \n",
      "Epoch: 78500, Loss: 0.0014816472539678216, \n",
      "Epoch: 79000, Loss: 0.0014696801081299782, \n",
      "Epoch: 79500, Loss: 0.00145495415199548, \n",
      "Epoch: 80000, Loss: 0.0014449709560722113, \n",
      "Epoch: 80500, Loss: 0.001434221281670034, \n",
      "Epoch: 81000, Loss: 0.0014170538634061813, \n",
      "Epoch: 81500, Loss: 0.0014048947487026453, \n",
      "Epoch: 82000, Loss: 0.001396734151057899, \n",
      "Epoch: 82500, Loss: 0.0013808537041768432, \n",
      "Epoch: 83000, Loss: 0.0013706223107874393, \n",
      "Epoch: 83500, Loss: 0.001362814218737185, \n",
      "Epoch: 84000, Loss: 0.0013511404395103455, \n",
      "Epoch: 84500, Loss: 0.0013351887464523315, \n",
      "Epoch: 85000, Loss: 0.0013270438648760319, \n",
      "Epoch: 85500, Loss: 0.0013187713921070099, \n",
      "Epoch: 86000, Loss: 0.0013084793463349342, \n",
      "Epoch: 86500, Loss: 0.0012941481545567513, \n",
      "Epoch: 87000, Loss: 0.0012852461077272892, \n",
      "Epoch: 87500, Loss: 0.0012746803695335984, \n",
      "Epoch: 88000, Loss: 0.0012681942898780107, \n",
      "Epoch: 88500, Loss: 0.0012594859581440687, \n",
      "Epoch: 89000, Loss: 0.0012420803541317582, \n",
      "Epoch: 89500, Loss: 0.0012359283864498138, \n",
      "Epoch: 90000, Loss: 0.0012270398437976837, \n",
      "Epoch: 90500, Loss: 0.001218495424836874, \n",
      "Epoch: 91000, Loss: 0.0012108510127291083, \n",
      "Epoch: 91500, Loss: 0.0012005458120256662, \n",
      "Epoch: 92000, Loss: 0.001190779497846961, \n",
      "Epoch: 92500, Loss: 0.001179547980427742, \n",
      "Epoch: 93000, Loss: 0.001171827781945467, \n",
      "Epoch: 93500, Loss: 0.001165188499726355, \n",
      "Epoch: 94000, Loss: 0.0011571240611374378, \n",
      "Epoch: 94500, Loss: 0.001148390700109303, \n",
      "Epoch: 95000, Loss: 0.0011400426737964153, \n",
      "Epoch: 95500, Loss: 0.001132698031142354, \n",
      "Epoch: 96000, Loss: 0.0011260239407420158, \n",
      "Epoch: 96500, Loss: 0.0011176535626873374, \n",
      "Epoch: 97000, Loss: 0.0011106508318334818, \n",
      "Epoch: 97500, Loss: 0.0011001934763044119, \n",
      "Epoch: 98000, Loss: 0.0010945061221718788, \n",
      "Epoch: 98500, Loss: 0.0010835299035534263, \n",
      "Epoch: 99000, Loss: 0.001077619381248951, \n",
      "Epoch: 99500, Loss: 0.0010701098944991827, \n",
      "OrderedDict([('fc1.weight', tensor([[ 0.2956, -2.3485],\n",
      "        [-0.1467,  4.8632]], device='cuda:0')), ('fc1.bias', tensor([ 0.7222, -2.1771], device='cuda:0')), ('fc2.weight', tensor([[-2.9656,  6.3072]], device='cuda:0')), ('fc2.bias', tensor([-1.8895], device='cuda:0'))])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{important}\r\n",
    "In PyTorch, models have a train() method which, somewhat\r\n",
    "disappointingly, does NOT perform a training step. Its only\r\n",
    "purpose is to set the model to **training mode**.\r\n",
    "Why is this important? Some models may use mechanisms like\r\n",
    "Dropout, for instance, which have distinct behaviors during\r\n",
    "training and evaluation phases.\r\n",
    ":::\r\n",
    "\r\n",
    "It is good practice to call `model.train()` in the training loop. It is also possible to set a model to evaluation mode. We will see this in later labs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{admonition} Your Turn\r\n",
    "Put the returned weights and biases into the XOR neural network diagram and try to work out the output when the input is `[0,0]`.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference (Forward Pass)\r\n",
    "\r\n",
    "Instead of verifying it maually, we can test out our XOR model, with input [0,1] by called the model with the input. Note we do not call the forward function directly, instead we provide input to the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model(torch.tensor([0.,1.]).to(device))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.9715], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging the model training for Visualisation in TensorBoard\r\n",
    "\r\n",
    "TensorBoard by TensorFlow is a very useful tool for visualising training progress and model architectures, despite being a competiting platform, PyTorch provides classes and methods for us to integrate it with our model.\r\n",
    "\r\n",
    "TensorBoard can be loaded inside Jupyter notebooks, or can be started externally from a command line. Before we run TensorBoard, we need to change to the directory of your model code, and create a folder, e.g. `runs` to keep a log of the training progress. \r\n",
    "\r\n",
    "The examples in this lab are running Tensorboard inside a notebook, but it is a good idea to run TensorBoard on the command line by giving the log directory. Assuming you are one level up the `runs` directory:\r\n",
    "\r\n",
    "```\r\n",
    "tensorboard --logdir runs\r\n",
    "```\r\n",
    "If successful, it will say that `TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)`, copy and paste the URL to your browser to see TensorBoard in action. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%load_ext tensorboard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorBoard running in Jupyter Notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{figure} ../images/tensorboard_1.png\r\n",
    ":alt: TensorBoard\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 600px\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SummaryWriter\r\n",
    "\r\n",
    "It all starts with the creation of a SummaryWriter. TensorBoard to look for logs inside the `runs` folder, it only makes sense to actually log to that folder. Moreover, to be able to distinguish between different experiments or models, we should also specify a sub-folder: test.\r\n",
    "\r\n",
    "If we do not specify any folder, TensorBoard will default to `runs/CURRENT_DATETIME_HOSTNAME`, which is not such a great name if you’d be looking for your experiment results in the future.\r\n",
    "\r\n",
    "So, it is recommended to try to name it in a more meaningful way, like runs/test or runs/simple_linear_regression. It will then create a subfolder inside runs (the folder we specified when we started TensorBoard).\r\n",
    "\r\n",
    "Even better, you should name it in a meaningful way and add datetime or a sequential number as a suffix, like `runs/test_001` or `runs/test_20200502172130`, to avoid writing data of multiple runs into the same folder (we'll see why this is bad in the add_scalars section below)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "writer = SummaryWriter('runs/test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `add_graph`\r\n",
    "\r\n",
    "It will produce an input-output graph that allows you to interactively inspect parameters, which is different from the TorchViz's computation graph (a static visualisation - not interactive). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "writer.add_graph(model, x_train_tensor)\r\n",
    "%tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{figure} ../images/tensorboard_2.png\r\n",
    ":alt: TensorBoard\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 600px\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `add_scalar`\r\n",
    "\r\n",
    "We can send the loss values to TensorBoard using the `add_scalars` method to send multiple scalar values at once, and it needs three arguments:\r\n",
    "- main_tag: the parent name of the tags or, the \"group tag\"\r\n",
    "- tag_scalar_dict: the dictionary containing the key: value pairs for the scalars you want to keep track of (can be training and validation losses)\r\n",
    "- global_step: step value, that is, the index you're associating with the values you're sending in the dictionary - the epoch comes to mind in our case, as losses are computed for each epoch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "writer.add_scalars(main_tag='loss',\r\n",
    "    tag_scalar_dict={'training': loss,\r\n",
    "                    'validation': loss},\r\n",
    "    global_step=epoch\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you run the code above after performing the model training, it will just send both loss values computed for the last epoch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{figure} ../images/tensorboard_3.png\r\n",
    ":alt: TensorBoard\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 600px\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from datetime import datetime\r\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like\r\n",
    "# Greek letter\r\n",
    "lr = 0.1\r\n",
    "\r\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\r\n",
    "torch.manual_seed(42)\r\n",
    "# Now we can create a model and send it at once to the device\r\n",
    "model = XOR(2,1)\r\n",
    "model = model.to(device)\r\n",
    "\r\n",
    "# Defines a SGD optimizer to update the parameters\r\n",
    "# (now retrieved directly from the model)\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "\r\n",
    "# Defines a MSE loss function\r\n",
    "loss_fn = nn.MSELoss(reduction='mean')\r\n",
    "\r\n",
    "# Tensorboard setup\r\n",
    "writer = SummaryWriter('runs/XOR' + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
    "writer.add_graph(model, x_train_tensor.to(device))\r\n",
    "\r\n",
    "# Defines number of epochs\r\n",
    "n_epochs = 10000\r\n",
    "\r\n",
    "losses = []\r\n",
    "val_losses = [] # note we did not use the validation data\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    #for j in range(steps):\r\n",
    "    model.train() # What is this?!?\r\n",
    "\r\n",
    "    # Step 1 - Computes model's predicted output - forward pass\r\n",
    "    # No more manual prediction!\r\n",
    "    yhat = model(x_train_tensor)\r\n",
    "\r\n",
    "    # Step 2 - Computes the loss\r\n",
    "    loss = loss_fn(yhat, y_train_tensor)\r\n",
    "    \r\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\r\n",
    "    loss.backward()\r\n",
    "\r\n",
    "    # Step 4 - Updates parameters using gradients and\r\n",
    "    # the learning rate\r\n",
    "    optimizer.step()\r\n",
    "    optimizer.zero_grad()\r\n",
    "    if (epoch % 500 == 0):\r\n",
    "        print(\"Epoch: {0}, Loss: {1}, \".\r\n",
    "            format(epoch, loss.to(\"cpu\").detach().numpy()))\r\n",
    "\r\n",
    "    losses.append(loss)\r\n",
    "    writer.add_scalars(main_tag='loss', \r\n",
    "                       tag_scalar_dict={'training': loss,\r\n",
    "                                      'validation': loss},\r\n",
    "                       global_step=epoch)\r\n",
    "\r\n",
    "writer.close()                        \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.27375736832618713, \n",
      "Epoch: 500, Loss: 0.2081705629825592, \n",
      "Epoch: 1000, Loss: 0.09176724404096603, \n",
      "Epoch: 1500, Loss: 0.03289078176021576, \n",
      "Epoch: 2000, Loss: 0.016263967379927635, \n",
      "Epoch: 2500, Loss: 0.009998900815844536, \n",
      "Epoch: 3000, Loss: 0.006978640332818031, \n",
      "Epoch: 3500, Loss: 0.005261328537017107, \n",
      "Epoch: 4000, Loss: 0.00416548689827323, \n",
      "Epoch: 4500, Loss: 0.0034290249459445477, \n",
      "Epoch: 5000, Loss: 0.002894176635891199, \n",
      "Epoch: 5500, Loss: 0.002497166395187378, \n",
      "Epoch: 6000, Loss: 0.0021936693228781223, \n",
      "Epoch: 6500, Loss: 0.0019435270223766565, \n",
      "Epoch: 7000, Loss: 0.0017444714903831482, \n",
      "Epoch: 7500, Loss: 0.0015824229922145605, \n",
      "Epoch: 8000, Loss: 0.001446553273126483, \n",
      "Epoch: 8500, Loss: 0.0013299942947924137, \n",
      "Epoch: 9000, Loss: 0.00122724543325603, \n",
      "Epoch: 9500, Loss: 0.0011413537431508303, \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{figure} ../images/tensorboard_4.png\r\n",
    ":alt: TensorBoard\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 600px\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{note}\r\n",
    "In the TensorBoard logged run, we increased the learning rate and shortened the number of epochs. Play with these two parameters to see what you can get. Or change the loss function to BCE or BCEWithLogits to see how your training loss changes. \r\n",
    ":::"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}