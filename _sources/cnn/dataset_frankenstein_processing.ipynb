{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Frankenstein Dataset At a Glance\r\n",
    "=================================\r\n",
    "\r\n",
    "Here we will build a text dataset from a digitized version of Mary Shelley's novel *Frankenstein*, available via *Project Gutenberg*. This section walks through the preprocessing; building\r\n",
    "a PyTorch Dataset class for this text dataset; and finally splitting the dataset into training,\r\n",
    "validation, and test sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "\r\n",
    "from argparse import Namespace\r\n",
    "import collections\r\n",
    "import nltk.data\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import string\r\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "args = Namespace(\r\n",
    "    raw_dataset_txt=\"../data/books/frankenstein.txt\",\r\n",
    "    window_size=5,\r\n",
    "    train_proportion=0.7,\r\n",
    "    val_proportion=0.15,\r\n",
    "    test_proportion=0.15,\r\n",
    "    output_munged_csv=\"../data/books/frankenstein_with_splits.csv\",\r\n",
    "    seed=1337\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing - Tokenizer\r\n",
    "\r\n",
    "Starting with the raw text file that Project Gutenberg distributes, the preprocessing is minimal: we use NLTK's Punkt tokenizer to split the text into separate sentences, then each sentence is converted to lowercase and the punctuation is completely removed. This preprocessing allows for us to later split strings on whitespace in order to retrieve a list of tokens. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Split the raw text book into sentences\r\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
    "with open(args.raw_dataset_txt) as fp:\r\n",
    "    book = fp.read()\r\n",
    "sentences = tokenizer.tokenize(book)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print (len(sentences), \"sentences\")\r\n",
    "print (\"Sample:\", sentences[100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3427 sentences\n",
      "Sample: No incidents have hitherto befallen us that would make a figure in a\n",
      "letter.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Clean sentences\r\n",
    "def preprocess_text(text):\r\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\r\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\r\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CBOW training data preparation\r\n",
    "\r\n",
    "As we know that CBOW is to use context in a specified window to predict the center word. We need to then to the list of tokens in each sentence and group them into context words in a specified window size, for each token as a center word. \r\n",
    "\r\n",
    ":::{note}\r\n",
    "[tqdm](https://github.com/tqdm/tqdm) instantly make your loops show a smart progress meter - just wrap any iterable with `tqdm(iterable)` in your `for` loop, and you'll see a progress bar when run the loop. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Global vars\r\n",
    "MASK_TOKEN = \"<MASK>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Create windows based on the window_size\r\n",
    "flatten = lambda outer_list: [item for inner_list in outer_list for item in inner_list]\r\n",
    "windows = flatten([list(nltk.ngrams([MASK_TOKEN] * args.window_size + sentence.split(' ') + \\\r\n",
    "    [MASK_TOKEN] * args.window_size, args.window_size * 2 + 1)) \\\r\n",
    "    for sentence in tqdm(cleaned_sentences)])\r\n",
    "\r\n",
    "# Create cbow data (extract target center word and context words)\r\n",
    "data = []\r\n",
    "for window in tqdm(windows):\r\n",
    "    target_token = window[args.window_size]\r\n",
    "    context = []\r\n",
    "    for i, token in enumerate(window):\r\n",
    "        if token == MASK_TOKEN or i == args.window_size:\r\n",
    "            continue\r\n",
    "        else:\r\n",
    "            context.append(token)\r\n",
    "    data.append([' '.join(token for token in context), target_token])\r\n",
    "    \r\n",
    "            \r\n",
    "# Convert to dataframe\r\n",
    "cbow_data = pd.DataFrame(data, columns=[\"context\", \"target\"])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "241f66ea897c4e0586e1803674af6545"
      },
      "text/plain": [
       "  0%|          | 0/3427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb546847f5a04348b9e3a71dcdbadec2"
      },
      "text/plain": [
       "  0%|          | 0/90698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training, Validation and Test data split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Create split data\r\n",
    "n = len(cbow_data)\r\n",
    "def get_split(row_num):\r\n",
    "    if row_num <= n*args.train_proportion:\r\n",
    "        return 'train'\r\n",
    "    elif (row_num > n*args.train_proportion) and (row_num <= n*args.train_proportion + n*args.val_proportion):\r\n",
    "        return 'val'\r\n",
    "    else:\r\n",
    "        return 'test'\r\n",
    "cbow_data['split']= cbow_data.apply(lambda row: get_split(row.name), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "cbow_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the modern prometheus</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern prometheus by</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus by mary</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by mary wo...</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frankenstein , or the prometheus by mary wolls...</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context        target  split\n",
       "0                         , or the modern prometheus  frankenstein  train\n",
       "1           frankenstein or the modern prometheus by             ,  train\n",
       "2       frankenstein , the modern prometheus by mary            or  train\n",
       "3  frankenstein , or modern prometheus by mary wo...           the  train\n",
       "4  frankenstein , or the prometheus by mary wolls...        modern  train"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Write split data to file\r\n",
    "cbow_data.to_csv(args.output_munged_csv, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}