{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolution Basics\r\n",
    "\r\n",
    "## Convolution in Raw Numpy Code\r\n",
    "\r\n",
    "There is one image, one channel, six by six pixels in size (shape $1\\times 1\\times 1 \\times 6 \\times 6). There is one filter, one channel, three by three pixels in size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "single = np.array(\r\n",
    "    [[[[5, 0, 8, 7, 8, 1],\r\n",
    "    [1, 9, 5, 0, 7, 7],\r\n",
    "    [6, 0, 2, 4, 6, 6],\r\n",
    "    [9, 7, 6, 6, 8, 4],\r\n",
    "    [8, 3, 8, 5, 1, 3],\r\n",
    "    [7, 2, 7, 0, 1, 0]]]]\r\n",
    ")\r\n",
    "single.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "filter = np.array(\r\n",
    "    [[[[0, 0, 0],\r\n",
    "    [0, 1, 0],\r\n",
    "    [0, 0, 0]]]]\r\n",
    "    )\r\n",
    "filter.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 1, 3, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "region = single[:, :, 0:3, 0:3]\r\n",
    "filtered_region = region * filter\r\n",
    "total = filtered_region.sum()\r\n",
    "total"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "new_region = single[:, :, 0:3, (0+1):(3+1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "new_filtered_region = new_region * filter\r\n",
    "new_total = new_filtered_region.sum()\r\n",
    "new_total"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "last_horizontal_region * filter"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10920/1958888837.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlast_horizontal_region\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying a filter always produces a single value, the reduction is equal to the filter size minus one when stride is 1. For example, the above matrix is $6\\times 6$, the filter size is $3 \\times 3$, the reduction is $3-1=2$, the the resulting matrix is $4 \\times 4$.\r\n",
    "\r\n",
    "How do we generalise this to a $n \\times m$ matrix, with a filter size $k \\times l$ and a stride $d$. The shape of the newly reduced matrix is then:\r\n",
    "\r\n",
    "$$ h \\times w = n - (k-1) - (d-1) \\times m - (l-1) - (d-1)$$\r\n",
    "\r\n",
    ":::{admonition} Your Turn\r\n",
    "Based on the code above, write a function conv() that takes\r\n",
    "Arguments: \r\n",
    "        input - a matrix to be converted    \r\n",
    "        filter - a filter matrix\r\n",
    "        stride - the number of places to move during convolution\r\n",
    "Returns: result - the resulting matrix after convolution           \r\n",
    ":::\r\n",
    "\r\n",
    "Make sure you take the border conditions into account."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "def conv(input, filter, stride):\r\n",
    "    # Setting up\r\n",
    "    m, n = input.shape[2:4]\r\n",
    "    k, l = filter.shape[2:4]\r\n",
    "    result = np.zeros([1,1,(m-(k-1))//stride, (n-(l-1))//stride])\r\n",
    "    h, w = result.shape[2:4]\r\n",
    "    print(result.shape)\r\n",
    "    row_idx = 0 # keep track of the row index of the output\r\n",
    "    col_idx = 0 # keep track of the column index of the output\r\n",
    "    for i in range(0, n, stride):\r\n",
    "        for j in range(0, m, stride):\r\n",
    "            if((i+k)<=n and (j+l)<=m and col_idx<w and row_idx<h ):\r\n",
    "                result[:,:,row_idx,col_idx] = (input[:, :, i:i+k, j:j+l]*filter).sum()\r\n",
    "                col_idx = col_idx + 1\r\n",
    "        row_idx = row_idx + 1 \r\n",
    "        col_idx = 0     \r\n",
    "    return result            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "conv(single, filter, 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 1, 4, 4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[9., 5., 0., 7.],\n",
       "         [0., 2., 4., 6.],\n",
       "         [7., 6., 6., 8.],\n",
       "         [3., 8., 5., 1.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "conv(single, filter, 2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 1, 2, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[9., 0.],\n",
       "         [7., 6.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolving in PyTorch\r\n",
    "\r\n",
    "`kernel` and `filter` are often used interchangably to refer to the matrix that will be applied over the original input matrix. \r\n",
    "\r\n",
    "Just like the activation functions and loss functions we've seen in earlier labs, convolutions also come in two flavors: functional and module. There is a fundamental difference between the two, though: the functional convolution takes the kernel/filter as an argument while the module has weights to represent the kernel/filter. Let's use the functional convolution, `F.conv2d`, to apply the filter to our input matrix (notice we're using stride=1 since we moved the region around one\r\n",
    "pixel at a time. \r\n",
    "\r\n",
    ":::{note}\r\n",
    "We need to convert Numpy arrays into Torch Tensors before we can make use the `F.conv2d` function. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "image = torch.as_tensor(single).float()\r\n",
    "kernel = torch.as_tensor(filter).float()\r\n",
    "convolved = F.conv2d(image, kernel, stride=1)\r\n",
    "convolved"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's turn our attention to PyTorch's convolution module, `nn.Conv2d`. It has many arguments, let's focus on the first four of them:\r\n",
    "\r\n",
    "- `in_channels`: number of channels of the input image\r\n",
    "- `out_channels`: number of channels produced by the convolution\r\n",
    "- `kernel_size`: size of the (square) convolution filter/kernel\r\n",
    "- `stride`: the size of the movement of the selected region\r\n",
    "\r\n",
    ":::{important}\r\n",
    "- First, there is no argument for the kernel/filter itself, there is only a `kernel_size` argument. This is because the actual filter is learned through training rather than specified. \r\n",
    "\r\n",
    "- Second, it is possible to produce multiple channels as output. It simply means the module is going to learn multiple filters. Each filter is going to produce a different result, which is being called a channel here.\r\n",
    ":::\r\n",
    "\r\n",
    "For a single channel matrix (you can think of 2D matrices are images) as input, and applying one filter (size three by three) to it, moving one pixel at a time, resulting in one output/channel. This is what it looks like in `nn.Conv2d` code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "import torch.nn as nn\r\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\r\n",
    "conv(image)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[-1.0575, -2.6052, -2.2154, -1.7589],\n",
       "          [ 1.7594,  0.5861,  1.2595,  0.6742],\n",
       "          [ 1.0704,  1.4473,  1.1077, -2.0348],\n",
       "          [-1.6991, -1.5751, -1.4593, -3.0604]]]],\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{important}\r\n",
    "These results are randomly initalised weights representing the kernel/filter. Yours will be different from this. \r\n",
    ":::\r\n",
    "\r\n",
    "This is the whole point of the convolutional module: it will learn the kernel/filter on its own. In traditional computer vision, people would develop different filters for different purposes: blurring, sharpening, edge\r\n",
    "detection, and so on. But, instead of being clever and trying to manually devise a filter that does the trick for a given problem, why not outsource the filter definition to the neural network as well? This way the network will come up with filters that highlight features that are relevant to the task at hand. \r\n",
    "\r\n",
    ":::{note} \r\n",
    "The resulting matrix shows a grad_fn attribute now: it will be used to compute gradients so the network can\r\n",
    "actually learn how to change the weights representing the filter.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{admonition} Your Turn\r\n",
    "Try changing the output channels to 2, and see how many filters have been randomly initialised. \r\n",
    ":::\r\n",
    "\r\n",
    "### Force to use a pre-defined filter \r\n",
    "\r\n",
    "We can also force a convolutional module to use a particular filter by setting its\r\n",
    "weights.\r\n",
    "\r\n",
    ":::{important}\r\n",
    "Setting the weights is a strictly no-gradient operation, so you should always use the no_grad context\r\n",
    "manager.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "with torch.no_grad():\r\n",
    "    conv.weight[0] = kernel\r\n",
    "    conv.bias[0] = 0 \r\n",
    "\r\n",
    "conv(image)    "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key hyperparameters in convolution\r\n",
    "\r\n",
    "### Dilation\r\n",
    "\r\n",
    "Instead of a *contiguous* kernel, **dilation** controls the number of rows and columns to skip when the convolutional kernel is applied to the input matrix. In the code below, a dilation of two stretch the $2\\time 2$ kernel into a ``holed`` $3\\times 3$ kernel. The shape of the output feature map is therefore $4\\times 4$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "conv_dilated = nn.Conv2d(in_channels=1, out_channels=1, \r\n",
    "          kernel_size=2, dilation=2, bias=False, stride=1)\r\n",
    "conv_dilated(image)  # [1x1x6x6] tensor as input        "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0450, -0.8075,  2.1801,  1.3727],\n",
       "          [-1.0471,  2.2050,  0.2247, -0.7972],\n",
       "          [ 0.4034, -1.2558,  0.9169,  1.2028],\n",
       "          [ 2.2282,  3.4014,  2.7813,  2.8520]]]],\n",
       "       grad_fn=<SlowConvDilated2DBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Padding\r\n",
    "By adding columns and rows of zeros around it, we expand the input image such that the gray region starts centered in the actual top left corner of the input image. This simple trick can be used to preserve the original size of the\r\n",
    "image. In code, as usual, PyTorch gives us two options: functional (F.pad) and module\r\n",
    "(nn.ConstantPad2d). Let’s start with the module version this time:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "constant_padder = nn.ConstantPad2d(padding=1, value=0)\r\n",
    "constant_padder(image)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can also do asymmetric padding, by specifying a tuple in the padding\r\n",
    "argument representing (left, right, top, bottom). So, if we were to stuff our\r\n",
    "image on left and right sides only, the argument would go like this: (1, 1, 0, 0)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "padded = F.pad(image, pad=(1, 1, 1, 1), mode='constant', value=0)\r\n",
    "padded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are three other modes: replicate, reflect, and circular.\r\n",
    "![Three Modes of Paddings](images/paddings.png \"Three Modes of Paddings\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Replication Padding\r\n",
    "In the replication padding, the padded pixels will have the same value as the\r\n",
    "closest real pixel. The padded corners will have the same value as the real corners.\r\n",
    "The other columns (left and right) and rows (top and bottom) will replicate the\r\n",
    "corresponding values of the original image. The values used in the replication are in\r\n",
    "a darker shade of orange. \r\n",
    "\r\n",
    "In PyTorch, one can use the functional form `F.pad` with `mode=\"replicate\"`, or use\r\n",
    "the module version `nn.ReplicationPad2d`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "replication_padder = nn.ReplicationPad2d(padding=1)\r\n",
    "replication_padder(image)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
       "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "F.pad(image, pad=(1, 1, 1, 1), mode='replicate')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
       "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reflection Padding\r\n",
    "In the reflection padding, the outer columns and rows are used as axes for the reflection. So, the left padded column (forget about the corners for now) will reflect the second column (since the first column is the axis of reflection). The same reasoning goes for the right padded column. Similarly, the top padded row will reflect the second row (since the first row is the axis of reflection), and the same reasoning goes for the bottom padded row. The values used in the reflection are in a darker shade of orange. The corners will have the same values as the intersection of the reflected rows and columns of the original image, or the diagnol value when the original corners are used as a reflection point.\r\n",
    "\r\n",
    "In PyTorch, you can use the functional form `F.pad` with `mode=\"reflect\"`, or use the\r\n",
    "module version `nn.ReflectionPad2d`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "reflection_padder = nn.ReflectionPad2d(padding=1)\r\n",
    "reflection_padder(image)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
       "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
       "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "F.pad(image, pad=(1, 1, 1, 1), mode='reflect')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
       "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
       "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Circular Padding\r\n",
    "In the circular padding, the left-most (right-most) column gets copied as the right\r\n",
    "(left) padded column (forget about the corners for now too). Similarly, the topmost\r\n",
    "(bottom-most) row gets copied as the bottom (top) padded row. The corners\r\n",
    "will receive the values of the diametrically opposed corner: the top-left padded\r\n",
    "pixel receives the value of the bottom-right corner of the original image. Once\r\n",
    "again, the values used in the padding are in a darker shade of orange.\r\n",
    "\r\n",
    "In PyTorch, you must use the functional form `F.pad` with `mode=\"circular\"` since\r\n",
    "there is no module version of the circular padding (at time of writing):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "F.pad(image, pad=(1, 1, 1, 1), mode='circular')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
       "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooling to Shrink Images\r\n",
    "\r\n",
    "Another way of shrinking images is to use **pooling**. It is different from the\r\n",
    "former operations: it splits the image into tiny chunks, performs an operation on\r\n",
    "each chunk (that yields a single value), and puts the chunks together as the\r\n",
    "resulting image. For example, a $6\\times 6$ image can be reduced to $3\\times 3$ by splitting the original image to 9 chunks. \r\n",
    "\r\n",
    "In PyTorch, as usual, we have both forms: `F.max_pool2d` and `nn.MaxPool2d`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "pooled = F.max_pool2d(padded, kernel_size=2)\r\n",
    "pooled"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\cits4012\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[5., 8., 8., 1.],\n",
       "          [6., 9., 7., 7.],\n",
       "          [9., 8., 8., 4.],\n",
       "          [7., 7., 1., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- A pooling kernel of two-by-two results in an image whose dimensions are half of\r\n",
    "the original. \r\n",
    "- A pooling kernel of three-by-three makes the resulting image one third\r\n",
    "the size of the original, and so on. \r\n",
    "\r\n",
    ":::{important}\r\n",
    "only full chunks count: if we try a\r\n",
    "kernel of five-by-five in our eight-by-eight image, only one chunk fits, and the resulting\r\n",
    "image would have a single pixel.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "maxpool5 = nn.MaxPool2d(kernel_size=5)\r\n",
    "pooled5 = maxpool5(padded)\r\n",
    "pooled5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[9.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Besides max-pooling, average pooling is also fairly common. As the name\r\n",
    "suggests, it will output the average pixel value for each chunk. In PyTorch, we have\r\n",
    "`F.avg_pool2d` and `nn.AvgPool2d`.\r\n",
    "\r\n",
    "You can also use a stride greater than 1 just like applying a convolution kernel: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "padded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "F.max_pool2d(padded, kernel_size=2, stride=2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[5., 8., 8., 1.],\n",
       "          [6., 9., 7., 7.],\n",
       "          [9., 8., 8., 4.],\n",
       "          [7., 7., 1., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flatterning \r\n",
    "It simply flattens a tensor, by appending the values of each row to its previous row. It has a module version `nn.Flatten()`. There is no functional version of it, as it can achieved by the `view()` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "pooled\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[5., 8., 8., 1.],\n",
       "          [6., 9., 7., 7.],\n",
       "          [9., 8., 8., 4.],\n",
       "          [7., 7., 1., 0.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "flatterned = nn.Flatten()(pooled)\r\n",
    "flatterned"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5., 8., 8., 1., 6., 9., 7., 7., 9., 8., 8., 4., 7., 7., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "pooled.view(1,-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5., 8., 8., 1., 6., 9., 7., 7., 9., 8., 8., 4., 7., 7., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1D Convolution\r\n",
    "\r\n",
    "Give a sequence of daily temperature readings, let's use a window (filter) of size five, like in the figure below. In its first step, the window is over days one to five. In the next step, since it can only move to the right, it will be over days two to six. The size of our movement to the\r\n",
    "right is, the stride. \r\n",
    "\r\n",
    "Now, let's assign the same value (0.2 = $\\frac{1}{5}$) for every weight in our filter and use\r\n",
    "PyTorch's F.conv1d to convolve the filter with our sequence (the shape is the NCL: the **N**umber of sequences, the number of **C**hannels and the **L**ength of the sequence.):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "\r\n",
    "temperatures = np.array([5, 11, 15, 6, 5, 3, 3, 0, 0, 3, 4, 2, 1])\r\n",
    "size = 5\r\n",
    "weight = torch.ones(size) * 0.2\r\n",
    "F.conv1d(torch.as_tensor(temperatures).float().view(1, 1, -1),\r\n",
    "         weight=weight.view(1, 1, -1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[8.4000, 8.0000, 6.4000, 3.4000, 2.2000, 1.8000, 2.0000, 1.8000,\n",
       "          2.0000]]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a moving average, as we have the same weight ($\\frac{1}{window\\_size}$) for the weighted sum."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "weight"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch.nn as nn\r\n",
    "torch.manual_seed(17)\r\n",
    "conv_seq = nn.Conv1d(in_channels=10, out_channels=100,\r\n",
    "    kernel_size=2, bias=False)\r\n",
    "conv_seq.weight, conv_seq.weight.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[-0.0294,  0.0157],\n",
       "          [ 0.1477, -0.1682],\n",
       "          [-0.2105,  0.0221],\n",
       "          ...,\n",
       "          [-0.0651, -0.1261],\n",
       "          [-0.2236, -0.1607],\n",
       "          [ 0.0448,  0.0294]],\n",
       " \n",
       "         [[-0.1812, -0.1802],\n",
       "          [-0.0575, -0.2116],\n",
       "          [-0.1365,  0.1271],\n",
       "          ...,\n",
       "          [ 0.2072,  0.0923],\n",
       "          [ 0.1267,  0.1031],\n",
       "          [-0.0246, -0.0949]],\n",
       " \n",
       "         [[-0.1262, -0.1012],\n",
       "          [ 0.1101, -0.0824],\n",
       "          [-0.0500,  0.1758],\n",
       "          ...,\n",
       "          [-0.1602,  0.0048],\n",
       "          [ 0.1420, -0.1564],\n",
       "          [ 0.1490, -0.1395]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1480, -0.1971],\n",
       "          [-0.1273,  0.0995],\n",
       "          [-0.0726, -0.1250],\n",
       "          ...,\n",
       "          [ 0.1296,  0.1590],\n",
       "          [-0.0313,  0.1204],\n",
       "          [-0.1540, -0.0166]],\n",
       " \n",
       "         [[ 0.1235,  0.2034],\n",
       "          [ 0.0404, -0.0861],\n",
       "          [ 0.0900, -0.1462],\n",
       "          ...,\n",
       "          [ 0.0307,  0.1345],\n",
       "          [-0.1840, -0.1889],\n",
       "          [-0.0731,  0.0381]],\n",
       " \n",
       "         [[ 0.1546, -0.0249],\n",
       "          [-0.1551,  0.0903],\n",
       "          [-0.0173, -0.1090],\n",
       "          ...,\n",
       "          [ 0.1704,  0.2202],\n",
       "          [-0.2147,  0.2092],\n",
       "          [-0.1115,  0.1248]]], requires_grad=True),\n",
       " torch.Size([100, 10, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}