{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Linear Models in Pytorch\r\n",
    "=========================\r\n",
    "\r\n",
    "Now that we have some basic knowledge of Torch tensors, let's see how we can implement the linear model earlier using Pytorch.\r\n",
    "\r\n",
    "## Data Preparation\r\n",
    "\r\n",
    "Let's first repeat the same data preparation process, first generating a synthetic dataset of 100 data points, then perform an 80%:20% split as training and validation dataset, respectively. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "true_b = 1\r\n",
    "true_w = 2\r\n",
    "N = 100\r\n",
    "# Data Generation\r\n",
    "np.random.seed(42)\r\n",
    "x = np.random.rand(N, 1)\r\n",
    "# Guassian noise to add some randomness to y\r\n",
    "epsilon = (.1 * np.random.randn(N, 1))\r\n",
    "y = true_b + true_w * x + epsilon\r\n",
    "\r\n",
    "# Shuffles the indices\r\n",
    "idx = np.arange(N)\r\n",
    "np.random.shuffle(idx)\r\n",
    "# Uses first 80 random indices for train\r\n",
    "train_idx = idx[:int(N*.8)]\r\n",
    "# Uses the remaining indices for validation\r\n",
    "val_idx = idx[int(N*.8):]\r\n",
    "# Generates train and validation sets\r\n",
    "x_train, y_train = x[train_idx], y[train_idx]\r\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No matter you have a GPU or not, the best practice is to use `.to(device)` method to make your code GPU ready. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "# Our data was in Numpy arrays, but we need to transform them\r\n",
    "# into PyTorch's Tensors and then we send them to the\r\n",
    "# chosen device\r\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\r\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Here we can see the difference - notice that .type() is more\r\n",
    "# useful since it also tells us WHERE the tensor is (device)\r\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We normally can turn a tensor back to a *Numpy* array using `sourceTensor.numpy()`, but now we have GPU tensor, which cannot be directly handled by Numpy. We have turn it back to a CPU tensor first before converting to a Numpy array. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "back_to_numpy = x_train_tensor.cpu().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{admonition} Good Practice\r\n",
    ":class: tip\r\n",
    "It is a good practice to always first `cpu()` and then `numpy()`, even if you are using a CPU. It follows the same principle of to(device): you may share your code with others who may be using a GPU.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Parameters\r\n",
    "\r\n",
    "What distinguishes a tensor used for training data (or validation, or test) — like the ones we've just created — from a tensor used as a (trainable) parameter/weight?\r\n",
    "\r\n",
    "The latter (a parameter) requires the computation of its gradients, so we can update their values (the parameters' values). \r\n",
    "\r\n",
    "In Pytorch, we use the  `requires_grad=True` argument to tell PyTorch to compute gradients for us.\r\n",
    "\r\n",
    "**A tensor for a learnable parameter requires a gradient!**\r\n",
    "\r\n",
    ":::{admonition} Good Practice\r\n",
    ":class: tip\r\n",
    "To make GPU ready code, we should specify the device at the moment of creation to avoid shadowing the gradient requirement. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# We can specify the device at the moment of creation\r\n",
    "# RECOMMENDED!\r\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\r\n",
    "torch.manual_seed(42)\r\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\r\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\r\n",
    "print(b, w)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{note}\r\n",
    "Notice that even with the same seed value, because *Pytorch* and *Numpy* are two different packages, they have different implemenations of the `randn()` method, thus different results. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autograd\r\n",
    "In Pytorch, we don't need to worry about partial derivatives, chain rule, or anything like it. Autograd is PyTorch's automatic differentiation package.\r\n",
    "\r\n",
    "### `backward()`\r\n",
    "To tell PyTorch to compute all gradients, we use the `backward()` method. It will compute gradients for all (requiring gradient) tensors involved in the computation of a given variable.\r\n",
    "\r\n",
    "Recall that we need to compute the partial derivatives of the loss function w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable: `loss.backward()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\r\n",
    "yhat = b + w * x_train_tensor\r\n",
    "# Step 2 - Computes the loss\r\n",
    "# We are using ALL data points, so this is BATCH gradient\r\n",
    "# descent. How wrong is our model? That's the error!\r\n",
    "error = (yhat - y_train_tensor)\r\n",
    "# It is a regression, so it computes mean squared error (MSE)\r\n",
    "loss = (error ** 2).mean()\r\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\r\n",
    "# No more manual computation of gradients!\r\n",
    "# b_grad = 2 * error.mean()\r\n",
    "# w_grad = 2 * (x_tensor * error).mean()\r\n",
    "loss.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have set `requires_grad=True` to both `b` and `w`, so they are obviously included in the list of gradient calculation. We use them both to compute `yhat`, so it will also make it to the list. Then we use `yhat` to compute the `error`, so `error` is also on the list.\r\n",
    "\r\n",
    "`x_train_tensor` and `y_train_tensor` however, are not gradient-requiring tensors, so `backward()` does not care about them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(error.requires_grad, yhat.requires_grad, b.requires_grad, w.requires_grad)\r\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### grad\r\n",
    "\r\n",
    "We can inspect the actual values of the gradients by looking at the `grad` attribute of a tensor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(b.grad, w.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-3.3881], device='cuda:0') tensor([-1.9439], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accumulated Gradients\r\n",
    "\r\n",
    "Let's run the backward function again:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\r\n",
    "yhat = b + w * x_train_tensor\r\n",
    "# Step 2 - Computes the loss\r\n",
    "# We are using ALL data points, so this is BATCH gradient\r\n",
    "# descent. How wrong is our model? That's the error!\r\n",
    "error = (yhat - y_train_tensor)\r\n",
    "# It is a regression, so it computes mean squared error (MSE)\r\n",
    "loss = (error ** 2).mean()\r\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\r\n",
    "# No more manual computation of gradients!\r\n",
    "# b_grad = 2 * error.mean()\r\n",
    "# w_grad = 2 * (x_tensor * error).mean()\r\n",
    "loss.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(b.grad, w.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-6.7762], device='cuda:0') tensor([-3.8878], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{note}\r\n",
    "If we ran this above code again, the gradient of $b$ and $w$ exactly doubled. This is because Pytorch implements an accumlated gradients to circumvent hardware limitations. If a minibatch is still too big to fit in memory, we can split it further into \"subminibatch\", that's when the aggregated gradients become useful.  \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### zero_\r\n",
    "\r\n",
    "For training problem that does not have memory limitations, every time we use the gradients to update the parameters, we need to zero the\r\n",
    "gradients afterward. This what `zero_()` is good for."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# This code will be placed _after_ Step 4\r\n",
    "# (updating the parameters)\r\n",
    "b.grad.zero_(), w.grad.zero_()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{important}\r\n",
    "In PyTorch, every method that ends with an underscore (_), like\r\n",
    "the `requires_grad_()` and `zero_()` method above, makes changes in-place, \r\n",
    "in other words, they will modify the underlying variable.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Put it all together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\r\n",
    "lr = 0.1\r\n",
    "\r\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\r\n",
    "torch.manual_seed(42)\r\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\r\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\r\n",
    "\r\n",
    "# Defines number of epochs\r\n",
    "n_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    # Step 1 - Computes model's predicted output - forward pass\r\n",
    "    yhat = b + w * x_train_tensor\r\n",
    "\r\n",
    "    # Step 2 - Computes the loss\r\n",
    "    # We are using ALL data points, so this is BATCH gradient\r\n",
    "    # descent. How wrong is our model? That's the error!\r\n",
    "    error = (yhat - y_train_tensor)\r\n",
    "    # It is a regression, so it computes mean squared error (MSE)\r\n",
    "    loss = (error ** 2).mean()\r\n",
    "\r\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\"\r\n",
    "    # parameters. No more manual computation of gradients!\r\n",
    "    # b_grad = 2 * error.mean()\r\n",
    "    # w_grad = 2 * (x_tensor * error).mean()\r\n",
    "    # We just tell PyTorch to work its way BACKWARDS\r\n",
    "    # from the specified loss!\r\n",
    "    loss.backward()\r\n",
    "\r\n",
    "    # Step 4 - Updates parameters using gradients and\r\n",
    "    # the learning rate. But not so fast...\r\n",
    "    # FIRST ATTEMPT - just using the same code as before\r\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\r\n",
    "    # b = b - lr * b.grad\r\n",
    "    # w = w - lr * w.grad\r\n",
    "    # print(b)\r\n",
    "\r\n",
    "    # SECOND ATTEMPT - using in-place Python assingment\r\n",
    "    # RuntimeError: a leaf Variable that requires grad\r\n",
    "    # has been used in an in-place operation.\r\n",
    "    # b -= lr * b.grad\r\n",
    "    # w -= lr * w.grad\r\n",
    "\r\n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\r\n",
    "    # We need to use NO_GRAD to keep the update out of\r\n",
    "    # the gradient computation. Why is that? It boils\r\n",
    "    # down to the DYNAMIC GRAPH that PyTorch uses...\r\n",
    "    with torch.no_grad():\r\n",
    "        b -= lr * b.grad\r\n",
    "        w -= lr * w.grad\r\n",
    "\r\n",
    "    # PyTorch is \"clingy\" to its computed gradients, we\r\n",
    "    # need to tell it to let it go...\r\n",
    "    b.grad.zero_()\r\n",
    "    w.grad.zero_()\r\n",
    "\r\n",
    "print(b, w)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the first attempt, if we use the same update structure as in our Numpy code, we'll get a weird error but we can get a hint of what's going on by looking at\r\n",
    "the tensor itself — once again, we \"lost\" the gradient while reassigning the update\r\n",
    "results to our parameters. Thus, the grad attribute turns out to be None, and it\r\n",
    "raises the error… \r\n",
    "\r\n",
    ":::{important}\r\n",
    "We use `with torch.no_grad():` to ensure the update is not tracked by the *dyanmic computation graph* mechanism of Pytorch. We will talk about compuation graph next lab. \r\n",
    ":::"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}