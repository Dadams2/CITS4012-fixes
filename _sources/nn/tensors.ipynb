{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Introduction to Pytorch Tensors\r\n",
    "===============================\r\n",
    "\r\n",
    "Pytorch is an optimised `tensor` manipulation library that offers an array of packages for deep learning. As compared to static frameworks such as Theano, Caffe and Tensorflow, Pytorch is in the family of dynamic frameworks, which does not require pre-defined computational graphs. This allows for a more flexible, imperative style of development, as it does not require the computational graphs to be first declared, compiled, and then excuted. However, this is potentially at the cost of computational efficiency, which makes it not as advantageous for production and mobile settings, but extremely useful during research and development.\r\n",
    "\r\n",
    "```{image} ../images/nlp_pytorch_book.jpg\r\n",
    ":alt: Pytorch for NLP Book\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 200px\r\n",
    ":align: left\r\n",
    "```\r\n",
    "```{image} ../images/logo_pytorch.jpeg\r\n",
    ":alt: Pytorch Logo\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 100px\r\n",
    ":align: right\r\n",
    "```\r\n",
    "\r\n",
    "Reference: *Natural Lanuage Processing with PyTorch* - Building intelligent lanaguage applications using deep learning, by Delip Rao and Brian McMahan (copyright O'REILLY Feb 2019)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors\r\n",
    "\r\n",
    "```{admonition} Tensor\r\n",
    "A tensor is a mathematical object holding some multidimensional data. \r\n",
    "```\r\n",
    "\r\n",
    "```{image} ../images/tensor.png\r\n",
    ":alt: Tensors\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 80%\r\n",
    ":align: center\r\n",
    "```\r\n",
    "* A tensor of order zero is just a number, or a `scalar`.\r\n",
    "* A tensor of order one (1st-order tensor) is an array of numbers, or a `vector`.\r\n",
    "* A tensor of order two (2nd-order tensor) is an array of vectors, or a `matrix`.\r\n",
    "* A tenosr of order n (nth-order tensor) is a generalised n-dimensional array of scalars. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Tensors\r\n",
    "\r\n",
    "You can create tensors in PyTorch pretty much the same way you create arrays in\r\n",
    "*Numpy*. Using `tensor()` you can create either a scalar or a tensor.\r\n",
    "PyTorch's tensors have equivalent functions as its Numpy counterparts, like:\r\n",
    "`ones()`, `zeros()`, `rand()`, `randn()` and many more. In the example below, we create one of each: scalar, vector, matrix and tensor or, saying it differently, one scalar and three tensors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "scalar = torch.tensor(3.14159)\r\n",
    "vector = torch.tensor([1, 2, 3])\r\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\r\n",
    "# two (2) 3x4 matrices\r\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\r\n",
    "print(scalar)\r\n",
    "print(vector)\r\n",
    "print(matrix)\r\n",
    "print(tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[-0.1062,  0.0259,  1.0003,  1.7506],\n",
      "         [ 1.5519,  0.5936,  0.0977,  1.0550],\n",
      "         [-0.8436, -0.2855,  0.0348, -1.2236]],\n",
      "\n",
      "        [[-0.7193, -1.7311,  0.3547, -2.5190],\n",
      "         [ 0.6258, -0.6508, -0.4601, -0.4112],\n",
      "         [ 0.6510,  0.2963, -0.1989,  0.3999]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A helper function `describe(x)`\r\n",
    "Given a torch tensor `x`, we can use either `x.size()` function or `x.shape` property to look at the dimensionality of the torch tensor.\r\n",
    "\r\n",
    ":::{note} \r\n",
    "`tensor.shape` is a property, not a callable function, whereas `tensor.size()` is a function.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def describe(x):\r\n",
    "    print(\"Type:{}\".format(x.type()))\r\n",
    "    print(\"Shape:{}\".format(x.shape))\r\n",
    "    print(\"Size():{}\".format(x.size()))\r\n",
    "    print(\"Values: \\n{}\".format(x))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "describe(scalar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([])\n",
      "Size():torch.Size([])\n",
      "Values: \n",
      "3.141590118408203\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "describe(vector)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape:torch.Size([3])\n",
      "Size():torch.Size([3])\n",
      "Values: \n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "describe(matrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "describe(tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3, 4])\n",
      "Size():torch.Size([2, 3, 4])\n",
      "Values: \n",
      "tensor([[[-0.1062,  0.0259,  1.0003,  1.7506],\n",
      "         [ 1.5519,  0.5936,  0.0977,  1.0550],\n",
      "         [-0.8436, -0.2855,  0.0348, -1.2236]],\n",
      "\n",
      "        [[-0.7193, -1.7311,  0.3547, -2.5190],\n",
      "         [ 0.6258, -0.6508, -0.4601, -0.4112],\n",
      "         [ 0.6510,  0.2963, -0.1989,  0.3999]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a tensor with `torch.Tensor()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "describe(torch.Tensor(2,3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.0000e+00, 0.0000e+00, 2.1019e-44],\n",
      "        [0.0000e+00, 1.4013e-45, 0.0000e+00]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{warning}\r\n",
    "You might have noted that we can create tensors using both `torch.tensor()` and `torch.Tensor()`, note the subtle difference between the case of letter \"t\".\r\n",
    "\r\n",
    "`torch.Tensor` is an alias for `torch.FloatTensor`, which creates tensors of float type. \r\n",
    "\r\n",
    "`torch.tensor` on the other hand, infers the `dtype` automatically, and allows explicit specification of `dtype` during creation. \r\n",
    "\r\n",
    "So let's stick to `torch.tensor` instead. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a randomly initialized tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\r\n",
    "\r\n",
    "describe(torch.rand(2,3))   # uniform random\r\n",
    "describe(torch.randn(2,3))  # normal random"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.2609, 0.1867, 0.2250],\n",
      "        [0.7788, 0.2673, 0.5694]])\n",
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 0.5022, -1.1496, -0.6783],\n",
      "        [ 0.7880, -0.0197,  1.7654]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a filled tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\r\n",
    "\r\n",
    "describe(torch.zeros(2,3))\r\n",
    "\r\n",
    "x = torch.ones(2,3)\r\n",
    "describe(x)\r\n",
    "\r\n",
    "x.fill_(5)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and initialising a tensor from lists\r\n",
    "\r\n",
    "Observe the type difference between the `torch.tensor()` and `torch.Tensor()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x = torch.tensor([[1, 2, 3],\r\n",
    "                  [4, 5, 6]])\r\n",
    "describe(x)                  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "x = torch.tensor([[1, 2, 3],\r\n",
    "                  [4, 5, 6]])\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape:torch.Size([2, 3])\n",
      "Size():torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and initialising a tensor from Numpy\r\n",
    "\r\n",
    "`from_numpy()` automatically inherits input array dtype. On the other hand, `torch.Tensor` is an alias for `torch.FloatTensor`.\r\n",
    "\r\n",
    "Therefore, if you pass int32 array to torch.Tensor, output tensor is float tensor and they wouldn't share the storage. torch.from_numpy gives you torch.LongTensor as expected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "a = np.arange(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "describe(torch.from_numpy(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.IntTensor\n",
      "Shape:torch.Size([10])\n",
      "Size():torch.Size([10])\n",
      "Values: \n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "describe(torch.Tensor(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape:torch.Size([10])\n",
      "Size():torch.Size([10])\n",
      "Values: \n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "describe(torch.tensor(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.IntTensor\n",
      "Shape:torch.Size([10])\n",
      "Size():torch.Size([10])\n",
      "Values: \n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subtleties in Torch Memory Model\r\n",
    "\r\n",
    "Another way of creating tensor is to use `torch.as_tensor()`. What's the difference between `torch.as_tensor()` and `torch.tensor()`?\r\n",
    "\r\n",
    ":::{tabbed} `torch.tensor()`\r\n",
    "`torch.tensor` always copies the data. For example, `torch.tensor(x)` is equivalent to `x.clone().detach()`.\r\n",
    ":::\r\n",
    "\r\n",
    ":::{tabbed} `torch.as_tensor()`\r\n",
    "`torch.as_tensor` always tries to avoid copies of the data. One of the cases where `as_tensor` avoids copying the data is if the original data is a numpy array.\r\n",
    ":::\r\n",
    "\r\n",
    "\r\n",
    "It is not always necessary or a good idea to copy and create a new tensor, especially when the tensor is large. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reshaping a tensor\r\n",
    "\r\n",
    "The same subtle difference exists between the methods that change the shape of a tensor, i.e. `view()` and `reshape()`. \r\n",
    "\r\n",
    ":::{important}\r\n",
    "The `view()` method only returns a tensor with the\r\n",
    "desired shape that shares the underlying data with the original\r\n",
    "tensor - it **DOES NOT** create a new, independent, tensor!\r\n",
    "\r\n",
    "The `reshape()` method **may or may not** create a copy! The\r\n",
    "reasons behind this apparently weird behavior are beyond the\r\n",
    "scope of this section - but this behavior is the reason why view()\r\n",
    "is preferred :-)\r\n",
    ":::\r\n",
    "\r\n",
    "Why does it matter? Using `view()`, we get the same tensor with a different shape, any modification to the reshaped tensor will change the original tensor. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# We get a tensor with a different shape but it still is\r\n",
    "# the SAME tensor\r\n",
    "same_matrix = matrix.view(1, 6)\r\n",
    "# If we change one of its elements...\r\n",
    "same_matrix[0, 1] = 2.\r\n",
    "# It changes both variables: matrix and same_matrix\r\n",
    "print(matrix)\r\n",
    "print(same_matrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To copy all data into a separate independent tensor in the memory, so future operations will not affect the orignal, we will need to use `new_tensor()` or `clone()` methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# We can use \"new_tensor\" method to REALLY copy it into a new one\r\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\r\n",
    "# Now, if we change one of its elements...\r\n",
    "different_matrix[0, 1] = 3.\r\n",
    "# The original tensor (matrix) is left untouched!\r\n",
    "# But we get a \"warning\" from PyTorch telling us\r\n",
    "# to use \"clone()\" instead!\r\n",
    "print(matrix)\r\n",
    "print(different_matrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\wei\\AppData\\Local\\Temp/ipykernel_10816/887542709.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  different_matrix = matrix.new_tensor(matrix.view(1, 6))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{warning}\r\n",
    "As from the warning message, we in fact should use `matrix.clone().detach()` to copy a tensor into a new duplicate, rather than `matrix.new_tensor()`.\r\n",
    ":::\r\n",
    "\r\n",
    "In summary, the preferred functions are:\r\n",
    "- from a list of values, use `torch.tensor(values, dtype=\"\")`\r\n",
    "- from numpy, use `torch.from_numpy()` or `torch.as_tensor()` to avoid data copying.\r\n",
    "- from numpy, use `torch.tensor()` to copy it into a new tensor. \r\n",
    "- from an existing tensor, `sourceTensor.view()` to avoid copying.\r\n",
    "- from an existing tensor, `sourceTensor.clone().detach()` for a fresh duplicate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor Slicing, Indexing and Joining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\r\n",
    "from functions import describe\r\n",
    "\r\n",
    "x = torch.arange(6).view(2,3)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contiguous Indexing using `[:a, :b]`\r\n",
    "\r\n",
    "The code below accesses up to row 1 but not including row 1, and up to col 2, but no including col 2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "describe(x[:1, :2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([1, 2])\n",
      "Values: \n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Noncontiguous Indexing\r\n",
    "\r\n",
    "Using function `torch.index_select()`, the code below accesses column (`dim=1`) indexed by 0 and 2. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "indices = torch.LongTensor([0, 2])\r\n",
    "describe(torch.index_select(x, dim=1, index=indices))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[0, 2],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can duplicate the same row or column multiple times, by specifying the same index multiple times. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "indices = torch.LongTensor([0, 0, 0])\r\n",
    "describe(torch.index_select(x, dim=0, index=indices))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([3, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use indices directly `[inices_list, indices_list]` can also achieve the same outcome."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "row_indices = torch.arange(2).long()\r\n",
    "col_indices = torch.LongTensor([0,2])\r\n",
    "describe(x[row_indices, col_indices])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values: \n",
      "tensor([0, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "describe(x[[0,1], [0,2]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values: \n",
      "tensor([0, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concatenating Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x = torch.arange(6).view(2,3)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "describe(torch.cat([x, x], dim=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([4, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "describe(torch.cat([x, x], dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 6])\n",
      "Values: \n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "describe(torch.stack([x, x], dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2, 3])\n",
      "Values: \n",
      "tensor([[[0, 1, 2],\n",
      "         [0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Algebra on tensors: multiplication"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x1 = torch.arange(6).view(2,3).float()\r\n",
    "describe(x1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{warning}\r\n",
    "`torch.arange()` creates LongTensor, for `torch.mm()`, we need to convert the LongTensor to FloatTensor by using `x.float()`.\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "x2 = torch.ones(3,2)\r\n",
    "x2[:, 1] += 1\r\n",
    "describe(x2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "describe(torch.mm(x1, x2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[ 3.,  6.],\n",
      "        [12., 24.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CUDA tensors\r\n",
    "\r\n",
    "So far, we have only created CPU tensors. What does it mean? It means the data in\r\n",
    "the tensor is stored in the computer’s main memory and any operations performed\r\n",
    "on it are going to be handled by its CPU (the Central Processing Unit, for instance,\r\n",
    "an Intel® Core™ i7 Processor). So, although the data is, technically speaking, in the memory, we're still calling this kind of tensor a CPU tensor.\r\n",
    "\r\n",
    "A GPU (which stands for Graphics Processing Unit)\r\n",
    "is the processor of a graphics card. These tensors store their data in the graphics\r\n",
    "card’s memory and operations on top of them are performed by the GPU.\r\n",
    "\r\n",
    "If you have a graphics card from NVIDIA, you can use the power of its GPU to speed up model training. PyTorch supports the use of these GPUs for model\r\n",
    "training using CUDA (Compute Unified Device Architecture), which needs to be\r\n",
    "previously installed and configured (please refer to the Setup Guide for more\r\n",
    "information on this).\r\n",
    "\r\n",
    "If you do have a GPU (and you managed to install CUDA), we're getting to the part\r\n",
    "where you get to use it with PyTorch. But, even if you do not have a GPU, you\r\n",
    "should stick around in this section anyway… why? First, you can use a free GPU\r\n",
    "from Google Colab and, second, you should always make your code GPU-ready,\r\n",
    "that is, it should automatically run in a GPU, if one is available.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# prefered method: device agnostic tensor instantiation\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, if you don’t have a GPU, your device is called cpu. If you do have a **GPU**, your\r\n",
    "device is called **cuda** or **cuda:0**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have multiple GPUs, and want to check how many GPUs it\r\n",
    "has, or which model they are, you can figure it out using `cuda.device_count()` and\r\n",
    "`cuda.get_device_name()`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "n_cudas = torch.cuda.device_count()\r\n",
    "for i in range(n_cudas):\r\n",
    "    print(torch.cuda.get_device_name(i))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use `.to(device)` to turn our tensor to a GPU tensor if you have a GPU device."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "x = torch.rand(3,2).to(device)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.cuda.FloatTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[0.3817, 0.3665],\n",
      "        [0.9877, 0.7927],\n",
      "        [0.9034, 0.5782]], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{warning}\r\n",
    "Mixing CUDA tensors with CPU-bound tensors will lead to errors. This is because we need to ensure the tensors are on the same device. \r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "y = torch.rand(3,2)\r\n",
    "x + y"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10816/1126695582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cpu_device = torch.device(\"cpu\")\r\n",
    "x = x.to(cpu_device)\r\n",
    "y = y.to(cpu_device)\r\n",
    "x + y"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tensor([[0.6276, 0.9583],\n",
       "        [0.7592, 1.2605],\n",
       "        [1.0946, 0.9480]])"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{note}\r\n",
    "It is expensive to move data back and forth from the GPU. Best practice is to carry out as much computation on GPU as possible and then just transfering the final results to CPU. \r\n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}