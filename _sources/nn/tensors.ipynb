{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Introduction to Pytorch Tensors\r\n",
    "===============================\r\n",
    "\r\n",
    "Pytorch is an optimised `tensor` manipulation library that offers an array of packages for deep learning. As compared to static frameworks such as Theano, Caffe and Tensorflow, Pytorch is in the family of dynamic frameworks, which does not require pre-defined computational graphs. This allows for a more flexible, imperative style of development, as it does not require the computational graphs to be first declared, compiled, and then excuted. However, this is potentially at the cost of computational efficiency, which makes it not as advantageous for production and mobile settings, but extremely useful during research and development.\r\n",
    "\r\n",
    "```{image} ../images/nlp_pytorch_book.jpg\r\n",
    ":alt: Pytorch for NLP Book\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 200px\r\n",
    ":align: left\r\n",
    "```\r\n",
    "```{image} ../images/logo_pytorch.jpeg\r\n",
    ":alt: Pytorch Logo\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 100px\r\n",
    ":align: right\r\n",
    "```\r\n",
    "\r\n",
    "Reference: *Natural Lanuage Processing with PyTorch* - Building intelligent lanaguage applications using deep learning, by Delip Rao and Brian McMahan (copyright O'REILLY Feb 2019)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors\r\n",
    "\r\n",
    "```{admonition} Tensor\r\n",
    "A tensor is a mathematical object holding some multidimensional data. \r\n",
    "```\r\n",
    "\r\n",
    "```{image} ../images/tensor.png\r\n",
    ":alt: Tensors\r\n",
    ":class: bg-primary mb-1\r\n",
    ":width: 80%\r\n",
    ":align: center\r\n",
    "```\r\n",
    "* A tensor of order zero is just a number, or a `scalar`.\r\n",
    "* A tensor of order one (1st-order tensor) is an array of numbers, or a `vector`.\r\n",
    "* A tensor of order two (2nd-order tensor) is an array of vectors, or a `matrix`.\r\n",
    "* A tenosr of order n (nth-order tensor) is a generalised n-dimensional array of scalars. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A helper function `describe(x)`\r\n",
    "`x` is a torch tensor\r\n",
    "\r\n",
    "NOTE: `tensor.shape` is a property, not a callable function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def describe(x):\r\n",
    "    print(\"Type:{}\".format(x.type()))\r\n",
    "    print(\"Shape/size:{}\".format(x.shape))\r\n",
    "    print(\"Values: \\n{}\".format(x))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a tensor with `torch.Tensor()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "\r\n",
    "describe(torch.Tensor(2,3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1.0862e+21, 3.5241e-40, 1.0879e+21],\n",
      "        [3.5241e-40, 1.0863e+21, 3.5241e-40]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a randomly initialized tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\r\n",
    "\r\n",
    "describe(torch.rand(2,3))   # uniform random\r\n",
    "describe(torch.randn(2,3))  # normal random"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.7992, 0.9784, 0.0448],\n",
      "        [0.9578, 0.0454, 0.0026]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 0.3601, -0.7138, -1.8004],\n",
      "        [-0.2314, -0.3745,  0.3433]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a filled tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "\r\n",
    "describe(torch.zeros(2,3))\r\n",
    "\r\n",
    "x = torch.ones(2,3)\r\n",
    "describe(x)\r\n",
    "\r\n",
    "x.fill_(5)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and initialising a tensor from lists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "x = torch.Tensor([[1, 2, 3],\r\n",
    "                  [4, 5, 6]])\r\n",
    "describe(x)                  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and initialising a tensor from Numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "npy = np.random.rand(2,3)\r\n",
    "describe(torch.from_numpy(npy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.DoubleTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.6475, 0.9183, 0.0888],\n",
      "        [0.1407, 0.9231, 0.8320]], dtype=torch.float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor Slicing, Indexing and Joining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch\r\n",
    "from functions import describe\r\n",
    "\r\n",
    "x = torch.arange(6).view(2,3)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contiguous Indexing using `[:a, :b]`\r\n",
    "\r\n",
    "The code below accesses up to row 1 but not including row 1, and up to col 2, but no including col 2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "describe(x[:1, :2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([1, 2])\n",
      "Values: \n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Noncontiguous Indexing\r\n",
    "\r\n",
    "Using function `torch.index_select()`, the code below accesses column (`dim=1`) indexed by 0 and 2. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "indices = torch.LongTensor([0, 2])\r\n",
    "describe(torch.index_select(x, dim=1, index=indices))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[0, 2],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can duplicate the same row or column multiple times, by specifying the same index multiple times. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "indices = torch.LongTensor([0, 0, 0])\r\n",
    "describe(torch.index_select(x, dim=0, index=indices))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([3, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use indices directly `[inices_list, indices_list]` can also achieve the same outcome."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "row_indices = torch.arange(2).long()\r\n",
    "col_indices = torch.LongTensor([0,2])\r\n",
    "describe(x[row_indices, col_indices])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values: \n",
      "tensor([0, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "describe(x[[0,1], [0,2]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values: \n",
      "tensor([0, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concatenating Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "x = torch.arange(6).view(2,3)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "describe(torch.cat([x, x], dim=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([4, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "describe(torch.cat([x, x], dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 6])\n",
      "Values: \n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "describe(torch.stack([x, x], dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2, 3])\n",
      "Values: \n",
      "tensor([[[0, 1, 2],\n",
      "         [0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Algebra on tensors: multiplication"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "x1 = torch.arange(6).view(2,3).float()\r\n",
    "describe(x1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "x2 = torch.ones(3,2)\r\n",
    "x2[:, 1] += 1\r\n",
    "describe(x2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "describe(torch.mm(x1, x2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[ 3.,  6.],\n",
      "        [12., 24.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CUDA tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\r\n",
    "from functions import describe\r\n",
    "\r\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# prefered method: device agnostic tensor instantiation\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "x = torch.rand(3,2).to(device)\r\n",
    "describe(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:torch.cuda.FloatTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[0.5107, 0.8196],\n",
      "        [0.9484, 0.2701],\n",
      "        [0.1767, 0.9292]], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{warning}\n",
    "Mixing CUDA tensors with CPU-bound tensors will lead to errors. This is because we need to ensure the tensors are on the same device. \n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "y = torch.rand(3,2)\r\n",
    "x + y"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25968/1126695582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cpu_device = torch.device(\"cpu\")\r\n",
    "x = x.to(cpu_device)\r\n",
    "y = y.to(cpu_device)\r\n",
    "x + y"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6276, 0.9583],\n",
       "        [0.7592, 1.2605],\n",
       "        [1.0946, 0.9480]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{note}\n",
    "It is expensive to move data back and forth from the GPU. Best practice is to carry out as much computation on GPU as possible and then just transfering the final results to CPU. \n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{warning}\n",
    "`torch.arange()` creates LongTensor, for `torch.mm()`, we need to convert the LongTensor to FloatTensor by using `x.float()`.\n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}