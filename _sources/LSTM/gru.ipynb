{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Gated Recurrent Units (GRUs)\r\n",
    "==================================\r\n",
    "\r\n",
    ":::{admonition} Update Gate \r\n",
    "Weights between old and new (candidate) hidden state\r\n",
    "\r\n",
    "$\\begin{align*}\r\n",
    "h_{new} &= tanh(t_h+t_x) \\\\\r\n",
    "h' &= h_{new} * (1 - z) + h_{old} * z\r\n",
    "\\end{align*}$\r\n",
    "\r\n",
    ":::\r\n",
    "\r\n",
    ":::{admonition} Reset Gate\r\n",
    "Weights between input and hidden state\r\n",
    "\r\n",
    "$h_{new} = tanh(r*t_h + t_x)$\r\n",
    "\r\n",
    ":::\r\n",
    "\r\n",
    ":::{admonition} GRU Cell\r\n",
    "\r\n",
    "$h' = tanh(r*t_h + t_x) * (1-z) + h*z$\r\n",
    "\r\n",
    ":::\r\n",
    "\r\n",
    "![GRU Cell](../images/gru_cell.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRU Cell\r\n",
    "\r\n",
    "RNN is a special case of GRU when $r=1$ and $z=0$.\r\n",
    "\r\n",
    "$$\\begin{align*}\r\n",
    "RNN: h'&=tanh(t_h+t_x)\r\n",
    "\\\\\r\n",
    "GRU: h'&=\\underbrace{\\underbrace{tanh(\\color{red}{r}*t_{hn}+t_{xn})}_{n}*\\color{#0066CC}{(1-z)} + \\color{gray}{h}*\\color{#0066CC}{z}}_{weighted\\ average\\ of\\ n\\ and\\ h}\r\n",
    "\\end{align*}$$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\r\n",
    "\\begin{align*}\r\n",
    "\\color{red}{r(reset\\ gate)} &\\color{red}{=\\sigma(t_{hr}+t_{xr})}\r\n",
    "\\\\\r\n",
    "\\color{#0066CC}{z(update\\ gate)} &\\color{#0066CC}{=\\sigma(t_{hz}+t_{xz})}\r\n",
    "\\\\\r\n",
    "\\color{black}{n (hidden\\ state)}&\\color{black}{=tanh(t_{hn}+t_{xn})}\r\n",
    "\\end{align*}\r\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\r\n",
    "\\begin{align*}\r\n",
    "\\color{red}{r\\ (hidden)}&\\color{red}{:t_{hr}}&\\color{red}{=}&\\color{red}{W_{hr}}&\\color{red}{h}&\\color{red}{+}&\\color{red}{b_{hr}}\r\n",
    "\\\\\r\n",
    "\\color{red}{r\\ (input)}&\\color{red}{:t_{xr}}&\\color{red}{=}&\\color{red}{W_{ir}}&\\color{red}{x}&\\color{red}{+}&\\color{red}{b_{ir}}\r\n",
    "\\\\\r\n",
    "\\color{#0066CC}{z\\ (hidden)}&\\color{#0066CC}{:t_{hz}}&\\color{#0066CC}{=}&\\color{#0066CC}{W_{hz}}&\\color{#0066CC}{h}&\\color{#0066CC}{+}&\\color{#0066CC}{b_{hz}}\r\n",
    "\\\\\r\n",
    "\\color{#0066CC}{z\\ (input)}&\\color{#0066CC}{:t_{xz}}&\\color{#0066CC}{=}&\\color{#0066CC}{W_{iz}}&\\color{#0066CC}{x}&\\color{#0066CC}{+}&\\color{#0066CC}{b_{iz}}\r\n",
    "\\\\\r\n",
    "n\\ (hidden)&:t_{hn}& = &W_{hn}&h&+&b_{hn}\r\n",
    "\\\\\r\n",
    "n\\ (input)&:t_{xn}& = &W_{in}&x&+&b_{in}\r\n",
    "\\end{align*}\r\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To really understand the flow of information inside the GRU cell, I suggest you try these exercises:\r\n",
    "- first, learn to look past (or literally ignore) the internals of the gates: both `r` and `z` are simply values between zero and one (for each hidden dimension)\r\n",
    "-  pretend `r=1`; can you see that the resulting n is equivalent to the output of a simple RNN?\r\n",
    "- keep `r=1` and now pretend `z=0`; can you see that the new hidden state `h'` is equivalent to the output of a simple RNN?\r\n",
    "- now pretend `z=1`; can you see that the new hidden state `h'` is simply a copy of the old hidden state (in other words, the data (`x`) does not have any effect)?\r\n",
    "- if you decrease `r` all the way to zero, the resulting `n` is less and less influenced by the old hidden state\r\n",
    "- if you decrease `z` all the way to zero, the new hidden state `h'` is closer and closer to `n`\r\n",
    "- for `r=0` and `z=0`, the cell becomes equivalent to a linear layer followed by a Tanh activation function (in other words, the old hidden state (`h`) does not have any effect)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\r\n",
    "from torch.nn.utils import rnn as rnn_utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRU Cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "n_features = 2 \r\n",
    "hidden_dim = 2\r\n",
    "torch.manual_seed(17) \r\n",
    "gru_cell = nn.GRUCell(input_size=n_features, hidden_size=hidden_dim) \r\n",
    "gru_state = gru_cell.state_dict()\r\n",
    "gru_state"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('weight_ih',\n",
       "              tensor([[-0.0930,  0.0497],\n",
       "                      [ 0.4670, -0.5319],\n",
       "                      [-0.6656,  0.0699],\n",
       "                      [-0.1662,  0.0654],\n",
       "                      [-0.0449, -0.6828],\n",
       "                      [-0.6769, -0.1889]])),\n",
       "             ('weight_hh',\n",
       "              tensor([[-0.4167, -0.4352],\n",
       "                      [-0.2060, -0.3989],\n",
       "                      [-0.7070, -0.5083],\n",
       "                      [ 0.1418,  0.0930],\n",
       "                      [-0.5729, -0.5700],\n",
       "                      [-0.1818, -0.6691]])),\n",
       "             ('bias_ih',\n",
       "              tensor([-0.4316,  0.4019,  0.1222, -0.4647, -0.5578,  0.4493])),\n",
       "             ('bias_hh',\n",
       "              tensor([-0.6800,  0.4422, -0.3559, -0.0279,  0.6553,  0.2918]))])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "Wi, bi = gru_state['weight_ih'], gru_state['bias_ih'] \r\n",
    "Wh, bh = gru_state['weight_hh'], gru_state['bias_hh']\r\n",
    "print(Wi.shape, Wh.shape)\r\n",
    "print(bi.shape, bh.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 2]) torch.Size([6, 2])\n",
      "torch.Size([6]) torch.Size([6])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting up the `weight_ih` as an example\r\n",
    "\r\n",
    "![GRU Cell State Split](../images/gru_cell_state_split.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "Wir, Wiz, Win = Wi.split(hidden_dim, dim=0) \r\n",
    "bir, biz, bin = bi.split(hidden_dim, dim=0)\r\n",
    "Whr, Whz, Whn = Wh.split(hidden_dim, dim=0) \r\n",
    "bhr, bhz, bhn = bh.split(hidden_dim, dim=0)\r\n",
    "Wxr, bxr"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-0.0930,  0.0497],\n",
       "         [ 0.4670, -0.5319]]),\n",
       " tensor([-0.4316,  0.4019]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Linear Layers\r\n",
    "\r\n",
    "We can use the weights and biases to create the corresponding linear layers:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def linear_layers(Wi, bi, Wh, bh):\r\n",
    "    hidden_dim, n_features = Wi.size()    \r\n",
    "    lin_input = nn.Linear(n_features, hidden_dim)\r\n",
    "    lin_input.load_state_dict({'weight': Wi, 'bias': bi})\r\n",
    "    lin_hidden = nn.Linear(hidden_dim, hidden_dim)\r\n",
    "    lin_hidden.load_state_dict({'weight': Wh, 'bias': bh})\r\n",
    "    return lin_hidden, lin_input\r\n",
    "\r\n",
    "# reset gate - red\r\n",
    "r_hidden, r_input = linear_layers(Wir, bir, Whr, bhr) \r\n",
    "# update gate - blue\r\n",
    "z_hidden, z_input = linear_layers(Wiz, biz, Whz, bhz) \r\n",
    "# candidate state - black\r\n",
    "n_hidden, n_input = linear_layers(Win, bin, Whn, bhn) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def reset_gate(h, x):\r\n",
    "    thr = r_hidden(h)\r\n",
    "    txr = r_input(x)\r\n",
    "    r = torch.sigmoid(thr + txr)\r\n",
    "    return r  # red\r\n",
    "    \r\n",
    "def update_gate(h, x):\r\n",
    "    thz = z_hidden(h)\r\n",
    "    txz = z_input(x)\r\n",
    "    z = torch.sigmoid(thz + txz)\r\n",
    "    return z  # blue\r\n",
    "    \r\n",
    "def candidate_n(h, x, r):\r\n",
    "    thn = n_hidden(h)\r\n",
    "    txn = n_input(x)\r\n",
    "    n = torch.tanh(r * thn + txn)\r\n",
    "    return n  # black"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def generate_sequences(n=128, variable_len=False, seed=13):\r\n",
    "    basic_corners = np.array([[-1, -1], [-1, 1], [1, 1], [1, -1]])\r\n",
    "    np.random.seed(seed)\r\n",
    "    bases = np.random.randint(4, size=n)\r\n",
    "    if variable_len:\r\n",
    "        lengths = np.random.randint(3, size=n) + 2\r\n",
    "    else:\r\n",
    "        lengths = [4] * n\r\n",
    "    directions = np.random.randint(2, size=n)\r\n",
    "    points = [basic_corners[[(b + i) % 4 for i in range(4)]][slice(None, None, d*2-1)][:l] + np.random.randn(l, 2) * 0.1 for b, d, l in zip(bases, directions, lengths)]\r\n",
    "    return points, directions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "points, directions = generate_sequences(n=128, seed=13)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "initial_hidden = torch.zeros(1, hidden_dim)\r\n",
    "X = torch.as_tensor(points[0]).float()\r\n",
    "first_corner = X[0:1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "r = reset_gate(initial_hidden, first_corner)\r\n",
    "r"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2387, 0.6928]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{important}\r\n",
    "The reset gate scales each hidden dimension independently. It can completely suppress the values from one of the hidden dimensions while letting the other pass unchallenged. In geometrical terms, it means that the hidden space may shrink in one direction while stretching in the other.\r\n",
    ":::"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "z = update_gate(initial_hidden, first_corner)\r\n",
    "z"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2984, 0.3540]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reset gate is an input for the candidate hidden state (`n`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "n = candidate_n(initial_hidden, first_corner, r)\r\n",
    "n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.8032, -0.2275]], grad_fn=<TanhBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The update gate is telling us to keep 29.84% of the first and 35.40% of the second dimensions of the initial hidden state. The remaining 60.16% and 64.6%, respectively, are coming from the candidate hidden\r\n",
    "state (`n`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "h_prime = n*(1-z) + initial_hidden*z\r\n",
    "h_prime"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.5635, -0.1470]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verify against GRU Cell\r\n",
    "\r\n",
    "We can see the `nn.GRUCell()` had encapsulated all the above steps. Effectively, a GRU consists of six linear layers, with \r\n",
    "- four of them require sigmoid action to learn \r\n",
    "   - two reset gates, each for hidden state and input respectively,  and\r\n",
    "   - two update gates, each for hidden state and input respectively \r\n",
    "- two require tanh() activation for learning the new candidate hidden state. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "gru_cell(first_corner)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.5635, -0.1470]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)"
  },
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}