{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memories (LSTMs)\n",
    "================================\n",
    "\n",
    "In addtion to the bounded hidden state (between [-1,1]), LSTM introduced an unbounded **cell state**, which is mimic the \"long term memory\". This allows more past information to be kept and thus passed to the current cell, because the gradient is not squashed to approach zero as fast as `tanh`, so it is also refered to as a \"gradient highway\". In contrast, the hidden state keeps more recent information, thus responsible for the \"short term memory\". \n",
    "\n",
    ":::{admonition} The candidate hidden state\n",
    "$$\n",
    "g = tanh(t_{hg}+t_{xg})\n",
    "$$\n",
    ":::\n",
    "\n",
    "Cell state is an unbounded weighted sum of the candidate hidden state $g$ and previous cell state $c$, weighted by learnt input gate $i$ and forget gate $f$, respectively. \n",
    "\n",
    ":::{admonition} The new cell state\n",
    "$$\n",
    "c' = g * i + c * f\n",
    "$$\n",
    ":::\n",
    "\n",
    "The new hidden state is obtained by bounding the new cell state ($c'$) before applying output gate weights ($o$):::\n",
    "$$\n",
    "h' = tanh(c') * o\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torch.nn.utils import rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell\n",
    "\n",
    "There are two sets of weights for each type of gates, one for hidden state, one for input. They are learnt through a linear layer followed by sigmoid like in a GRU cell.  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\color{#82b366}{i\\ (hidden)}&\\color{#82b366}{:t_{hi}}&\\color{#82b366}{=}&\\color{#82b366}{W_{hi}}&\\color{#82b366}{h}&\\color{#82b366}{+}&\\color{#82b366}{b_{hi}}\n",
    "\\\\\n",
    "\\color{#82b366}{i\\ (input)}&\\color{#82b366}{:t_{xi}}&\\color{#82b366}{=}&\\color{#82b366}{W_{ii}}&\\color{#82b366}{x}&\\color{#82b366}{+}&\\color{#82b366}{b_{ii}}\n",
    "\\\\\n",
    "\\color{red}{f\\ (hidden)}&\\color{red}{:t_{hf}}&\\color{red}{=}&\\color{red}{W_{hf}}&\\color{red}{h}&\\color{red}{+}&\\color{red}{b_{hf}}\n",
    "\\\\\n",
    "\\color{red}{f\\ (input)}&\\color{red}{:t_{xf}}&\\color{red}{=}&\\color{red}{W_{if}}&\\color{red}{x}&\\color{red}{+}&\\color{red}{b_{if}}\n",
    "\\\\\n",
    "g\\ (hidden)&:t_{hg}& = &W_{hg}&h&+&b_{hg}\n",
    "\\\\\n",
    "g\\ (input)&:t_{xg}& = &W_{ig}&x&+&b_{ig}\n",
    "\\\\\n",
    "\\color{#0066cc}{o\\ (hidden)}&\\color{#0066cc}{:t_{ho}}&\\color{#0066cc}{=}&\\color{#0066cc}{W_{ho}}&\\color{#0066cc}{h}&\\color{#0066cc}{+}&\\color{#0066cc}{b_{ho}}\n",
    "\\\\\n",
    "\\color{#0066cc}{o\\ (input)}&\\color{#0066cc}{:t_{xo}}&\\color{#0066cc}{=}&\\color{#0066cc}{W_{io}}&\\color{#0066cc}{x}&\\color{#0066cc}{+}&\\color{#0066cc}{b_{io}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "![LSTM Cell](../images/lstm_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.LSTMCell`\n",
    "\n",
    "Let's take a look at the weights generated by `nn.LSTMCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight_ih',\n",
       "              tensor([[-0.0930,  0.0497],\n",
       "                      [ 0.4670, -0.5319],\n",
       "                      [-0.6656,  0.0699],\n",
       "                      [-0.1662,  0.0654],\n",
       "                      [-0.0449, -0.6828],\n",
       "                      [-0.6769, -0.1889],\n",
       "                      [-0.4167, -0.4352],\n",
       "                      [-0.2060, -0.3989]])),\n",
       "             ('weight_hh',\n",
       "              tensor([[-0.7070, -0.5083],\n",
       "                      [ 0.1418,  0.0930],\n",
       "                      [-0.5729, -0.5700],\n",
       "                      [-0.1818, -0.6691],\n",
       "                      [-0.4316,  0.4019],\n",
       "                      [ 0.1222, -0.4647],\n",
       "                      [-0.5578,  0.4493],\n",
       "                      [-0.6800,  0.4422]])),\n",
       "             ('bias_ih',\n",
       "              tensor([-0.3559, -0.0279,  0.6553,  0.2918,  0.4007,  0.3262, -0.0778, -0.3002])),\n",
       "             ('bias_hh',\n",
       "              tensor([-0.3991, -0.3200,  0.3483, -0.2604, -0.1582,  0.5558,  0.5761, -0.3919]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 2\n",
    "hidden_dim = 2\n",
    "\n",
    "torch.manual_seed(17)\n",
    "lstm_cell = nn.LSTMCell(input_size=n_features, hidden_size=hidden_dim)\n",
    "lstm_state = lstm_cell.state_dict()\n",
    "lstm_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate a LSTMCell Manually\n",
    "\n",
    "The code below tries to use the above the weights, manually create the linear layers for learning the gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layers(Wi, bi, Wh, bh):\n",
    "    hidden_dim, n_features = Wi.size()    \n",
    "    lin_input = nn.Linear(n_features, hidden_dim)\n",
    "    lin_input.load_state_dict({'weight': Wi, 'bias': bi})\n",
    "    lin_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "    lin_hidden.load_state_dict({'weight': Wh, 'bias': bh})\n",
    "    return lin_hidden, lin_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the weights and the create the linear layers for input, forget and output gate. Note the candidate hidden state is an Elman RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx, bx = lstm_state['weight_ih'], lstm_state['bias_ih']\n",
    "Wh, bh = lstm_state['weight_hh'], lstm_state['bias_hh']\n",
    "\n",
    "# Split weights and biases for data points\n",
    "Wxi, Wxf, Wxg, Wxo = Wx.split(hidden_dim, dim=0)\n",
    "bxi, bxf, bxg, bxo = bx.split(hidden_dim, dim=0)\n",
    "# Split weights and biases for hidden state\n",
    "Whi, Whf, Whg, Who = Wh.split(hidden_dim, dim=0)\n",
    "bhi, bhf, bhg, bho = bh.split(hidden_dim, dim=0)\n",
    "\n",
    "# Creates linear layers for the components\n",
    "i_hidden, i_input = linear_layers(Wxi, bxi, Whi, bhi) # input gate - green\n",
    "f_hidden, f_input = linear_layers(Wxf, bxf, Whf, bhf) # forget gate - red\n",
    "o_hidden, o_input = linear_layers(Wxo, bxo, Who, bho) # output gate - blue\n",
    "g_cell = nn.RNNCell(n_features, hidden_dim) # black\n",
    "g_cell.load_state_dict({'weight_ih': Wxg, 'bias_ih': bxg,\n",
    "                        'weight_hh': Whg, 'bias_hh': bhg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forget_gate(h, x):\n",
    "    thf = f_hidden(h)\n",
    "    txf = f_input(x)\n",
    "    f = torch.sigmoid(thf + txf)\n",
    "    return f  # red\n",
    "    \n",
    "def output_gate(h, x):\n",
    "    tho = o_hidden(h)\n",
    "    txo = o_input(x)\n",
    "    o = torch.sigmoid(tho + txo)\n",
    "    return o  # blue\n",
    "\n",
    "def input_gate(h, x):\n",
    "    thi = i_hidden(h)\n",
    "    txi = i_input(x)\n",
    "    i = torch.sigmoid(thi + txi)\n",
    "    return i  # green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation \n",
    "\n",
    "Same square sequence is used here to verify that our manual LSTM produces the same result as `nn.LSTMCell`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(n=128, variable_len=False, seed=13):\n",
    "    basic_corners = np.array([[-1, -1], [-1, 1], [1, 1], [1, -1]])\n",
    "    np.random.seed(seed)\n",
    "    bases = np.random.randint(4, size=n)\n",
    "    if variable_len:\n",
    "        lengths = np.random.randint(3, size=n) + 2\n",
    "    else:\n",
    "        lengths = [4] * n\n",
    "    directions = np.random.randint(2, size=n)\n",
    "    points = [basic_corners[[(b + i) % 4 for i in range(4)]][slice(None, None, d*2-1)][:l] + np.random.randn(l, 2) * 0.1 for b, d, l in zip(bases, directions, lengths)]\n",
    "    return points, directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, directions = generate_sequences(n=128, seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_hidden = torch.zeros(1, hidden_dim)\n",
    "initial_cell = torch.zeros(1, hidden_dim)\n",
    "\n",
    "X = torch.as_tensor(points[0]).float()\n",
    "first_corner = X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1340, -0.0004]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = g_cell(first_corner)\n",
    "i = input_gate(initial_hidden, first_corner)\n",
    "gated_input = g * i\n",
    "gated_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = forget_gate(initial_hidden, first_corner)\n",
    "gated_cell = initial_cell * f\n",
    "gated_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1340, -0.0004]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_prime = gated_cell + gated_input\n",
    "c_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.4936e-02, -8.3810e-05]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = output_gate(initial_hidden, first_corner)\n",
    "h_prime = o * torch.tanh(c_prime)\n",
    "h_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Cell output hidden state and cell state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.4936e-02, -8.3810e-05]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.1340, -0.0004]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_prime, c_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the result against `nn.LSTMCell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.4936e-02, -8.3810e-05]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.1340, -0.0004]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cell(first_corner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} BiLSTM Layer\n",
    "Find out how to create a Bidirectional LSTM Layer in PyTorch? What's the output look like?\n",
    "::: "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d990147e05fc0cc60dd3871899a6233eb6a5324c1885ded43d013dc915f7e535"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('cits4012': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
