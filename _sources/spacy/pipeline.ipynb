{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "NLP Pipelines\r\n",
    "=================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traditional NLP Pipeline in NLTK\r\n",
    "\r\n",
    "![image](../images/ie-architecture.png)\r\n",
    "\r\n",
    "Image credit of [Information Extraction in NLTK](http://www.nltk.org/howto/relextract.html#:~:text=Relation\\%20Extraction,The\\%20sem.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt') # Sentence Tokenize\r\n",
    "nltk.download('averaged_perceptron_tagger') # POS Tagging\r\n",
    "nltk.download('maxent_ne_chunker') # Named Entity Chunking\r\n",
    "nltk.download('words') # Word Tokenize\r\n",
    "\r\n",
    "# texts is a collection of documents.\r\n",
    "# Here is a single document with two sentences.\r\n",
    "texts = [u\"A storm hit the beach in Perth. It started to rain.\"]\r\n",
    "for text in texts:\r\n",
    "    sentences = nltk.sent_tokenize(text)\r\n",
    "    for sentence in sentences:\r\n",
    "      words = nltk.word_tokenize(sentence)\r\n",
    "      tagged_words = nltk.pos_tag(words)\r\n",
    "      ne_tagged_words = nltk.ne_chunk(tagged_words)\r\n",
    "      print(ne_tagged_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S A/DT storm/NN hit/VBD the/DT beach/NN in/IN (GPE Perth/NNP) ./.)\n",
      "(S It/PRP started/VBD to/TO rain/VB ./.)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising NER in spaCy\r\n",
    "\r\n",
    "We can use the `Doc.user_data` *attribute* to set a title for the visualisation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "from spacy import displacy\r\n",
    "doc.user_data['title'] = \"An example of an entity visualization\"\r\n",
    "displacy.render(doc, style='ent')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">An example of an entity visualization</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I need a taxi to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Subiaco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SUBURB</span>\n",
       "</mark>\n",
       "</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write the visualisation to a file\r\n",
    "\r\n",
    "We can inform the render to not display the visualisation in the Jupyter Notebook instead write into a file by calling the render with two extra argument:\r\n",
    "\r\n",
    "```\r\n",
    "jupyter=False, page=True\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "from pathlib import Path\r\n",
    "# the page=True indicates that we want to write to a file\r\n",
    "html = displacy.render(doc, style='ent', jupyter=False, page=True)\r\n",
    "output_path = Path(\"C:\\\\Users\\\\wei\\\\CITS4012\\\\ent_visual.html\")\r\n",
    "output_path.open(\"w\", encoding=\"utf-8\").write(html)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLP pipeline in spaCy\r\n",
    "\r\n",
    "Recall that spaCy's container objects represent linguistic units, suchas a text (i.e. document), a sentence and an individual token withlinguistic features already extracted for them.\r\n",
    "\r\n",
    "How does spaCy create these containers and fill them withrelevant data?\r\n",
    "\r\n",
    "A spaCy pipeline include, by default, a part-of-speech tagger (`tagger`), a dependency parser (`parser`), a lemmatizer (`lemmatizer`), an entity recognizer (`ner`), an attribute ruler (`attribute_ruler` and a word vectorisation model (`tok2vec`))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "nlp.pipe_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "spaCy allows you to load a selected set of pipeline components, dis-abling those that aren't necessary.\r\n",
    "\r\n",
    "You can do this either when creating a nlp object or disable it after the nlp object is created. \r\n",
    "\r\n",
    "```{tabbed} Disabling when create\r\n",
    "`nlp = spacy.load('en_core_web_sm',disable=['parser'])`\r\n",
    "```\r\n",
    "\r\n",
    "```{tabbed} Disabling after creation\r\n",
    "`nlp.disable_pipes('tagger')`\r\n",
    "\r\n",
    "`nlp.disable_pipes('parser')`\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customising a NLP pipe in spaCy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "doc = nlp(u'I need a taxi to Cottesloe.')\r\n",
    "for ent in doc.ents:\r\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cottesloe GPE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} What if?\r\n",
    "If we would like to introduce a new entity type `SUBURB` for `Cottesloe` and other suburb names, how should we informthe NER component about it?\r\n",
    "```\r\n",
    "\r\n",
    "*Steps of Customising a spaCy NER pipe*\r\n",
    "1. Create a training example to show the entity recognizer so it will learn what to apply the SUBURB label to;\r\n",
    "2. Add a new label called SUBURB to the list of supported entitytypes;\r\n",
    "3. Disable other pipe to ensure that only the entity recogniser will beupdated during training;\r\n",
    "4. Start training;\r\n",
    "5. Test your new NER pipe;\r\n",
    "6. Serialise the pipe to disk;\r\n",
    "7. Load the customised NER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Specify new label and training data\r\n",
    "LABEL = 'SUBURB' \r\n",
    "TRAIN_DATA = [('I need a taxi to Cottesloe', \r\n",
    "                { 'entities': [(17, 26, 'SUBURB')] }),\r\n",
    "              ('I like red oranges', { 'entities': []})]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# Add new label to the ner pipe\r\n",
    "ner = nlp.get_pipe('ner')\r\n",
    "ner.add_label(LABEL)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Train\r\n",
    "optimizer = nlp.create_optimizer() \r\n",
    "import random\r\n",
    "from spacy.tokens import Doc\r\n",
    "from spacy.training import Example\r\n",
    "for i in range(25):\r\n",
    "  random.shuffle(TRAIN_DATA)\r\n",
    "  for text, annotations in TRAIN_DATA:\r\n",
    "    doc = Doc(nlp.vocab, words=text.split(\" \"))\r\n",
    "    # We need to create a training example object\r\n",
    "    example = Example.from_dict(doc, annotations)\r\n",
    "    nlp.update([example], sgd=optimizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Test\r\n",
    "doc = nlp(u'I need a taxi to Crawley')\r\n",
    "for ent in doc.ents:\r\n",
    "  print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Crawley SUBURB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# Serialize the entire model to disk\r\n",
    "nlp.to_disk('C:\\\\Users\\\\wei\\\\CITS4012') # Windows Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Load spacy model from disk\r\n",
    "import spacy\r\n",
    "nlp_updated = spacy.load('C:\\\\Users\\\\wei\\\\CITS4012')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# Test\r\n",
    "doc = nlp_updated(u'I need a taxi to Subiaco')\r\n",
    "for ent in doc.ents:\r\n",
    "  print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subiaco SUBURB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ":::{admonition} Your Turn \r\n",
    "- Replace the suburb name with a few others, for example 'Claremont', 'Western Australia' and see what the the entity label is. \r\n",
    "- Take a look at the directory and see how the `nlp` model is stored. \r\n",
    "- This [blog post on How to Train spaCy to Autodetect New Entities (NER) [Complete Guide]](https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/) has more extensive examples on how to train a ner model with more data. \r\n",
    ":::"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('lda': conda)"
  },
  "interpreter": {
   "hash": "e5f76115fc97e95e5e5f274d9a04d3733842aec59c7431144116cf5affd2efc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}