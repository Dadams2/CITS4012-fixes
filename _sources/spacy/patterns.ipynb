{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Finding Patterns\r\n",
    "================ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imagine you are building a chat bot and we are trying to find utterances in user input that express one of the following: \r\n",
    "\r\n",
    "```{tabbed} What's Expressed\r\n",
    "ability, possibility, permission, or obligation (as opposed to utterances that describe real actions that have occurred, are occurring, or occur regularly)\r\n",
    "```\r\n",
    "```{tabbed} Example Sentences\r\n",
    "For instance, we want to find “I can do it.”\r\n",
    "but not “I’ve done it.”\r\n",
    "```\r\n",
    "```{tabbed} Linguistic Pattern\r\n",
    "`subject + auxiliary + verb + . . . + direct object + ...`\r\n",
    "\r\n",
    "The ellipses indicate that the direct object isn't necessarily\r\n",
    "located immediately behind the verb, there might be other words in between.\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check spaCy version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!pip show spacy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name: spacy\n",
      "Version: 3.0.5\n",
      "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
      "Home-page: https://spacy.io\n",
      "Author: Explosion\n",
      "Author-email: contact@explosion.ai\n",
      "License: MIT\n",
      "Location: c:\\programdata\\anaconda3\\envs\\lda\\lib\\site-packages\n",
      "Requires: tqdm, setuptools, pydantic, jinja2, packaging, murmurhash, wasabi, pathy, requests, cymem, preshed, blis, srsly, typer, spacy-legacy, thinc, numpy, catalogue\n",
      "Required-by: en-core-web-sm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hard-coded patterns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "def dep_pattern(doc):\r\n",
    "    for i in range(len(doc)-1):\r\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and doc[i+2].dep_ == 'ROOT':\r\n",
    "            for tok in doc[i+2].children:\r\n",
    "                if tok.dep_ == 'dobj':\r\n",
    "                    return True\r\n",
    "    \r\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# doc = nlp(u'We can overtake them.')\r\n",
    "doc = nlp(u'I might send them a card as a reminder.')\r\n",
    "if dep_pattern(doc):\r\n",
    "    print('Found')\r\n",
    "else:\r\n",
    "    print('Not found')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{dropdown} Code Explanation\r\n",
    "The `dep_pattern` function above takes a Doc object as parameter and returns a binary value `True` if the hard-coded pattern `subject + auxiliary + verb + . . . + direct object + ...` is found, otherwise `False`. The function iterates over the Doc object's tokens,\r\n",
    "searching for a `subject + auxiliary + verb`, where the `verb` is the `root` of the dependency tree. If the pattern is found, then we check whether the verb has a direct object among its syntactic children. Finally, if we find a direct object, the function returns `True`.\r\n",
    "Otherwise, it returns `False`.\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using spaCy pattern matcher\r\n",
    "\r\n",
    "spaCy has a predefined tool called `Matcher`, that\r\n",
    "is specially designed to find sequences of tokens based on pattern rules. An implementation of the “subject + auxiliary + verb” pattern with\r\n",
    "`Matcher` might look like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import spacy\r\n",
    "from spacy.matcher import Matcher\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "matcher = Matcher(nlp.vocab)\r\n",
    "pattern = [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\": \"ROOT\"}]\r\n",
    "matcher.add(\"NsubjAuxRoot\", [pattern])\r\n",
    "doc = nlp(\"We can overtake them.\")\r\n",
    "matches = matcher(doc)\r\n",
    "for match_id, start, end in matches:\r\n",
    "    span = doc[start:end]\r\n",
    "    print(\"Span: \", span.text)\r\n",
    "    print(\"The positions in the doc are: \", start, \"-\", end)\r\n",
    "    print(\"Match ID \", match_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Span:  We can overtake\n",
      "The positions in the doc are:  0 - 3\n",
      "Match ID  10599197345289971701\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{dropdown} Code Explanation\r\n",
    "spaCy `Matcher` class takes a model's vocabulary as input and creates a matcher object named `matcher`. Then we need to define a pattern of interest. The pattern is specified in a dictionary object, and the order of the key value pairs indicate the desired sequence we are trying to find a match for. Once the pattern is found, a list of tuples in the form of `(match_id, start, end)` is returned. The `match_id` is the hash value of the string ID \"NsubjAuxRoot\". To get the string value, you can look up the ID in the StringStore.\r\n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('lda': conda)"
  },
  "interpreter": {
   "hash": "e5f76115fc97e95e5e5f274d9a04d3733842aec59c7431144116cf5affd2efc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}