
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Recurrent Neural Networks - Introduction &#8212; CITS4012 Natural Language Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Classifying Synthetic Sequences - The Square Model" href="elman_rnn_square.html" />
    <link rel="prev" title="Lab10: RNN for NLP" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo_ntlp.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CITS4012 Natural Language Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   HOME
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/intro.html">
   Lab 01: Conda Environment and Python Refresher
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/installation.html">
     1. CITS4012 Base Environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/installation_misc.html">
     2. CITS4012 MISC Enviornment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lab_machines.html">
     3. Use Lab Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/python_review.html">
     4. Python Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/iterables.html">
     5. Iterables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/numpy.html">
     6. Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/matplotlib.html">
     7. Matplotlib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../NLTK/intro.html">
   Lab02: NLTK
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/start.html">
     1. Starting with NLTK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/closer_look.html">
     2. A Closer Look at Python: Texts as Lists of Words
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/computing.html">
     3. Computing with Language: Simple Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/take_control.html">
     4. Back to Python: Making Decisions and Taking Control
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/exercises.html">
     5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../spacy/intro.html">
   Lab03: spaCy NLP pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/container.html">
     1. Container Objects in spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/pipeline.html">
     2. NLP Pipelines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/patterns.html">
     3. Finding Patterns
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/telegram.html">
     4. Your first chatbot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/exercise.html">
     5. Exercise
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../gensim/intro.html">
   Lab04: Count-Based Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../gensim/tf-idf.html">
     1. TF-IDF in scikit-learn and Gensim
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gensim/classification.html">
     2. Document Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../nn/intro.html">
   Lab05: Introduction to Neural Networks and Pytorch
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/linear_model_numpy.html">
     1. Linear Models in Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/tensors.html">
     2. Introduction to Pytorch Tensors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/linear_model_pytorch.html">
     3. Linear Models in Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../pytorch/intro.html">
   Lab06: Neural Network Building Blocks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/activations.html">
     1. Activation Functions and their derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/loss.html">
     2. Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/computational_graph.html">
     3. Dynamic Computational Graph in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/nn_oop.html">
     4. Neural Networks in PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../embeddings/intro.html">
   Lab07: Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/svd.html">
     1. Word Vectors from Word-Word Coocurrence Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/glove.html">
     2. GloVe: Global Vectors for Word Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/word2vec.html">
     3. Word2Vec
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../classification/intro.html">
   Lab08: Document Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/perceptron.html">
     1. Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/data_prep.html">
     2. Dataset and DataLoader
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/yelp_preprocessing.html">
     3. Yelp Dataset at a glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/yelp.html">
     4. Yelp Review Dataset - Document Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cnn/intro.html">
   Lab09: CNN for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/dataset_frankenstein_processing.html">
     1. Frankenstein Dataset At a Glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/CBOW.html">
     2. Learning Embeddings with Continuous Bag of Words (CBOW)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/convolution.html">
     3. Convolution Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/dataset_AG_News_processing.html">
     4. AG News Dataset at A Glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/Document_Classification_with_CNN.html">
     5. Using CNN for Document Classification with Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   Lab10: RNN for NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1. Recurrent Neural Networks - Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="elman_rnn_square.html">
     2. Classifying Synthetic Sequences - The Square Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Surname_Dataset.html">
     3. Surname Dataset Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Surname_Classification.html">
     4. Surname Classification with RNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../LSTM/intro.html">
   Lab11: GRU, LSTM and Seq2Seq
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/gru.html">
     1. Gated Recurrent Units (GRUs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/lstm.html">
     2. Long Short Term Memories (LSTMs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/gru_lstm_square.html">
     3. The Square Model Using GRU and LSTM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/seq2seq.html">
     4. Sequence to Sequence Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/Surname_Generation_Unconditioned.html">
     5. Surname Generation - Unconditioned
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/Surname_Generation_Conditioned.html">
     6. Surname Generation Conditioned
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../attention/intro.html">
   Lab12: Sequence to Sequence Learning with Attention
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/attention.html">
     1. Attention Mechanism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/self_attention.html">
     2. Self-Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/nmt_data_processing.html">
     3. Neual Machine Translation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/PackedSequence.html">
     4. Packed Sequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/NMT_No_Sampling.html">
     5. Neural Machine Translation - No Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../attention/NMT_scheduled_sampling.html">
     6. Neural Machine Translation - Scheduled Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../transformer/intro.html">
   Extra: Transformers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/position_encoding.html">
     1. Positional Encoding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/LayerNorm.html">
     2. Layer Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/transformer.html">
     3. Transform and Roll Out
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://drive.google.com/file/d/1AQxhGLhoHE162HALo1NkgdaN-LVY4QWo/view?usp=sharing">
   data.zip
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/weiliu2k/CITS4012/raw/master/CITS4012LabBook.pdf">
   PDF
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/rnn/rnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/rnn/rnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   1.1. Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rnn-cell">
   1.2. RNN Cell
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rnn-layer">
   1.3. RNN Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sequence-first-shape-default-in-rnn">
   1.4. Sequence-first shape default in RNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-some-synthetic-data">
     1.4.1. Generate some synthetic data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-first-tensor">
     1.4.2. Batch-first tensor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-use-rnn-with-correctly-shaped-tensors">
     1.4.3. How to use RNN with correctly shaped tensors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#option-1">
       1.4.3.1. Option 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#option-2">
       1.4.3.2. Option 2
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-rnn-with-two-layers">
   1.5. Stacked RNN with Two Layers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#manually-stacking-two-rnns">
     1.5.1. Manually Stacking Two RNNs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bidirectional-rnn">
   1.6. Bidirectional RNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#manually-created-bidirectional-rnn">
     1.6.1. Manually Created Bidirectional RNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#step-0-a-batch-sequence-and-its-reverse">
       1.6.1.1. Step 0: A batch sequence and its reverse
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#step-1-feed-each-rnn-with-its-corresponding-sequence">
       1.6.1.2. Step 1: Feed each RNN with its corresponding sequence
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-tidy-up-the-output">
     1.6.2. Step 2: Tidy up the output
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="recurrent-neural-networks-introduction">
<h1><span class="section-number">1. </span>Recurrent Neural Networks - Introduction<a class="headerlink" href="#recurrent-neural-networks-introduction" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we will look at the basics of how to use <code class="docutils literal notranslate"><span class="pre">nn.RNNCell</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.RNN</span></code> to create RNN Cells and RNN layers, and illustrate the important tensor shape expectation of RNN.</p>
<div class="section" id="imports">
<h2><span class="section-number">1.1. </span>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="rnn-cell">
<h2><span class="section-number">1.2. </span>RNN Cell<a class="headerlink" href="#rnn-cell" title="Permalink to this headline">¶</a></h2>
<p>Given an input feature <span class="math notranslate nohighlight">\(n\)</span>, and a hidden dimension <span class="math notranslate nohighlight">\(d\)</span>, if the transformed dimension is m, then the dimensionality of <span class="math notranslate nohighlight">\(W_{ih}\)</span> should be <span class="math notranslate nohighlight">\(m \times n\)</span>, and the <span class="math notranslate nohighlight">\(W_{hh}\)</span> should be <span class="math notranslate nohighlight">\(m \times d\)</span>.</p>
<p>The following code randomly initialised a (<span class="math notranslate nohighlight">\(2\times 2\)</span>) hidden to hidden matrix, and a (<span class="math notranslate nohighlight">\(2\times 2\)</span>) input to hidden matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">rnn_cell</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight_ih&#39;,
              tensor([[ 0.6627, -0.4245],
                      [ 0.5373,  0.2294]])),
             (&#39;weight_hh&#39;,
              tensor([[-0.4015, -0.5385],
                      [-0.1956, -0.6835]])),
             (&#39;bias_ih&#39;, tensor([0.4954, 0.6533])),
             (&#39;bias_hh&#39;, tensor([-0.3565, -0.2904]))])
</pre></div>
</div>
</div>
</div>
<div class="admonition-your-turn admonition">
<p class="admonition-title">Your Turn</p>
<p>Modify the input feature and hidden dimension, to observe how the weight matrices change.</p>
</div>
</div>
<div class="section" id="rnn-layer">
<h2><span class="section-number">1.3. </span>RNN Layer<a class="headerlink" href="#rnn-layer" title="Permalink to this headline">¶</a></h2>
<p>RNN cell requires us manually feed the cell hidden layer as one of the two inputs (previous hidden state and current input) to the same RNN cell in a for loop. See the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method in the <a class="reference external" href="https://weiliu2k.github.io/CITS4012/rnn/Surname_Classification.html#model">ElmanRNN model class</a> as an example. Luckily PyTorch has a <code class="docutils literal notranslate"><span class="pre">nn.RNN()</span></code> function that looks after this recurrent behaviour for us.</p>
<p>The example below creates the same set of weights, but with <code class="docutils literal notranslate"><span class="pre">l0</span></code> suffix for the weight matrix keys, to indicate these weights are the first layer of RNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">rnn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight_ih_l0&#39;,
              tensor([[ 0.6627, -0.4245],
                      [ 0.5373,  0.2294]])),
             (&#39;weight_hh_l0&#39;,
              tensor([[-0.4015, -0.5385],
                      [-0.1956, -0.6835]])),
             (&#39;bias_ih_l0&#39;, tensor([0.4954, 0.6533])),
             (&#39;bias_hh_l0&#39;, tensor([-0.3565, -0.2904]))])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sequence-first-shape-default-in-rnn">
<h2><span class="section-number">1.4. </span>Sequence-first shape default in RNN<a class="headerlink" href="#sequence-first-shape-default-in-rnn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="generate-some-synthetic-data">
<h3><span class="section-number">1.4.1. </span>Generate some synthetic data<a class="headerlink" href="#generate-some-synthetic-data" title="Permalink to this headline">¶</a></h3>
<p>The code below generate random sequences of four points (<code class="docutils literal notranslate"><span class="pre">points</span></code> e.g. A, B, C, D) Each point has two values, which can be think of a data point in a 2D space. The sequence are points in sequence that are ordered either clock-wise or counter-wise (<code class="docutils literal notranslate"><span class="pre">direction</span></code>). This is a simplified version of sentences,</p>
<ul class="simple">
<li><p>sentences are sequences (of words);</p></li>
<li><p>the order or direction of the above sequence is analogus to classes, e.g. sentiment or news categories</p></li>
<li><p>words are elements in the sequence, representing as vectors with dimensions.</p></li>
</ul>
<p>The example data points has a feature space of two dimensions, so they can be easily visualised using a x-y Cartesian coordinate system, whereas words may be in 50, 100, 300 etc. dimensions depending on the embedding methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">generate_sequences</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">variable_len</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">):</span>
    <span class="n">basic_corners</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">bases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">variable_len</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span>
    <span class="n">directions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">basic_corners</span><span class="p">[[(</span><span class="n">b</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]][</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">d</span><span class="o">*</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">)][:</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bases</span><span class="p">,</span> <span class="n">directions</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">directions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">points</span><span class="p">,</span> <span class="n">directions</span> <span class="o">=</span> <span class="n">generate_sequences</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="batch-first-tensor">
<h3><span class="section-number">1.4.2. </span>Batch-first tensor<a class="headerlink" href="#batch-first-tensor" title="Permalink to this headline">¶</a></h3>
<p>We now take three (N=3) sequences, each sequence has four data points (L=4), with two features (F=2) representing each data point. This is an example of batch-first input tensor (N,L,F), as shown in the example below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">batch</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 4, 2])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="how-to-use-rnn-with-correctly-shaped-tensors">
<h3><span class="section-number">1.4.3. </span>How to use RNN with correctly shaped tensors<a class="headerlink" href="#how-to-use-rnn-with-correctly-shaped-tensors" title="Permalink to this headline">¶</a></h3>
<p>However, RNN uses <em>sequence-first</em> by default (L,N,F), we need to make our tensor <strong>RNN friendly</strong>. Two options:</p>
<ol class="simple">
<li><p>We could explicitly change the shape of the batch using <code class="docutils literal notranslate"><span class="pre">permute()</span></code> to flip the first two dimensions.</p></li>
<li><p>We could use the <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> argument in the RNN layer construction.</p></li>
</ol>
<div class="section" id="option-1">
<h4><span class="section-number">1.4.3.1. </span>Option 1<a class="headerlink" href="#option-1" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># From a batch-first tensor to sequence-first</span>
<span class="n">permuted_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">permuted_batch</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 3, 2])
</pre></div>
</div>
</div>
</div>
<p>Once the data is in an “RNN-friendly” shape and we can run it through a regular RNN to get two sequence-first tensors back:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">final_hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">permuted_batch</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">final_hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([4, 3, 2]), torch.Size([1, 3, 2]))
</pre></div>
</div>
</div>
</div>
<p>Once we’re done with the RNN we can turn the data back to our familiar batchfirst
shape:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_hidden</span> <span class="o">=</span> <span class="n">final_hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">batch</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 4, 2])
</pre></div>
</div>
</div>
</div>
<div class="admonition-your-turn admonition">
<p class="admonition-title">Your Turn</p>
<p>In the code above, the hidden state dimension (<code class="docutils literal notranslate"><span class="pre">hidden_dim</span></code>) happens to be the same as the number of input features (<code class="docutils literal notranslate"><span class="pre">n_features</span></code>), which is 2. These two do not have to agree. For example, the word embedding dimension can be 100 (e.g. <code class="docutils literal notranslate"><span class="pre">n_features=100</span></code>), the hidden dimension can be 50 (<code class="docutils literal notranslate"><span class="pre">hidden_dim=50</span></code>). Change these two parameters to observe the shape change and get familar with the sequence-first and batch-first tensor shapes.</p>
</div>
</div>
<div class="section" id="option-2">
<h4><span class="section-number">1.4.3.2. </span>Option 2<a class="headerlink" href="#option-2" title="Permalink to this headline">¶</a></h4>
<p>Option 1 is a lot of work to keep track of, we can instead set RNN’s <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> argument to True so we can use the batch above without any modifications.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>But you get these two distinct shapes as a result: batch-first (N,L,H) for the
output and sequence-first (1,N,H) for the final hidden state.</p>
</div>
<p>On the one hand, this can lead to confusion. On the other hand, most of the time we
would not be handling the hidden state, and we will handle the <em>batch-first</em> output instead.
So, we can stick with batch-first for now and, when it comes the time we have to
handle the hidden state, we will highlight the difference in shapes once again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn_batch_first</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">final_hidden</span> <span class="o">=</span> <span class="n">rnn_batch_first</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">final_hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([3, 4, 2]), torch.Size([1, 3, 2]))
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For simple RNNs, the last element of the output IS the final hidden state!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">final_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(True)
</pre></div>
</div>
</div>
</div>
<div class="admonition-summary admonition">
<p class="admonition-title">Summary</p>
<p>The RNN’s default behavior is to handle tensors having the shape (L,N,H) for hidden states and (L,N,F) for sequences of data points.</p>
<p>Datasets and data loaders, unless customized otherwise, will produce data points in the shape (N,L,F).</p>
<p>To address this difference, we’ll be using OPTION 2 the <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> argument to turn both inputs and outputs into this familiar batch-first shape. But be aware of the shape difference between hidden state and output state. In other words, with <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> argument to be true, we have input and output “batch-first”, but the hidden states are still “sequence-first”.</p>
</div>
</div>
</div>
</div>
<div class="section" id="stacked-rnn-with-two-layers">
<h2><span class="section-number">1.5. </span>Stacked RNN with Two Layers<a class="headerlink" href="#stacked-rnn-with-two-layers" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn_stacked</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">rnn_stacked</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight_ih_l0&#39;,
              tensor([[ 0.6627, -0.4245],
                      [ 0.5373,  0.2294]])),
             (&#39;weight_hh_l0&#39;,
              tensor([[-0.4015, -0.5385],
                      [-0.1956, -0.6835]])),
             (&#39;bias_ih_l0&#39;, tensor([0.4954, 0.6533])),
             (&#39;bias_hh_l0&#39;, tensor([-0.3565, -0.2904])),
             (&#39;weight_ih_l1&#39;,
              tensor([[-0.6701, -0.5811],
                      [-0.0170, -0.5856]])),
             (&#39;weight_hh_l1&#39;,
              tensor([[ 0.1159, -0.6978],
                      [ 0.3241, -0.0983]])),
             (&#39;bias_ih_l1&#39;, tensor([-0.3163, -0.2153])),
             (&#39;bias_hh_l1&#39;, tensor([ 0.0722, -0.3242]))])
</pre></div>
</div>
</div>
</div>
<div class="section" id="manually-stacking-two-rnns">
<h3><span class="section-number">1.5.1. </span>Manually Stacking Two RNNs<a class="headerlink" href="#manually-stacking-two-rnns" title="Permalink to this headline">¶</a></h3>
<p>Let’s replicate the above with two RNNs, “manually” stacked together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;weight_ih_l0&#39;,
  tensor([[ 0.6627, -0.4245],
          [ 0.5373,  0.2294]])),
 (&#39;weight_hh_l0&#39;,
  tensor([[-0.4015, -0.5385],
          [-0.1956, -0.6835]])),
 (&#39;bias_ih_l0&#39;, tensor([0.4954, 0.6533])),
 (&#39;bias_hh_l0&#39;, tensor([-0.3565, -0.2904])),
 (&#39;weight_ih_l1&#39;,
  tensor([[-0.6701, -0.5811],
          [-0.0170, -0.5856]])),
 (&#39;weight_hh_l1&#39;,
  tensor([[ 0.1159, -0.6978],
          [ 0.3241, -0.0983]])),
 (&#39;bias_ih_l1&#39;, tensor([-0.3163, -0.2153])),
 (&#39;bias_hh_l1&#39;, tensor([ 0.0722, -0.3242]))]
</pre></div>
</div>
</div>
</div>
<div class="admonition-your-turn admonition">
<p class="admonition-title">Your Turn</p>
<p>Give a string <code class="docutils literal notranslate"><span class="pre">k</span></code>, what does <code class="docutils literal notranslate"><span class="pre">k[:-1]</span></code> do?</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>
<span class="nb">str</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;te&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">dict</span><span class="p">([(</span><span class="n">k</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">4</span><span class="p">:]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;weight_ih_l0&#39;: tensor([[-0.6701, -0.5811],
         [-0.0170, -0.5856]]),
 &#39;weight_hh_l0&#39;: tensor([[ 0.1159, -0.6978],
         [ 0.3241, -0.0983]]),
 &#39;bias_ih_l0&#39;: tensor([-0.3163, -0.2153]),
 &#39;bias_hh_l0&#39;: tensor([ 0.0722, -0.3242])}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create two RNNs</span>
<span class="n">rnn_layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">rnn_layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Load the same weights from above</span>
<span class="n">rnn_layer0</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">4</span><span class="p">]))</span>
<span class="c1"># Note the layer label (keys) need to change</span>
<span class="n">rnn_layer1</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">([(</span><span class="n">k</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">4</span><span class="p">:]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<p>Step 0: A batch sequence from the sample (N=1, L=4, F=2)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Step 1: Feed the input to the first RNN layer</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out0</span><span class="p">,</span> <span class="n">h0</span> <span class="o">=</span> <span class="n">rnn_layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Step 2: Feed the output of the first layer to the second RNN layer</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out1</span><span class="p">,</span> <span class="n">h1</span> <span class="o">=</span> <span class="n">rnn_layer1</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The overall output of the stacked RNN must have two elements as well:</p>
<ul class="simple">
<li><p>a sequence of hidden states, those produced by the last layer (<code class="docutils literal notranslate"><span class="pre">out1</span></code>)</p></li>
<li><p>the concatenation of final hidden states of all layers</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h0</span><span class="p">,</span> <span class="n">h1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[-0.7533, -0.7711],
          [-0.0566, -0.5960],
          [ 0.4324, -0.2908],
          [ 0.1563, -0.5152]]], grad_fn=&lt;TransposeBackward1&gt;),
 tensor([[[-0.5297,  0.3551]],
 
         [[ 0.1563, -0.5152]]], grad_fn=&lt;CatBackward&gt;))
</pre></div>
</div>
</div>
</div>
<p>This should be the same as running a stacked RNN, as shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn_stacked</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[-0.7533, -0.7711],
          [-0.0566, -0.5960],
          [ 0.4324, -0.2908],
          [ 0.1563, -0.5152]]], grad_fn=&lt;TransposeBackward1&gt;),
 tensor([[[-0.5297,  0.3551]],
 
         [[ 0.1563, -0.5152]]], grad_fn=&lt;StackBackward&gt;))
</pre></div>
</div>
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For stacked RNNs, the last element of the output is the final hidden state of the LAST LAYER!</p>
</div>
<p>But, since we’re using a batch_first layer, we need to permute the hidden state’s
dimensions to batch-first as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(True)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="bidirectional-rnn">
<h2><span class="section-number">1.6. </span>Bidirectional RNN<a class="headerlink" href="#bidirectional-rnn" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">rnn_bidirect</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">rnn_bidirect</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight_ih_l0&#39;,
              tensor([[ 0.6627, -0.4245],
                      [ 0.5373,  0.2294]])),
             (&#39;weight_hh_l0&#39;,
              tensor([[-0.4015, -0.5385],
                      [-0.1956, -0.6835]])),
             (&#39;bias_ih_l0&#39;, tensor([0.4954, 0.6533])),
             (&#39;bias_hh_l0&#39;, tensor([-0.3565, -0.2904])),
             (&#39;weight_ih_l0_reverse&#39;,
              tensor([[-0.6701, -0.5811],
                      [-0.0170, -0.5856]])),
             (&#39;weight_hh_l0_reverse&#39;,
              tensor([[ 0.1159, -0.6978],
                      [ 0.3241, -0.0983]])),
             (&#39;bias_ih_l0_reverse&#39;, tensor([-0.3163, -0.2153])),
             (&#39;bias_hh_l0_reverse&#39;, tensor([ 0.0722, -0.3242]))])
</pre></div>
</div>
</div>
</div>
<div class="section" id="manually-created-bidirectional-rnn">
<h3><span class="section-number">1.6.1. </span>Manually Created Bidirectional RNN<a class="headerlink" href="#manually-created-bidirectional-rnn" title="Permalink to this headline">¶</a></h3>
<p>Once again, we can create two simple RNNs, and use the weights and biases above to
set their weights accordingly. Each RNN will behave as one of the layers from the
bidirectional one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnn_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">rnn_reverse</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">rnn_forward</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">rnn_reverse</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">([(</span><span class="n">k</span><span class="p">[:</span><span class="o">-</span><span class="mi">8</span><span class="p">],</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">4</span><span class="p">:]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="step-0-a-batch-sequence-and-its-reverse">
<h4><span class="section-number">1.6.1.1. </span>Step 0: A batch sequence and its reverse<a class="headerlink" href="#step-0-a-batch-sequence-and-its-reverse" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x_rev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1">#N, L, F</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_rev</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.0349,  0.9661],
         [ 0.8055, -0.9169],
         [-0.8251, -0.9499],
         [-0.8670,  0.9342]]])
tensor([[[-0.8670,  0.9342],
         [-0.8251, -0.9499],
         [ 0.8055, -0.9169],
         [ 1.0349,  0.9661]]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-1-feed-each-rnn-with-its-corresponding-sequence">
<h4><span class="section-number">1.6.1.2. </span>Step 1: Feed each RNN with its corresponding sequence<a class="headerlink" href="#step-1-feed-each-rnn-with-its-corresponding-sequence" title="Permalink to this headline">¶</a></h4>
<p>Since there is no dependency between the two layers, we just need to feed each
layer its corresponding sequence (regular and reversed) and remember to reverse
back the sequence of hidden states.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out_rev</span><span class="p">,</span> <span class="n">h_rev</span> <span class="o">=</span> <span class="n">rnn_reverse</span><span class="p">(</span><span class="n">x_rev</span><span class="p">)</span>
<span class="n">out_rev_back</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">out_rev</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="step-2-tidy-up-the-output">
<h3><span class="section-number">1.6.2. </span>Step 2: Tidy up the output<a class="headerlink" href="#step-2-tidy-up-the-output" title="Permalink to this headline">¶</a></h3>
<p>The overall output of the bidirectional RNN must have two elements as well:</p>
<ul class="simple">
<li><p>a concatenation side-by-side of both sequences of hidden states (out and
out_rev_back)</p></li>
<li><p>the concatenation of final hidden states of both layers</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.3924,  0.8146],
         [ 0.4347, -0.0481],
         [-0.1521, -0.3367],
         [-0.5297,  0.3551]]], grad_fn=&lt;TransposeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out_rev_back</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.9355, -0.8353],
         [-0.1766,  0.2596],
         [ 0.8829,  0.0425],
         [-0.2032, -0.7901]]], grad_fn=&lt;FlipBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">out</span><span class="p">,</span> <span class="n">out_rev_back</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h</span><span class="p">,</span> <span class="n">h_rev</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 0.3924,  0.8146, -0.9355, -0.8353],
          [ 0.4347, -0.0481, -0.1766,  0.2596],
          [-0.1521, -0.3367,  0.8829,  0.0425],
          [-0.5297,  0.3551, -0.2032, -0.7901]]], grad_fn=&lt;CatBackward&gt;),
 tensor([[[-0.5297,  0.3551]],
 
         [[-0.9355, -0.8353]]], grad_fn=&lt;CatBackward&gt;))
</pre></div>
</div>
</div>
</div>
<p>Double check the results with the bi-directional RNN itself</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn_bidirect</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 0.3924,  0.8146, -0.9355, -0.8353],
          [ 0.4347, -0.0481, -0.1766,  0.2596],
          [-0.1521, -0.3367,  0.8829,  0.0425],
          [-0.5297,  0.3551, -0.2032, -0.7901]]], grad_fn=&lt;TransposeBackward1&gt;),
 tensor([[[-0.5297,  0.3551]],
 
         [[-0.9355, -0.8353]]], grad_fn=&lt;StackBackward&gt;))
</pre></div>
</div>
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For bidirectional RNNs, the last element of the output <strong>ISN’T</strong> the final hidden state! Once again, since we’re using a <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> layer, we need to permute the hidden state’s dimensions to batch-first as well:</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ True,  True, False, False]])
</pre></div>
</div>
</div>
</div>
<p>Bidirectional RNNs are different because the final hidden state corresponds to the last element in the sequence for the forward layer and to the first element in the sequence for the reverse
layer. The output, on the other hand, is aligned to sequence, hence the difference.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./rnn"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="intro.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Lab10: RNN for NLP</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="elman_rnn_square.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">2. </span>Classifying Synthetic Sequences - The Square Model</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By A/Prof. Wei Liu<br/>
        
            &copy; Copyright UWA 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>