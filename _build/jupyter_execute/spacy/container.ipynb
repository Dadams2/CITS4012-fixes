{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Container Objects in spaCy\r\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Container objects in spaCy mimic the structure of natural language texts: a text is composed of sentences, and each sentence contains tokens. Token, Span, and Doc, the most widely used container objects in spaCy from a user's standpoint, represent a token, a phrase or sentence, and a text, respectively. A container can contain other containers - for example, a Doc contains Tokens. In this section, we’ll explore working with these container objects. \r\n",
    "\r\n",
    "## Doc\r\n",
    "\r\n",
    "The `Doc()` constructor, requires two parameters:\r\n",
    "\r\n",
    "-   a vocab object, which is a storage container that provides\r\n",
    "    vocabulary data, such as lexical types (adjective, verb, noun ...);\r\n",
    "\r\n",
    "-   a list of tokens to add to the Doc object being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! \n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens.doc import Doc\r\n",
    "from spacy.vocab import Vocab\r\n",
    "\r\n",
    "\"\"\" \r\n",
    "create a spacy.tokens.doc.Doc object\r\n",
    "using its constructor\r\n",
    "\"\"\"\r\n",
    "doc = Doc(Vocab(), words = [u'Hello', u'World!'])\r\n",
    "print(doc)\r\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token\r\n",
    "\r\n",
    "spaCy's Token object is a container for a set of annotations related to a single token, such as that token's part of speech.\r\n",
    "\r\n",
    "A Doc object contains a collection of the Token objects generated as a result of the tokenization performed on a submitted text. These tokens have indices, allowing you to access them based on their positions in the text.\r\n",
    "\r\n",
    "![image](../images/spacy-doc-container.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'a', 'green', 'apple', '.']\n",
      "['I', 'want', 'a', 'green', 'apple', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "doc = nlp(u'I want a green apple.')\r\n",
    "# token_text1 and token_text2 produce the same results\r\n",
    "token_text1 = [token.text for token in doc]\r\n",
    "token_text2 = [doc[i].text for i in range(len(doc))]\r\n",
    "print(token_text1)\r\n",
    "print(token_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Token.lefts` `Token.rights` and `Token.children`\r\n",
    "\r\n",
    "![image](../images/spacy-left-children.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a, green]\n",
      "[a, green]\n",
      "[apple, .]\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "doc = nlp(u'I want a green apple.')\r\n",
    "print([t for t in doc[4].lefts])\r\n",
    "print([t for t in doc[4].children])\r\n",
    "print([t for t in doc[1].rights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\r\n",
    "\r\n",
    "![image](../images/spacy-vocab-stringstore.png)\r\n",
    "\r\n",
    "-   Whenever possible, spaCy tries to store data in a vocabulary, the\r\n",
    "    <span>Vocab</span> storage class, that will be shared by multiple\r\n",
    "    documents;\r\n",
    "\r\n",
    "-   To save memory, spaCy also encodes all strings to hash values. For\r\n",
    "    example, “coffee” has the hash 3197928453018144401.\r\n",
    "\r\n",
    "-   Entity labels like “ORG” and part-of-speech tags like “VERB” are\r\n",
    "    also encoded.\r\n",
    "\r\n",
    "[spaCy 101](https://spacy.io/usage/spacy-101#vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n",
      "! 17494803046312582752 ! ! ! False False False en\n",
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy \r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "doc = nlp('I love coffee!')\r\n",
    "for token in doc:\r\n",
    "    lexeme = doc.vocab[token.text]\r\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, \r\n",
    "          lexeme.prefix_, lexeme.suffix_, lexeme.is_alpha, \r\n",
    "          lexeme.is_digit, lexeme.is_title, lexeme.lang_)\r\n",
    "\r\n",
    "print(doc.vocab.strings[\"coffee\"]) # 3197928453018144401            \r\n",
    "print(doc.vocab.strings[3197928453018144401]) # 'coffee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span\r\n",
    "\r\n",
    "Span can be obtained as simple as <span>doc[start:end]</span> where\r\n",
    "<span>start</span> and <span>end</span> are the index of starting token\r\n",
    "and the ending token, respectively. The two indices can be\r\n",
    "\r\n",
    "-   manually specified; or\r\n",
    "\r\n",
    "-   computed through pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:  We can overtake\n",
      "The positions in the doc are:  0 - 3\n",
      "We can overtake NsubjAuxRoot\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "from spacy.matcher import Matcher\r\n",
    "from spacy.tokens import Doc, Span, Token\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "matcher = Matcher(nlp.vocab)\r\n",
    "# A dependency label pattern that matches a word sequence \r\n",
    "pattern = [{\"DEP\": \"nsubj\"},{\"DEP\": \"aux\"},{\"DEP\": \"ROOT\"}]\r\n",
    "matcher.add(\"NsubjAuxRoot\", [pattern])\r\n",
    "doc = nlp(u\"We can overtake them.\")\r\n",
    "# 1. Return (match_id, start, end) tuples\r\n",
    "matches = matcher(doc)\r\n",
    "for match_id, start, end in matches:\r\n",
    "    span = doc[start:end]\r\n",
    "    print(\"Span: \", span.text)\r\n",
    "    print(\"The positions in the doc are: \", start, \"-\", end)\r\n",
    "# 2. Return Span objects directly\r\n",
    "matches = matcher(doc, as_spans=True)\r\n",
    "for span in matches:\r\n",
    "    print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc.noun_chunks and Retokenising\r\n",
    "\r\n",
    "A **noun chunk** is a phrase that has a noun as its head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Golden Gate Bridge <class 'spacy.tokens.span.Span'>\n",
      "an iconic landmark <class 'spacy.tokens.span.Span'>\n",
      "San Francisco <class 'spacy.tokens.span.Span'>\n",
      "The Golden Gate Bridge\n",
      "is\n",
      "an iconic landmark\n",
      "in\n",
      "San Francisco\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\r\n",
    "\r\n",
    "# Retokenize to treat each noun_chunk as a single token\r\n",
    "with doc.retokenize() as retokenizer:  \r\n",
    "  for chunk in doc.noun_chunks:\r\n",
    "    print(chunk.text + ' ' + str(type(chunk)))\r\n",
    "    retokenizer.merge(chunk)\r\n",
    "    #doc.retokenize().merge(chunk)\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "  print(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc.sents\r\n",
    "\r\n",
    "the Doc object’s doc.sents property is an\r\n",
    "iterator over the sentences in a Doc object. For this reason, you can’t use\r\n",
    "this property to refer to sentences by index, but you can iterate over them\r\n",
    "in a loop or create a list of Span objects where each span represents a sentence.\r\n",
    "-   Doc object's `doc.sents` property is an `generator` object, i.e. an iterator over the sentences in a Doc object. You can\r\n",
    "    use <span>for</span> each <span>in</span> loop, but not subset\r\n",
    "    indexing. \r\n",
    "\r\n",
    "-   Each member of the generator object is a <span>Span</span> of type\r\n",
    "    <span>spacy.tokens.span.Span</span>.\r\n",
    "\r\n",
    ":::{admonition} Tip\r\n",
    ":class: tip\r\n",
    "`spans = list(doc.sents)` will return a list of span objects that each represent a sentence. \r\n",
    ":::        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[A, storm, hit, the, beach, ., It, started, to, rain, .]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "doc = nlp(u'A storm hit the beach. It started to rain.')\r\n",
    "for sent in doc.sents:\r\n",
    "  print(type(sent))\r\n",
    "  # Sentence level index\r\n",
    "  [sent[i] for i in range(len(sent))]\r\n",
    "# Doc level index  \r\n",
    "[doc[i] for i in range(len(doc))] "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5f76115fc97e95e5e5f274d9a04d3733842aec59c7431144116cf5affd2efc8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('lda': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}