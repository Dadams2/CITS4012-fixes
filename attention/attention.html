
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Attention Mechanism &#8212; CITS4012 Natural Language Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Self-Attention" href="self_attention.html" />
    <link rel="prev" title="Lab12: Sequence to Sequence Learning with Attention" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo_ntlp.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CITS4012 Natural Language Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   HOME
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/intro.html">
   Lab 01: Conda Environment and Python Refresher
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/installation.html">
     1. CITS4012 Base Environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/installation_misc.html">
     2. CITS4012 MISC Enviornment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lab_machines.html">
     3. Use Lab Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/python_review.html">
     4. Python Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/iterables.html">
     5. Iterables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/numpy.html">
     6. Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/matplotlib.html">
     7. Matplotlib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../NLTK/intro.html">
   Lab02: NLTK
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/start.html">
     1. Starting with NLTK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/closer_look.html">
     2. A Closer Look at Python: Texts as Lists of Words
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/computing.html">
     3. Computing with Language: Simple Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/take_control.html">
     4. Back to Python: Making Decisions and Taking Control
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLTK/exercises.html">
     5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../spacy/intro.html">
   Lab03: spaCy NLP pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/container.html">
     1. Container Objects in spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/pipeline.html">
     2. NLP Pipelines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/patterns.html">
     3. Finding Patterns
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/telegram.html">
     4. Your first chatbot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../spacy/exercise.html">
     5. Exercise
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../gensim/intro.html">
   Lab04: Count-Based Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../gensim/tf-idf.html">
     1. TF-IDF in scikit-learn and Gensim
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gensim/classification.html">
     2. Document Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../nn/intro.html">
   Lab05: Introduction to Neural Networks and Pytorch
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/linear_model_numpy.html">
     1. Linear Models in Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/tensors.html">
     2. Introduction to Pytorch Tensors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/linear_model_pytorch.html">
     3. Linear Models in Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../pytorch/intro.html">
   Lab06: Neural Network Building Blocks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/activations.html">
     1. Activation Functions and their derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/loss.html">
     2. Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/computational_graph.html">
     3. Dynamic Computational Graph in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pytorch/nn_oop.html">
     4. Neural Networks in PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../embeddings/intro.html">
   Lab07: Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/svd.html">
     1. Word Vectors from Word-Word Coocurrence Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/glove.html">
     2. GloVe: Global Vectors for Word Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../embeddings/word2vec.html">
     3. Word2Vec
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../classification/intro.html">
   Lab08: Document Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/perceptron.html">
     1. Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/data_prep.html">
     2. Dataset and DataLoader
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/yelp_preprocessing.html">
     3. Yelp Dataset at a glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../classification/yelp.html">
     4. Yelp Review Dataset - Document Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cnn/intro.html">
   Lab09: CNN for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/dataset_frankenstein_processing.html">
     1. Frankenstein Dataset At a Glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/CBOW.html">
     2. Learning Embeddings with Continuous Bag of Words (CBOW)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/convolution.html">
     3. Convolution Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/dataset_AG_News_processing.html">
     4. AG News Dataset at A Glance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cnn/Document_Classification_with_CNN.html">
     5. Using CNN for Document Classification with Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../rnn/intro.html">
   Lab10: RNN for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../rnn/rnn.html">
     1. Recurrent Neural Networks - Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rnn/elman_rnn_square.html">
     2. Classifying Synthetic Sequences - The Square Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rnn/Surname_Dataset.html">
     3. Surname Dataset Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rnn/Surname_Classification.html">
     4. Surname Classification with RNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../LSTM/intro.html">
   Lab11: GRU, LSTM and Seq2Seq
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/gru.html">
     1. Gated Recurrent Units (GRUs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/lstm.html">
     2. Long Short Term Memories (LSTMs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/gru_lstm_square.html">
     3. The Square Model Using GRU and LSTM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/seq2seq.html">
     4. Sequence to Sequence Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/Surname_Generation_Unconditioned.html">
     5. Surname Generation - Unconditioned
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../LSTM/Surname_Generation_Conditioned.html">
     6. Surname Generation Conditioned
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   Lab12: Sequence to Sequence Learning with Attention
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1. Attention Mechanism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="self_attention.html">
     2. Self-Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nmt_data_processing.html">
     3. Neual Machine Translation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="PackedSequence.html">
     4. Packed Sequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NMT_No_Sampling.html">
     5. Neural Machine Translation - No Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NMT_scheduled_sampling.html">
     6. Neural Machine Translation - Scheduled Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../transformer/intro.html">
   Extra: Transformers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/position_encoding.html">
     1. Positional Encoding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/LayerNorm.html">
     2. Layer Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../transformer/transformer.html">
     3. Transform and Roll Out
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://drive.google.com/file/d/1AQxhGLhoHE162HALo1NkgdaN-LVY4QWo/view?usp=sharing">
   data.zip
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/weiliu2k/CITS4012/raw/master/CITS4012LabBook.pdf">
   PDF
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/attention/attention.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/attention/attention.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#download-untility-files-for-plotting-and-data-generation">
   1.1. Download Untility Files for Plotting and Data Generation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   1.2. Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-generation">
   1.3. Data Generation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plain-encoder-decoder">
   1.4. Plain Encoder-Decoder
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-mechanism-explained">
   1.5. Attention Mechanism Explained
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-illustration-of-attention-scores">
     1.5.1. An illustration of Attention Scores
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#context-vector">
     1.5.2. Context Vector
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#values-and-keys">
     1.5.3. “Values” and “Keys”
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#query">
     1.5.4. Query
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compute-the-attention-score">
     1.5.5. Compute the Attention Score
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scoring-method">
   1.6. Scoring Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dot-product">
     1.6.1. Dot product
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-scores">
     1.6.2. Attention Scores
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-the-context">
     1.6.3. Visualizing the Context
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaled-dot">
     1.6.4. Scaled Dot
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance-of-a-batch-dot-product-is-roughly-the-same-as-the-dimension-of-the-vector">
       1.6.4.1. Variance of a batch dot product is roughly the same as the dimension of the vector
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#encoder-decoder-with-attention-mechanism">
   1.7. Encoder Decoder with Attention Mechanism
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-class">
     1.7.1. Attention Class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#source-mask">
     1.7.2. Source Mask
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder-with-attention">
     1.7.3. Decoder with Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#encoder-decoder-attention">
     1.7.4. Encoder + Decoder + Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-preparation">
     1.7.5. Data Preparation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-configuration-training">
     1.7.6. Model Configuration &amp; Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-predictions">
     1.7.7. Visualizing Predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-attention">
     1.7.8. Visualizing Attention
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="attention-mechanism">
<h1><span class="section-number">1. </span>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we introduce the attention mechanism to our plain sequence to sequence model, as illustrated in the diagram below:</p>
<p><img alt="Attention Mechanism" src="../_images/encdec_attn.png" /></p>
<div class="section" id="download-untility-files-for-plotting-and-data-generation">
<h2><span class="section-number">1.1. </span>Download Untility Files for Plotting and Data Generation<a class="headerlink" href="#download-untility-files-for-plotting-and-data-generation" title="Permalink to this headline">¶</a></h2>
<p>Download these utility functions first and place them in the same directory as this notebook. These files are the same as the ones in <a class="reference external" href="https://weiliu2k.github.io/CITS4012/LSTM/seq2seq.html">Lab11 Sequence to Sequence Model</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">FileLink</span><span class="p">,</span> <span class="n">FileLinks</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FileLink</span><span class="p">(</span><span class="s1">&#39;plots.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href='plots.py' target='_blank'>plots.py</a><br></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FileLink</span><span class="p">(</span><span class="s1">&#39;plots_seq2seq.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href='plots_seq2seq.py' target='_blank'>plots_seq2seq.py</a><br></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FileLink</span><span class="p">(</span><span class="s1">&#39;util.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href='util.py' target='_blank'>util.py</a><br></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FileLink</span><span class="p">(</span><span class="s1">&#39;replay.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href='replay.py' target='_blank'>replay.py</a><br></div></div>
</div>
</div>
<div class="section" id="imports">
<h2><span class="section-number">1.2. </span>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">random_split</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">util</span> <span class="kn">import</span> <span class="n">StepByStep</span>
<span class="kn">from</span> <span class="nn">plots</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">plots_seq2seq</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-generation">
<h2><span class="section-number">1.3. </span>Data Generation<a class="headerlink" href="#data-generation" title="Permalink to this headline">¶</a></h2>
<p>We still make use of the Square Sequences, using the first two corners to predict the last two. Same method as before for generating noisy squares.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_sequences</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">variable_len</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">):</span>
    <span class="n">basic_corners</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">bases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">variable_len</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span>
    <span class="n">directions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">basic_corners</span><span class="p">[[(</span><span class="n">b</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]][</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">d</span><span class="o">*</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">)][:</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bases</span><span class="p">,</span> <span class="n">directions</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">directions</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plain-encoder-decoder">
<h2><span class="section-number">1.4. </span>Plain Encoder-Decoder<a class="headerlink" href="#plain-encoder-decoder" title="Permalink to this headline">¶</a></h2>
<p>We use exactly the same encoder-decoder from Lab11’s <a class="reference external" href="https://weiliu2k.github.io/CITS4012/LSTM/seq2seq.html">sequence to sequence notebook</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>        
        <span class="n">rnn_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">rnn_out</span> <span class="c1"># N, L, F</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">regression</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_seq</span><span class="p">):</span>
        <span class="c1"># We only need the final hidden state</span>
        <span class="n">hidden_final</span> <span class="o">=</span> <span class="n">hidden_seq</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># N, 1, H</span>
        <span class="c1"># But we need to make it sequence-first</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden_final</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 1, N, H                      </span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># X is N, 1, F</span>
        <span class="n">batch_first_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span> 
        
        <span class="n">last_output</span> <span class="o">=</span> <span class="n">batch_first_output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span><span class="n">last_output</span><span class="p">)</span>
        
        <span class="c1"># N, 1, F</span>
        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>               
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">teacher_forcing_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_len</span> <span class="o">=</span> <span class="n">input_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_len</span> <span class="o">=</span> <span class="n">target_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher_forcing_prob</span> <span class="o">=</span> <span class="n">teacher_forcing_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">def</span> <span class="nf">init_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="c1"># N, L (target), F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> 
                              <span class="bp">self</span><span class="o">.</span><span class="n">target_len</span><span class="p">,</span> 
                              <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
        <span class="c1"># Stores the output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">out</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>               
        <span class="c1"># splits the data in source and target sequences</span>
        <span class="c1"># the target seq will be empty in testing mode</span>
        <span class="c1"># N, L, F</span>
        <span class="n">source_seq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">input_len</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">target_seq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_len</span><span class="p">:,</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_outputs</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>        
        
        <span class="c1"># Encoder expected N, L, F</span>
        <span class="n">hidden_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">source_seq</span><span class="p">)</span>
        <span class="c1"># Output is N, L, H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">hidden_seq</span><span class="p">)</span>
        
        <span class="c1"># The last input of the encoder is also</span>
        <span class="c1"># the first input of the decoder</span>
        <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">source_seq</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>
        
        <span class="c1"># Generates as many outputs as the target length</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_len</span><span class="p">):</span>
            <span class="c1"># Output of decoder is N, 1, F</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">store_output</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
            
            <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">teacher_forcing_prob</span>
            <span class="c1"># In evaluation/test the target sequence is</span>
            <span class="c1"># unknown, so we cannot use teacher forcing</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="mi">0</span>
                
            <span class="c1"># If it is teacher forcing</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">prob</span><span class="p">:</span>
                <span class="c1"># Takes the actual element</span>
                <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">target_seq</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Otherwise uses the last predicted output</span>
                <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">out</span>
            
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="attention-mechanism-explained">
<h2><span class="section-number">1.5. </span>Attention Mechanism Explained<a class="headerlink" href="#attention-mechanism-explained" title="Permalink to this headline">¶</a></h2>
<div class="section" id="an-illustration-of-attention-scores">
<h3><span class="section-number">1.5.1. </span>An illustration of Attention Scores<a class="headerlink" href="#an-illustration-of-attention-scores" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">figure9</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_16_0.png" src="../_images/attention_16_0.png" />
</div>
</div>
</div>
<div class="section" id="context-vector">
<h3><span class="section-number">1.5.2. </span>Context Vector<a class="headerlink" href="#context-vector" title="Permalink to this headline">¶</a></h3>
<p>The context vector is the weighted average of encoder hidden states.</p>
<div class="math notranslate nohighlight">
\[
context\ vector = \underbrace{\alpha_0 * h_0}_{alignment\ vector_0} + \underbrace{\alpha_1 * h_1}_{alignment\ vector_1} = 0.8 * value_{the} + 0.2 *value_{zone}
\]</div>
<p>But how do we compute alphas - attention scores?</p>
<p>Let’s use our own sequence-to-sequence problem, and the “perfect” square as input to illustrate how the context vector is computed.</p>
<p>A sequence <code class="docutils literal notranslate"><span class="pre">full_seq</span></code> is splitted into source and target sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">source_seq</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">target_seq</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="values-and-keys">
<h3><span class="section-number">1.5.3. </span>“Values” and “Keys”<a class="headerlink" href="#values-and-keys" title="Permalink to this headline">¶</a></h3>
<p>The source sequence is the input of the encoder. The hidden states the encoder outputs are going to be both “values” (V) and “keys” (K):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">hidden_seq</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">source_seq</span><span class="p">)</span>

<span class="n">values</span> <span class="o">=</span> <span class="n">hidden_seq</span> <span class="c1"># N, L, H</span>
<span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.0832, -0.0356],
         [ 0.3105, -0.5263]]], grad_fn=&lt;TransposeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="n">hidden_seq</span> <span class="c1"># N, L, H</span>
<span class="n">keys</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.0832, -0.0356],
         [ 0.3105, -0.5263]]], grad_fn=&lt;TransposeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="query">
<h3><span class="section-number">1.5.4. </span>Query<a class="headerlink" href="#query" title="Permalink to this headline">¶</a></h3>
<p>The encoder-decoder dynamics stay exactly the same: despite that we are sending the entire sequence of hidden states to the decoder, we still use the encoder’s final hidden state as the decoder’s initial hidden state.</p>
<p>In this example of using the first two corners to predict the last two, we still use the last element of the source sequence as input to the first step of the decoder:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">hidden_seq</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">source_seq</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first “query” (Q) is the decoder’s hidden state (remember, hidden states are
always sequence-first, so we permute it to batch-first):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># N, 1, H</span>
<span class="n">query</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.3913, -0.6853]]], grad_fn=&lt;PermuteBackward&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="compute-the-attention-score">
<h3><span class="section-number">1.5.5. </span>Compute the Attention Score<a class="headerlink" href="#compute-the-attention-score" title="Permalink to this headline">¶</a></h3>
<p>Once we have the “keys” and a “query”, we can compute attention scores (alphas) using them. This is for illustration only, we will progressively develop the <code class="docutils literal notranslate"><span class="pre">calc_alphas()</span></code> to reflect the true processing in the digram.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The alphas here do not make use of the values of keys and query, they are simply ones averaged by the sequence length.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_alphas</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="n">ks</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="n">L</span>
    <span class="k">return</span> <span class="n">alphas</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="n">calc_alphas</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="n">alphas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.5000, 0.5000]]])
</pre></div>
</div>
</div>
</div>
<p>We had to make sure alphas had the right shape (N, 1, L) so that, when multiplied by the “values” with shape (N, L, H), it will result in a weighted sum of the alignment vectors with shape (N, 1, H). We can use batch matrix multiplication (torch.bmm) for that:</p>
<div class="math notranslate nohighlight">
\[(\textcolor{green}{N}, 1, \textcolor{red}{L}) \times (\textcolor{green}{N}, \textcolor{red}{L}, H) = (N, 1, H)\]</div>
<p>We can simply ignore the first dimension and PyTorch will go over all the elements in the mini-batch for us.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Why are we spending so much time on shapes and matrix multiplication?</p>
<p>Although it seems a fairly basic topic, getting the shapes and dimensions right is of utmost importance for the correct implementation of an algorithm or technique.
Worst case is when using the wrong dimensions in an operation, PyTorch <em>may not raise an explicit error</em>, causing the almost undetectable Logical Error. This will ultimately damage the model’s ability to learn.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># N, 1, L x N, L, H -&gt; 1, L x L, H -&gt; 1, H</span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="n">context_vector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.1968, -0.2809]]], grad_fn=&lt;BmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Once the context vector is ready, we can concatenate it to the “query” (the
decoder’s hidden state) and use it as the input for the linear layer that actually
generates the predicted coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">concatenated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">concatenated</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.1968, -0.2809,  0.3913, -0.6853]]], grad_fn=&lt;CatBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>In summary, the above can be summarised into the following steps of a typical attention mechanism:</p>
<ul class="simple">
<li><p>encoder to get hidden states, which are used as <code class="docutils literal notranslate"><span class="pre">&quot;values&quot;</span></code> as well as <code class="docutils literal notranslate"><span class="pre">&quot;keys&quot;</span></code></p></li>
<li><p>decoder to get the first hidden state, which is used as <code class="docutils literal notranslate"><span class="pre">&quot;query&quot;</span></code></p></li>
<li><p>query and keys to work out attention scores, <code class="docutils literal notranslate"><span class="pre">alphas</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;values&quot;</span></code> weighted by <code class="docutils literal notranslate"><span class="pre">alphas</span></code> to get <code class="docutils literal notranslate"><span class="pre">alignment</span> <span class="pre">vectors</span></code></p></li>
<li><p>summing up <code class="docutils literal notranslate"><span class="pre">alignment</span> <span class="pre">vectors</span></code> to get the <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vector</span></code></p></li>
<li><p>concatenate <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vector</span></code> with the decoder hidden state (in this simplest case, the same as <code class="docutils literal notranslate"><span class="pre">query</span></code>), for output predication.</p></li>
</ul>
</div>
</div>
<div class="section" id="scoring-method">
<h2><span class="section-number">1.6. </span>Scoring Method<a class="headerlink" href="#scoring-method" title="Permalink to this headline">¶</a></h2>
<p>In the <code class="docutils literal notranslate"><span class="pre">calc_alpha()</span></code> function above, we used the simple <span class="math notranslate nohighlight">\(\frac{1}{L}\)</span> as weights, which produces an average of the hidden states. Below we show the use of dot product between Q and K as a scoring method.</p>
<div class="section" id="dot-product">
<h3><span class="section-number">1.6.1. </span>Dot product<a class="headerlink" href="#dot-product" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[
cos\ \theta = \frac{\sum_i{q_ik_i}}{\sqrt{\sum_j{q_j^2}}\sqrt{\sum_j{k_j^2}}}
\]</div>
<div class="math notranslate nohighlight">
\[
cos\ \theta \sqrt{\sum_j{q_j^2}}\sqrt{\sum_j{k_j^2}}= \sum_i{q_ik_i}
\]</div>
<div class="math notranslate nohighlight">
\[
cos \theta\ ||Q||\ ||K|| = Q \cdot K
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># N, 1, H x N, H, L -&gt; N, 1, L</span>
<span class="n">products</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">products</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.0569, 0.4821]]], grad_fn=&lt;BmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="attention-scores">
<h3><span class="section-number">1.6.2. </span>Attention Scores<a class="headerlink" href="#attention-scores" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">alphas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.3953, 0.6047]]], grad_fn=&lt;SoftmaxBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_alphas</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="c1"># N, 1, H x N, H, L -&gt; N, 1, L</span>
    <span class="n">products</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">ks</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">alphas</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualizing-the-context">
<h3><span class="section-number">1.6.3. </span>Visualizing the Context<a class="headerlink" href="#visualizing-the-context" title="Permalink to this headline">¶</a></h3>
<p>Here we use a single query (<code class="docutils literal notranslate"><span class="pre">q</span></code>) and three keys (<code class="docutils literal notranslate"><span class="pre">k</span></code>) to illustrate the idea of using dot products in measuring vector similarity. Visually, the red query vector (<span class="math notranslate nohighlight">\(Q\)</span>) is closer to <span class="math notranslate nohighlight">\(K_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">.55</span><span class="p">,</span> <span class="mf">.95</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># N, 1, H</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">.65</span><span class="p">,</span> <span class="mf">.2</span><span class="p">],</span> 
                  <span class="p">[</span><span class="mf">.85</span><span class="p">,</span> <span class="o">-</span><span class="mf">.4</span><span class="p">],</span> 
                  <span class="p">[</span><span class="o">-</span><span class="mf">.95</span><span class="p">,</span> <span class="o">-</span><span class="mf">.75</span><span class="p">]])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># N, L, H</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">query_and_keys</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_47_0.png" src="../_images/attention_47_0.png" />
</div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
Q \cdot K_0 = cos \theta_0\ ||Q||\ ||K_0|| =&amp; &amp;0.73 &amp;*&amp; 1.10 &amp;*&amp; 0.68 =&amp; &amp;0.54
\\
Q \cdot K_1 = cos \theta_1\ ||Q||\ ||K_1|| =&amp; &amp;0.08 &amp;*&amp; 1.10 &amp;*&amp; 0.94 =&amp; &amp;0.08
\\
Q \cdot K_2 = cos \theta_2\ ||Q||\ ||K_2|| =&amp; -&amp;0.93 &amp;*&amp; 1.10 &amp;*&amp; 1.21 =&amp; -&amp;1.23
\end{align*}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># N, 1, H x N, H, L -&gt; N, 1, L</span>
<span class="n">prod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">prod</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.5475,  0.0875, -1.2350]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.5557, 0.3508, 0.0935]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">k</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">context</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.5706, -0.0993]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">query_and_keys</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_52_0.png" src="../_images/attention_52_0.png" />
</div>
</div>
</div>
<div class="section" id="scaled-dot">
<h3><span class="section-number">1.6.4. </span>Scaled Dot<a class="headerlink" href="#scaled-dot" title="Permalink to this headline">¶</a></h3>
<p>The dot product score is sensitive to the length of the vectors, and we can normalise using the standard deviation, which is roughly the sequare root of the dimension of vector as verified using randomly generated normally distributed data.</p>
<div class="section" id="variance-of-a-batch-dot-product-is-roughly-the-same-as-the-dimension-of-the-vector">
<h4><span class="section-number">1.6.4.1. </span>Variance of a batch dot product is roughly the same as the dimension of the vector<a class="headerlink" href="#variance-of-a-batch-dot-product-is-roughly-the-same-as-the-dimension-of-the-vector" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_dims</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">dummy_qs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_dims</span><span class="p">)</span>
<span class="n">dummy_ks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_dims</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">dummy_qs</span><span class="p">,</span> <span class="n">dummy_ks</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(9.7670)
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
Var(Q \cdot K) = d_q = d_k
\\
\sigma(Q \cdot K) = \sqrt{d_q} = \sqrt{d_k}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_product</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dummy_product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">dummy_product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.9526, 0.0474]), tensor([1., 0.]))
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[
scaled\ dot\ product = \frac{Q \cdot K}{\sqrt{d_k}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaled_products</span> <span class="o">=</span> <span class="n">products</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">scaled_products</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.0403, 0.3409]]], grad_fn=&lt;DivBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">alphas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.4254, 0.5746]]], grad_fn=&lt;SoftmaxBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="../_images/aiayn_dot.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_alphas</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># N, 1, H x N, H, L -&gt; N, 1, L</span>
    <span class="n">products</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">ks</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">scaled_products</span> <span class="o">=</span> <span class="n">products</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">alphas</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">calc_alphas</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="c1"># N, 1, L x N, L, H -&gt; 1, L x L, H -&gt; 1, H</span>
<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="n">context_vector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.2138, -0.3175]]], grad_fn=&lt;BmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="encoder-decoder-with-attention-mechanism">
<h2><span class="section-number">1.7. </span>Encoder Decoder with Attention Mechanism<a class="headerlink" href="#encoder-decoder-with-attention-mechanism" title="Permalink to this headline">¶</a></h2>
<div class="section" id="attention-class">
<h3><span class="section-number">1.7.1. </span>Attention Class<a class="headerlink" href="#attention-class" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">proj_values</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="k">if</span> <span class="n">input_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_values</span> <span class="o">=</span> <span class="n">proj_values</span>
        <span class="c1"># Affine transformations for Q, K, and V</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="kc">None</span>
                
    <span class="k">def</span> <span class="nf">init_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_key</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">)</span> \
                      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_values</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span>
                
    <span class="k">def</span> <span class="nf">score_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
        <span class="n">proj_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="c1"># scaled dot product</span>
        <span class="c1"># N, 1, H x N, H, L -&gt; N, 1, L</span>
        <span class="n">dot_products</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_keys</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">scores</span> <span class="o">=</span>  <span class="n">dot_products</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Query is batch-first N, 1, H</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_function</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="c1"># N, 1, L</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># N, 1, L</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        
        <span class="c1"># N, 1, L x N, L, H -&gt; N, 1, H</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">context</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="source-mask">
<h3><span class="section-number">1.7.2. </span>Source Mask<a class="headerlink" href="#source-mask" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">source_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]])</span>
<span class="c1"># pretend there&#39;s an encoder here...</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">.38</span><span class="p">,</span> <span class="mf">.44</span><span class="p">],</span> <span class="p">[</span><span class="mf">.85</span><span class="p">,</span> <span class="o">-</span><span class="mf">.05</span><span class="p">]]])</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">source_seq</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">source_mask</span> <span class="c1"># N, 1, L</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ True, False]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
<span class="n">attnh</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">attnh</span><span class="o">.</span><span class="n">init_keys</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

<span class="n">context</span> <span class="o">=</span> <span class="n">attnh</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">)</span>
<span class="n">attnh</span><span class="o">.</span><span class="n">alphas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[1., 0.]]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="decoder-with-attention">
<h3><span class="section-number">1.7.3. </span>Decoder with Attention<a class="headerlink" href="#decoder-with-attention" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecoderAttn</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regression</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_seq</span><span class="p">):</span>
        <span class="c1"># the output of the encoder is N, L, H</span>
        <span class="c1"># and init_keys expects batch-first as well</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">init_keys</span><span class="p">(</span><span class="n">hidden_seq</span><span class="p">)</span>
        <span class="n">hidden_final</span> <span class="o">=</span> <span class="n">hidden_seq</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden_final</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># L, N, H                      </span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># X is N, 1, F</span>
        <span class="n">batch_first_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span> 
        
        <span class="n">query</span> <span class="o">=</span> <span class="n">batch_first_output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># Attention </span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">concatenated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">context</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span><span class="n">concatenated</span><span class="p">)</span>
        
        <span class="c1"># N, 1, F</span>
        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">source_seq</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">target_seq</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">decoder_attn</span> <span class="o">=</span> <span class="n">DecoderAttn</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Generates hidden states (keys and values)</span>
<span class="n">hidden_seq</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">source_seq</span><span class="p">)</span>
<span class="n">decoder_attn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">hidden_seq</span><span class="p">)</span>

<span class="c1"># Target sequence generation</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">source_seq</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">target_len</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_len</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">decoder_attn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output: </span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>    
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">out</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output: tensor([[[-0.3555, -0.1220]]], grad_fn=&lt;ViewBackward&gt;)
Output: tensor([[[-0.2641, -0.2521]]], grad_fn=&lt;ViewBackward&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="encoder-decoder-attention">
<h3><span class="section-number">1.7.4. </span>Encoder + Decoder + Attention<a class="headerlink" href="#encoder-decoder-attention" title="Permalink to this headline">¶</a></h3>
<p>We can safely use the orginal code for grouping the encoder and decoder (in this case, the decoder with attention), and create a valid model, but we would like to store the output to visualise attention scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encdec</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder_attn</span><span class="p">,</span> <span class="n">input_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">target_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">teacher_forcing_prob</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">encdec</span><span class="p">(</span><span class="n">full_seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.3555, -0.1220],
         [-0.2641, -0.2521]]], grad_fn=&lt;CopySlices&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncoderDecoderAttn</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">teacher_forcing_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">teacher_forcing_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">def</span> <span class="nf">init_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="c1"># N, L (target), F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> 
                              <span class="bp">self</span><span class="o">.</span><span class="n">target_len</span><span class="p">,</span> 
                              <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># N, L (target), L (source)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> 
                                  <span class="bp">self</span><span class="o">.</span><span class="n">target_len</span><span class="p">,</span> 
                                  <span class="bp">self</span><span class="o">.</span><span class="n">input_len</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">store_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
        <span class="c1"># Stores the output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">alphas</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-preparation">
<h3><span class="section-number">1.7.5. </span>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h3>
<p>Training data of the square sequences generated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">points</span><span class="p">,</span> <span class="n">directions</span> <span class="o">=</span> <span class="n">generate_sequences</span><span class="p">()</span>
<span class="n">full_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">target_train</span> <span class="o">=</span> <span class="n">full_train</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_points</span><span class="p">,</span> <span class="n">test_directions</span> <span class="o">=</span> <span class="n">generate_sequences</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">)</span>
<span class="n">full_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">source_test</span> <span class="o">=</span> <span class="n">full_test</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">target_test</span> <span class="o">=</span> <span class="n">full_test</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">full_train</span><span class="p">,</span> <span class="n">target_train</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">source_test</span><span class="p">,</span> <span class="n">target_test</span><span class="p">)</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="model-configuration-training">
<h3><span class="section-number">1.7.6. </span>Model Configuration &amp; Training<a class="headerlink" href="#model-configuration-training" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">decoder_attn</span> <span class="o">=</span> <span class="n">DecoderAttn</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoderAttn</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder_attn</span><span class="p">,</span> <span class="n">input_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">target_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">teacher_forcing_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sbs_seq_attn</span> <span class="o">=</span> <span class="n">StepByStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">sbs_seq_attn</span><span class="o">.</span><span class="n">set_loaders</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="n">sbs_seq_attn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sbs_seq_attn</span><span class="o">.</span><span class="n">plot_losses</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_86_0.png" src="../_images/attention_86_0.png" />
</div>
</div>
</div>
<div class="section" id="visualizing-predictions">
<h3><span class="section-number">1.7.7. </span>Visualizing Predictions<a class="headerlink" href="#visualizing-predictions" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sequence_pred</span><span class="p">(</span><span class="n">sbs_seq_attn</span><span class="p">,</span> <span class="n">full_test</span><span class="p">,</span> <span class="n">test_directions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_88_0.png" src="../_images/attention_88_0.png" />
</div>
</div>
</div>
<div class="section" id="visualizing-attention">
<h3><span class="section-number">1.7.8. </span>Visualizing Attention<a class="headerlink" href="#visualizing-attention" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">full_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">sbs_seq_attn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">sbs_seq_attn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[7.8848e-04, 9.9921e-01],
         [1.0210e-02, 9.8979e-01]]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large
\begin{array}{c|cc}
&amp; source\\
target&amp; \color{red}{x_0} &amp; \color{red}{x_1} \\
\hline
 \color{green}{x_2} &amp; \alpha_{\color{green}{2}\color{red}0} &amp; \alpha_{{\color{green}{2}\color{red}1}} \\
 \color{green}{x_3} &amp; \alpha_{\color{green}{3}\color{red}0} &amp; \alpha_{{\color{green}{3}\color{red}1}}
\end{array}
\implies
\Large
\begin{array}{c|cc}
&amp; source\\
target&amp; \color{red}{x_0} &amp; \color{red}{x_1} \\
\hline
 \color{green}{x_2} &amp; 0.0011 &amp; 0.9989\\
 \color{green}{x_3} &amp; 0.0146 &amp; 0.9854
\end{array}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">full_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">source_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Point #1&#39;</span><span class="p">,</span> <span class="s1">&#39;Point #2&#39;</span><span class="p">]</span>
<span class="n">target_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Point #3&#39;</span><span class="p">,</span> <span class="s1">&#39;Point #4&#39;</span><span class="p">]</span>
<span class="n">point_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;Counter-&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">directions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="si">}</span><span class="s1">Clockwise</span><span class="se">\n</span><span class="s1">Point #1: </span><span class="si">{</span><span class="n">inp</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">inp</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_attention</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">point_labels</span><span class="p">,</span> <span class="n">source_labels</span><span class="p">,</span> <span class="n">target_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/attention_92_0.png" src="../_images/attention_92_0.png" />
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./attention"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="intro.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Lab12: Sequence to Sequence Learning with Attention</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="self_attention.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">2. </span>Self-Attention</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By A/Prof. Wei Liu<br/>
        
            &copy; Copyright UWA 2022.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>